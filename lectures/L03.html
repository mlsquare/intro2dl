<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.55">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="mlsquare">

<title>KANs – Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../lectures/L04.html" rel="next">
<link href="../lectures/L02.html" rel="prev">
<link href="../logo.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/L01.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/L03.html">KANs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlsquare/intro2dl" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Course</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">FFNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CNNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">KANs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RNNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SSMs</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lab</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/00-01-FFN-Classification-Iris.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">FFN Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/00-02-FFN-Regression-Friedman2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">FFNs Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-01-KAN-Intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KAN Intro</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-02-KAN-Splines.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KAN with Splines</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-03-KAN-RBFs.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KANs with RBFs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-04-KAN-Chebyshev.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KANs with Chebyshev</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-05-KAN-Wavelets.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KANs with Wavelets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-07-KAN-PDE.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KAN PDE Solver</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#materials" id="toc-materials" class="nav-link active" data-scroll-target="#materials">Materials:</a>
  <ul class="collapse">
  <li><a href="#pre-work" id="toc-pre-work" class="nav-link" data-scroll-target="#pre-work">Pre-work:</a></li>
  <li><a href="#in-class" id="toc-in-class" class="nav-link" data-scroll-target="#in-class">In-Class</a></li>
  <li><a href="#lab" id="toc-lab" class="nav-link" data-scroll-target="#lab">Lab</a></li>
  <li><a href="#post-class" id="toc-post-class" class="nav-link" data-scroll-target="#post-class">Post-class:</a></li>
  <li><a href="#kan-papers-and-advancements" id="toc-kan-papers-and-advancements" class="nav-link" data-scroll-target="#kan-papers-and-advancements">KAN Papers and Advancements</a></li>
  <li><a href="#theory" id="toc-theory" class="nav-link" data-scroll-target="#theory">Theory</a></li>
  <li><a href="#additional-references" id="toc-additional-references" class="nav-link" data-scroll-target="#additional-references">Additional References</a></li>
  </ul></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes</a>
  <ul class="collapse">
  <li><a href="#kolmogorov-arnold-representation-theorem" id="toc-kolmogorov-arnold-representation-theorem" class="nav-link" data-scroll-target="#kolmogorov-arnold-representation-theorem"><span style="color:blue">Kolmogorov-Arnold representation theorem</span></a></li>
  <li><a href="#kolmogorov-arnold-network-kan" id="toc-kolmogorov-arnold-network-kan" class="nav-link" data-scroll-target="#kolmogorov-arnold-network-kan"><span style="color:blue">Kolmogorov-Arnold Network (KAN)</span></a></li>
  <li><a href="#shallow-nonparametric-regression" id="toc-shallow-nonparametric-regression" class="nav-link" data-scroll-target="#shallow-nonparametric-regression">Shallow Nonparametric Regression</a></li>
  <li><a href="#deep-nonparametric-regression" id="toc-deep-nonparametric-regression" class="nav-link" data-scroll-target="#deep-nonparametric-regression">Deep Nonparametric Regression</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mlsquare/intro2dl/edit/main/lectures/L03.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlsquare/intro2dl/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/L01.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/L03.html">KANs</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">KANs</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="materials" class="level2">
<h2 class="anchored" data-anchor-id="materials">Materials:</h2>
<p>Date: Saturday, 21-Sep-2024, 1.30pm, IST.</p>
<section id="pre-work" class="level3">
<h3 class="anchored" data-anchor-id="pre-work">Pre-work:</h3>
<ul>
<li>Refresh ML foundations.</li>
<li>Read “The 100 page ML book” by Andiry Burkov. Chapters accessible <a href="https://themlbook.com/wiki/doku.php">here</a></li>
<li><a href="../lectures/L01.html">FFNs</a></li>
<li><a href="../lectures/L02.html">CNNs</a></li>
</ul>
</section>
<section id="in-class" class="level3">
<h3 class="anchored" data-anchor-id="in-class">In-Class</h3>
<ul>
<li><a href="https://arxiv.org/abs/2404.19756">KAN: Kolmogorov-Arnold Networks</a></li>
<li>Implementation <a href="https://github.com/KindXiaoming/pykan">PyKAN</a></li>
<li>Implementation <a href="https://github.com/Blealtan/efficient-kan">Efficient-KAN</a></li>
<li><a href="https://arxiv.org/abs/2408.10205">KAN 2.0: Kolmogorov-Arnold Networks Meet Science</a></li>
</ul>
</section>
<section id="lab" class="level3">
<h3 class="anchored" data-anchor-id="lab">Lab</h3>
<ul>
<li><a href="../notebooks/02-01-KAN-Intro.html">Fit a function</a> from <a href="https://github.com/KindXiaoming/pykan">PyKAN</a> simple functions</li>
<li><a href="../notebooks/02-02-KAN-Splines.html">Splines</a> from <a href="https://github.com/KindXiaoming/pykan">PyKAN</a> on a Doppler function.</li>
<li><a href="../notebooks/02-03-KAN-RBFs.html">RBFs</a> from <a href="https://github.com/ZiyaoLi/fast-kan">Fast-KAN</a> on a Doppler function</li>
<li><a href="../notebooks/02-04-KAN-Chebyshev.html">Chebyshev Polynomials</a> from <a href="https://github.com/SynodicMonth/ChebyKAN">ChebyKAN</a> on a Doppler function</li>
<li><a href="../notebooks/02-05-KAN-Wavelets.html">Wavelets</a> from <a href="https://github.com/zavareh1/Wav-KAN">WavKAN</a> on a Doppler function</li>
<li><a href="../notebooks/02-07-KAN-PDE.html">PDE solver</a></li>
<li><a href="./../notebooks/02-06-KAN-PyWavelets.ipynb">Wavelets with IWT</a></li>
<li><a href="https://github.com/jseabold/web-site/blob/master/content/notebooks/wavelet-regression-in-python.ipynb">Wavelet Regression in Python</a></li>
</ul>
</section>
<section id="post-class" class="level3">
<h3 class="anchored" data-anchor-id="post-class">Post-class:</h3>
<ul>
<li>[paper] <a href="https://arxiv.org/abs/2405.07200v1">Chebyshev KAN</a> Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation. Implementation <a href="https://github.com/SynodicMonth/ChebyKAN">ChebyKAN</a></li>
<li>[paper] <a href="https://arxiv.org/abs/2407.11075">Survey of KANs</a> A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</li>
<li>[code] <a href="https://github.com/ZiyaoLi/fast-kan">Fast KAN</a> Replace B-splines with RBF to make it 3.3x faster than <a href="https://github.com/Blealtan/efficient-kan">Efficient-KAN</a></li>
<li>[paper]<a href="https://arxiv.org/abs/2407.01092">Kolmogorov-Arnold Convolutions: Design Principles and Empirical Studies</a> | <a href="https://github.com/IvanDrokin/torch-conv-kan">code</a></li>
<li>[collection] <a href="https://github.com/mintisan/awesome-kan">awesome KANs</a> - a collection of papers, codes, tutorials on KANs.</li>
</ul>
</section>
<section id="kan-papers-and-advancements" class="level3">
<h3 class="anchored" data-anchor-id="kan-papers-and-advancements">KAN Papers and Advancements</h3>
<ul>
<li><a href="https://arxiv.org/abs/2407.16674">KAN vs MLP</a> KAN or MLP: A Fairer Comparison</li>
<li><a href="https://arxiv.org/abs/2406.11045">KANs for PINNs</a> Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks</li>
<li><a href="https://arxiv.org/abs/2405.12832">Wav-KAN</a> Wav-KAN: Wavelet Kolmogorov-Arnold Networks</li>
<li><a href="https://github.com/AdityaNG/kan-gpt">KAN GPT</a></li>
</ul>
</section>
<section id="theory" class="level3">
<h3 class="anchored" data-anchor-id="theory">Theory</h3>
<ul>
<li>1957-<a href="https://cs.uwaterloo.ca/~y328yu/classics/Kolmogorov57.pdf">On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables</a> : The original Kolmogorov Arnold paper</li>
<li>1957-<a href="https://cs.uwaterloo.ca/~y328yu/classics/Arnold57.pdf">On functions of three variables</a></li>
<li>2009-<a href="https://ins.uni-bonn.de/media/public/publication-media/remonkoe.pdf?pk=82">On a constructive proof of Kolmogorov’s superposition theorem</a></li>
<li>2021-<a href="https://arxiv.org/abs/2007.15884">The Kolmogorov-Arnold representation theorem revisited</a></li>
<li>2021-<a href="https://arxiv.org/pdf/2112.09963">The Kolmogorov Superposition Theorem can Break the Curse of Dimension When Approximating High Dimensional Functions</a></li>
</ul>
</section>
<section id="additional-references" class="level3">
<h3 class="anchored" data-anchor-id="additional-references">Additional References</h3>
<ul>
<li>[Book] <a href="https://link.springer.com/book/10.1007/978-1-4612-0709-2">Essential Wavelets for Statistical Applications and Data Analysis</a></li>
<li>[Book] <a href="https://www.stat.cmu.edu/~larry/all-of-nonpar/index.html">All of Nonparametric Statistics</a> - Chapter 9</li>
<li>[Book] <a href="https://link.springer.com/book/10.1007/b98888">Functional Data Analysis</a> Chapter 3 on Fourier basis, Splines, Wavelets and Polynomials</li>
<li>[Book] <a href="https://link.springer.com/book/10.1007/978-0-387-98185-7">Functional Data Analysis with R and MATLAB</a> - Chapter 2 on specifying basis functions</li>
<li>[Book] <a href="https://www.taylorfrancis.com/books/mono/10.1201/9780203753781/generalized-additive-models-hastie">Generalized Additive Models</a> by Hastie and Tibshirani. <a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full">paper</a></li>
<li>[Tutorial] <a href="https://arxiv.org/abs/1406.4068">Functional Regression</a></li>
<li>[Tutorial] <a href="https://arxiv.org/abs/2312.05523">Functional Data Analysis: An Introduction and Recent Developments</a></li>
</ul>
</section>
</section>
<section id="notes" class="level2">
<h2 class="anchored" data-anchor-id="notes">Notes</h2>
<p>The following (written in <span style="color:blue">blue</span>) is taken mostly <em>verbatim</em> from <a href="https://github.com/KindXiaoming/pykan/blob/master/hellokan.ipynb">hellokan.ipynb</a>, the authors of <a href="https://arxiv.org/abs/2404.19756">KAN</a>.</p>
<section id="kolmogorov-arnold-representation-theorem" class="level3">
<h3 class="anchored" data-anchor-id="kolmogorov-arnold-representation-theorem"><span style="color:blue">Kolmogorov-Arnold representation theorem</span></h3>
<p style="color:blue">
Kolmogorov-Arnold representation theorem states that if <span class="math inline">\(f\)</span> is a multivariate continuous function on a bounded domain, then it can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. More specifically, for a smooth <span class="math inline">\(f : [0,1]^{n} \to \mathbb{R}\)</span>, <span class="math display">\[f(x) = f(x_1,...,x_{n})=\sum_{q=1}^{2{n}+1}\Phi_q(\sum_{p=1}^{n} \phi_{q,p}(x_p))\]</span> where <span class="math inline">\(\phi_{q,p}:[0,1]\to\mathbb{R}\)</span> and <span class="math inline">\(\Phi_q:\mathbb{R}\to\mathbb{R}\)</span>. In a sense, they showed that the only true multivariate function is addition, since every other function can be written using univariate functions and sum. However, this 2-Layer width-<span class="math inline">\((2{n}+1)\)</span> Kolmogorov-Arnold representation may not be smooth due to its limited expressive power. We augment its expressive power by generalizing it to arbitrary depths and widths.
</p>
</section>
<section id="kolmogorov-arnold-network-kan" class="level3">
<h3 class="anchored" data-anchor-id="kolmogorov-arnold-network-kan"><span style="color:blue">Kolmogorov-Arnold Network (KAN)</span></h3>
<p style="color:blue">
The Kolmogorov-Arnold representation can be written in matrix form <span class="math display">\[f(x)={\bf \Phi}_{\rm out}\circ{\bf \Phi}_{\rm in}\circ {\bf x}\]</span> where <span class="math display">\[{\bf \Phi}_{\rm in}= \begin{pmatrix} \phi_{1,1}(\cdot) &amp; \cdots &amp; \phi_{1,n}(\cdot) \\ \vdots &amp; &amp; \vdots \\ \phi_{2n+1,1}(\cdot) &amp; \cdots &amp; \phi_{2n+1,n}(\cdot) \end{pmatrix},\quad {\bf \Phi}_{\rm out}=\begin{pmatrix} \Phi_1(\cdot) &amp; \cdots &amp; \Phi_{2n+1}(\cdot)\end{pmatrix}\]</span> We notice that both <span class="math inline">\({\bf \Phi}_{\rm in}\)</span> and <span class="math inline">\({\bf \Phi}_{\rm out}\)</span> are special cases of the following function matrix <span class="math inline">\({\bf \Phi}\)</span> (with <span class="math inline">\(n_{\rm in}\)</span> inputs, and <span class="math inline">\(n_{\rm out}\)</span> outputs), we call a Kolmogorov-Arnold layer: <span class="math display">\[{\bf \Phi}= \begin{pmatrix} \phi_{1,1}(\cdot) &amp; \cdots &amp; \phi_{1,n_{\rm in}}(\cdot) \\ \vdots &amp; &amp; \vdots \\ \phi_{n_{\rm out},1}(\cdot) &amp; \cdots &amp; \phi_{n_{\rm out},n_{\rm in}}(\cdot) \end{pmatrix}\]</span> <span class="math inline">\({\bf \Phi}_{\rm in}\)</span> corresponds to <span class="math inline">\(n_{\rm in}=n, n_{\rm out}=2n+1\)</span>, and <span class="math inline">\({\bf \Phi}_{\rm out}\)</span> corresponds to <span class="math inline">\(n_{\rm in}=2n+1, n_{\rm out}=1\)</span>. After defining the layer, we can construct a Kolmogorov-Arnold network simply by stacking layers! Let’s say we have <span class="math inline">\(L\)</span> layers, with the <span class="math inline">\(l^{\rm th}\)</span> layer <span class="math inline">\({\bf \Phi}_l\)</span> have shape <span class="math inline">\((n_{l+1}, n_{l})\)</span>. Then the whole network is <span class="math display">\[{\rm KAN}({\bf x})={\bf \Phi}_{L-1}\circ\cdots \circ{\bf \Phi}_1\circ{\bf \Phi}_0\circ {\bf x}\]</span> In constrast, a Multi-Layer Perceptron is interleaved by linear layers <span class="math inline">\({\bf W}_l\)</span> and nonlinearities <span class="math inline">\(\sigma\)</span>: <span class="math display">\[{\rm MLP}({\bf x})={\bf W}_{L-1}\circ\sigma\circ\cdots\circ {\bf W}_1\circ\sigma\circ {\bf W}_0\circ {\bf x}\]</span> Even though cumbersome to write, but simpler to see is the following form of the KAN network. Assuming output dimension <span class="math inline">\(n_{L}=1\)</span>, and define <span class="math inline">\(f(\bf{x})\equiv {\rm KAN}(\bf{x})\)</span>: <span class="math display">\[
f(\bf{x})=\sum_{i_{L-1}=1}^{n_{L-1}}\phi_{L-1,i_{L},i_{L-1}}\left(\sum_{i_{L-2}=1}^{n_{L-2}}\cdots\left(\sum_{i_2=1}^{n_2}\phi_{2,i_3,i_2}\left(\sum_{i_1=1}^{n_1}\phi_{1,i_2,i_1}\left(\sum_{i_0=1}^{n_0}\phi_{0,i_1,i_0}(x_{i_0})\right)\right)\right)\cdots\right)
\]</span>
</p>
<p>The basic ingredient is the so called <em>learnable activation</em> <span class="math inline">\(\phi_{l,j,i}\)</span> which maps the post-activation of <span class="math inline">\(i\)</span>th neuron in layer <span class="math inline">\(l\)</span> to the pre-activation of <span class="math inline">\(j\)</span>th neuron in the <span class="math inline">\(l+1\)</span>th layer. Effectively, it is the edge connecting two neurons on adjacent layers. But how can it be made learnable? Represent this activation function as: <span class="math display">\[
\begin{align}
\phi(x)=w_{b} b(x)+w_{s}{\rm spline}(x) \\
b(x)={\rm silu}(x)=x/(1+e^{-x}) \\
{\rm spline}(x) = \sum_i c_iB_i(x)
\end{align}
\]</span> where <span class="math inline">\(c_i\)</span>s are trainable/learnable parameters, <span class="math inline">\(B_i\)</span> are the Spline basis functions (in the original KAN paper). The authors included <span class="math inline">\(b(x)\)</span>, a <span class="math inline">\(SiLU\)</span>, as a residual connection.</p>
<p>The above development of KANs is compelling but it is much more illustrative to approach it from a Nonparametric Regression point of view, and then see that KAN’s are a type of Deep Nonparametric Regressors. Then, we realize that the standard terminology like neurons, activations, pre-activations, post-activation etc., can be completely dropped.</p>
</section>
<section id="shallow-nonparametric-regression" class="level3">
<h3 class="anchored" data-anchor-id="shallow-nonparametric-regression">Shallow Nonparametric Regression</h3>
<p>Consider the following regression problem <span class="math display">\[y^{[i]} \equiv f(x^{[i]}) + e^{[i]} \equiv \phi(x^{[i]}) + e^{[i]}, i \in \left\{1,\dots,N\right\}\]</span> with <span class="math inline">\(D = \{x^{[i]}, y^{[i]}\}_{i=1}^{N}\)</span> representing all the data available to fit (train) the model <span class="math inline">\(f(x)\)</span>. It is customary to write the model as <span class="math inline">\(f(x)\)</span> instead of <span class="math inline">\(\phi(x)\)</span>. It is done for compatibility with KAN. In the shallow case <span class="math inline">\(f(x) = \phi(x)\)</span> otherwise <span class="math inline">\(\phi(x)\)</span> is used to denote the building blocks and <span class="math inline">\(f(x)\)</span> will be a composition of many such building blocks.</p>
<p>For a moment, w.l.o.g, assume <span class="math inline">\(x\)</span> to be univariate. In a typical regression setup, one constructs features such as <span class="math inline">\(x^2, x^3,\dots\)</span>, like in polynomial regression and treat this as a Linear Regression problem. We can view this standard procedure as expanding the function <span class="math inline">\(f(x)\)</span> on a set of Polynomials. Seen in a more general sense, we can choose an appropriate <em>Basis Functions</em> to construct the feature space. For example [see Chapter 9 of <a href="https://www.stat.cmu.edu/~larry/all-of-nonpar/index.html">All of Nonparametric Statistics</a>] <span class="math display">\[f(x) \equiv \phi(x) =  \sum_{i=1}^{\infty} \beta_i B_i(x)\]</span> where <span class="math inline">\(B_1(x) \equiv 1, B_i(x) \equiv \sqrt{2}\cos((i-1)\pi x) \text{ for } i \ge 2\)</span>. See the figure below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./../figs/KANs-Shallow-Regression.drawio.png" class="img-fluid figure-img"></p>
<figcaption>Shallow Nonparametric Regression</figcaption>
</figure>
</div>
<p>In practice, we will truncate the expansion up to some finite number of terms. For illustration, say, we choose <span class="math inline">\(p\)</span> terms. Then, in matrix notation, the regression problem is: <span class="math display">\[
\begin{array}{left}
{\bf y} = {\bf X}{\bf \beta} + {\bf \epsilon}
\end{array}
\]</span> where <span class="math display">\[
\begin{array}{left}
{\bf X}_{N \times p} &amp;=&amp;
\begin{pmatrix} 1 &amp; \sqrt{2}\cos(\pi x^{[1]}) &amp; \dots &amp; \sqrt{2}\cos((p-1)\pi x^{[1]}) \\
1 &amp; \sqrt{2}\cos(\pi x^{[2]}) &amp; \dots &amp; \sqrt{2}\cos((p-1)\pi x^{[2]}) \\
\vdots &amp; &amp; &amp; \vdots \\
1 &amp; \sqrt{2}\cos(\pi x^{[N]}) &amp; \dots &amp; \sqrt{2}\cos((p-1)\pi x^{[N]})
\end{pmatrix} \\
{\bf \beta}_{p \times 1} &amp;=&amp; [\beta_1, \beta_2, \dots, \beta_p ]^T \\
{\bf y}_{N \times 1} &amp;=&amp; [y^{[1]}, y^{[2]}, \dots, y^{[N]} ]^T \\
\end{array}
\]</span> This is the classic Nonparametric Regression setup. Different choices of the basis functions lead to different design matrices <span class="math inline">\({\bf X}\)</span>. Some popular choices are Splines, Chebyshev Polynomials, Legendre Polynomials, Wavelets, among others.</p>
<p style="color:blue">
Specifically, the function approximation with wavelets takes the following form [see <a href="https://github.com/jseabold/web-site/blob/master/content/notebooks/wavelet-regression-in-python.ipynb">Wavelet Regression in Python</a> for a very nice demonstration of wavelets for denoising]: <span class="math display">\[\phi(x) = \alpha\zeta(x) + \sum_{j=0}^{J-1}\sum_{k=0}^{2^j-1}\beta_{jk}\psi_{jk}(x)\]</span> where <span class="math display">\[\alpha=\int_0^1\phi(x)\zeta(x)dx\text{, }\beta_{jk}=\int_0^1\phi(x)\psi_{jk}(x)dx.\]</span> Here <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta_{jk}\)</span> are called the scaling coefficients and the detail coefficients, respectively. The basic idea is that the detail coefficients capture the coarser details of the function while the scaling, or smoothing, coefficients capture the overall functional form.
</p>
<p>So, by choosing a basis function, we map the observed inputs into the feature space defined by the basis functions, and project the response onto the space spanned by the basis functions. This is basically a Linear Regression in a different function space (induced by the basis functions), commonly referred to as Nonparametric Regression.</p>
<section id="generalized-additive-models" class="level5">
<h5 class="anchored" data-anchor-id="generalized-additive-models">Generalized Additive Models</h5>
<p>How do we extend the above formulation to multivariate case. Say, we have <span class="math inline">\(n_{0}\)</span> dimensional inputs. One obvious but non-trivial way is to choose a multivariate basis function. The other way to do that is to construct feature space based on univariate functions which we know how to approximate already. For example, take the cartesian product space as <span class="math inline">\(f(x) \equiv \phi(x) = \Pi_{p=1}^{n_{0}} \phi_p(x_p)\)</span> where <span class="math inline">\(\phi_p\)</span> can be expanded like before (univariate case). Or we can construct the function <span class="math inline">\(f(x)\)</span> additively as <span class="math display">\[
\begin{array}{left}
f(x) \equiv \phi(x) = \sum_{p=1}^{n_{0}} \phi_p(x_p)
\end{array}
\]</span> See the figure below for an illustration.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./../figs/KANs-GAMs.drawio.png" class="img-fluid figure-img"></p>
<figcaption>KAN Neuron = GAM</figcaption>
</figure>
</div>
<p>That is, for every dimension <span class="math inline">\(p\)</span> of input, there is a corresponding <span class="math inline">\(\phi_p\)</span> and we can add them to get a multivariate function. In fact, the above specification appears in the framework developed by Hastie and Tibhirani in 1986, known as Generalized Addtive Models (see <a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full">paper</a>, <a href="https://en.wikipedia.org/wiki/Generalized_additive_model">wiki</a>) or GAMs as they are popularly referred to.</p>
<p>In the context of KANs, we can see this exactly as node at layer <span class="math inline">\(l+1\)</span>, which takes inputs from layer <span class="math inline">\(l\)</span>, whose edges represent the transformation of the inputs via <span class="math inline">\(\phi_p\)</span>. Except for notational differences, each neuron in KAN is a GAM.</p>
</section>
<section id="vector-generalized-additive-models" class="level5">
<h5 class="anchored" data-anchor-id="vector-generalized-additive-models">Vector Generalized Additive Models</h5>
<p>So far we are dealing with single output and multiple inputs. But what if there are <span class="math inline">\(n_{1}\)</span> outputs. This leads us to Vector Generalized Additive Models (VGAMs), proposed by Yee and Wild in 1996 (see <a href="https://www.jstor.org/stable/2345888">paper</a>, <a href="https://en.wikipedia.org/wiki/Vector_generalized_linear_model">wiki</a>). We can represent VGAM as:</p>
<p><span class="math display">\[
\begin{array}{left}
y_q \equiv f_{q,.}(x) \equiv \phi_{q,.}(x)  = \sum_{p=1}^{n_{0}} \phi_{q,p}(x_p)
\end{array}
\]</span></p>
<p>See the figure below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./../figs/KANs-VGAMs.drawio.png" class="img-fluid figure-img"></p>
<figcaption>KAN Layer = VGAM</figcaption>
</figure>
</div>
<p>In effect, one KAN layer is actually a VGAM. So, given <span class="math inline">\(D=\{x^{[i]}, y^{[i]}\}_{i=1}^{N}\)</span> with <span class="math inline">\(x^{[i]} \in R^{n_{0}}, y^{[i]} \in R^{n_{1}}\)</span> outputs, KAN Layer and VGAM learn <span class="math inline">\(\phi_{q,p}\)</span> which are specified in terms of the basis functions. Note that, the choice of the basis functions should correspond to the domain and range of the functions being modeled. GAMS and VGAM models are trained (or leant) using back-fitting techniques. There is no reason why we can not apply backprop and fit these models using gradient descent.</p>
</section>
</section>
<section id="deep-nonparametric-regression" class="level3">
<h3 class="anchored" data-anchor-id="deep-nonparametric-regression">Deep Nonparametric Regression</h3>
<p>What remains to be done is to stack the KAN layers or VGAMs. That gets us to the model we have seen before: <span class="math display">\[
f(\bf{x})=\sum_{i_{L-1}=1}^{n_{L-1}}\phi_{L-1,i_{L},i_{L-1}}\left(\sum_{i_{L-2}=1}^{n_{L-2}}\cdots\left(\sum_{i_2=1}^{n_2}\phi_{2,i_3,i_2}\left(\sum_{i_1=1}^{n_1}\phi_{1,i_2,i_1}\left(\sum_{i_0=1}^{n_0}\phi_{0,i_1,i_0}(x_{i_0})\right)\right)\right)\cdots\right)
\]</span> For clarity sake, <span class="math inline">\(x_{i_0}\)</span> is referring to the <span class="math inline">\(i_0\)</span>th input dimension or feature, <span class="math inline">\(\phi_{l,q,p}(.)\)</span> refers to the function that maps <span class="math inline">\(p\)</span>th input of layer <span class="math inline">\(l\)</span> to the (before summation) <span class="math inline">\(q\)</span>th output.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./../figs/KANs-Deep-VGAMS.drawio.png" class="img-fluid figure-img"></p>
<figcaption>KAN = Deep Nonparametric Regression Model</figcaption>
</figure>
</div>
<p>From the above figure, it is clear that the edges of the KAN network are nonparametric functions <span class="math inline">\(\phi(x)\)</span> which are specified via basis functions such as Splines. Even MLPs have weights on the edges. So, the mainstream interpretation that, in KANs, the activations are on the edges and they are learnable is not very convincing. In both MLPs and KANs, the edges are learnable. For example, if we choose <span class="math inline">\(\phi(x) = \beta x\)</span>, and replace the sum with sum preceded by elementwise, fixed activations like <span class="math inline">\(SiLU\)</span>, we get the standard MLP network. <a href="https://arxiv.org/abs/2404.19756">KAN</a> proposed by Liu et al in 2024 is a Deep Nonparametric Regression framework. Specifically, we note the following:</p>
<ul>
<li>KAN neuron is a Generalized Additive Model (GAM)</li>
<li>KAN layer is a Vector GAM (VGAM)</li>
<li>KAN Network is a Deep VGAM or simply put a Deep Nonparametric Regression Model</li>
</ul>
<p>The generality of this technique comes from</p>
<ul>
<li>flexibility of the specification (multiple inputs, multiple outputs, varying depth and width, choice of basis functions) and</li>
<li>the deep learning framework itself (so we can fit almost all methods using backprop without doing any custom implementations). See for example,
<ul>
<li>Fast-KAN: KANs with Radial Basis Functions <a href="https://github.com/ZiyaoLi/fast-kan">code</a></li>
<li>ChebyKAN: KANs with Chebyshev Polynomials <a href="https://arxiv.org/abs/2405.07200v1">paper</a></li>
<li>WavKAN: KANs with Wavelets <a href="https://arxiv.org/abs/2405.12832">paper</a></li>
<li>TorchKAN: KANs with Monomials, Legendre Polynomials <a href="https://github.com/1ssb/torchkan">code</a></li>
</ul></li>
</ul>
<p>Not only that, the modularity of the KAN layer allows one to mix and match KAN layer with other modules such as a Transformer block or an RNN block, for example. We can replace MLP with KAN almost like a drop-in. We have already seen implementation of KANs in GPT models <a href="https://github.com/AdityaNG/kan-gpt">GTP-KAN</a>, <a href="https://github.com/CG80499/KAN-GPT-2">GPT2-KAN</a>. They also started appearing in CNNs. For more resources see <a href="https://github.com/mintisan/awesome-kan">awesome-KANS</a>.</p>
<p>Explore the features of KANs such as interpretability, solving PDEs, check out this <a href="../notebooks/02-01-KAN-Intro.html">KAN Features</a> notebook or go through the examples from <a href="https://github.com/KindXiaoming/pykan">PyKAN</a> official repo.</p>
</section>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>Few limitation of KANs at this time are:</p>
<ul>
<li>They are relying on one dimensional (univariate) functions as the building blocks. This need not be efficient always. For example, consider 2d functions that have certain spatial or temporal properties. To apply KANs, we have to convert them to 1d first and then model them in KAN layers. It would be much better if we can find multivariate basis functions that can naturally deal with arbitrary dimensions. For example, to process images, 2d wavelets could be a better choice.</li>
<li>In many cases, MLPs still seem to be doing better. See <a href="https://arxiv.org/abs/2407.16674">KAN or MLP: A Fairer Comparison</a> for details. KANs seem to be good at symbolic regression problems which have much more grounding in physical and other sciences.</li>
<li>Spline-based KANs, as noted by others are computationally slow. Knot selection (how many and where to place them) are important hyperparameters which can affect the performance significancy. Grid refinement implemented by the KAN authors is a welcoming step in this direction but it is a hard problem.</li>
</ul>
<p>That said, KANs are extremely interesting in the sense that:</p>
<ul>
<li>KAN is a Deep Nonparametric regression framework</li>
<li>Very generic</li>
<li>After pruning and doing some symbolic search, one can recover interpretable equations, which may be difficult to do in a typical Deep Neural Network.</li>
<li>Combining the power of Wavelets, Filter Banks, Multi Resolution Analysis (MRA) in the KAN framework would be interesting to pursue.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mlsquare\.github\.io\/intro2dl");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../lectures/L02.html" class="pagination-link" aria-label="CNNs">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">CNNs</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../lectures/L04.html" class="pagination-link" aria-label="RNNs">
        <span class="nav-page-text">RNNs</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>course under prep.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlsquare/intro2dl/edit/main/lectures/L03.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlsquare/intro2dl/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This course material was built with <a href="https://quarto.org/">Quarto</a> with <a href="https://github.com/hadley/r4ds">r4ds</a> theme.</p>
</div>
  </div>
</footer>




</body></html>