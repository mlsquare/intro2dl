# RNNs {.unnumbered}

## Materials:
Date: Saturday, 12-October-2024, 1.30pm, IST.

### Pre-work:

- Refresh ML foundations.
- Read "The 100 page ML book" by Andiry Burkov. Chapters accessible [here](https://themlbook.com/wiki/doku.php)
- [FFNs](./L01.qmd) 
- [CNNs](./L02.qmd) 
- [KNNs](./L03.qmd) 



### In-Class

- [Chapter 9](https://d2l.ai/chapter_recurrent-neural-networks/index.html) from on RNNs from d2l.
- Sebastian Raschka's lecture on RNNs [pdf](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf)
- [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Chris Cola's Blog
- [Lecture 14](https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Handout/Lecture14.pdf) on RNNs from from Mitesh Khapra's [CS7015](https://www.cse.iitm.ac.in/~miteshk/CS7015_2019.html) 
- [Lecture 15](https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Handout/Lecture15.pdf) on GRUs, LSTMs, from Mitesh Khapra's [CS7015](https://www.cse.iitm.ac.in/~miteshk/CS7015_2019.html) 

    
### Lab

- Sebastican Raschka's tuotrial [blog](https://sebastianraschka.com/blog/2021/dl-course.html#l15-introduction-to-recurrent-neural-networks), RNN Classifier on IMDB [notebook](https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L15/1_lstm.ipynb)
- [Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis), with [RNNs](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/main/2%20-%20Recurrent%20Neural%20Networks.ipynb)
- [micrograd](https://github.com/karpathy/micrograd) a no dependency, tiny backprop, with a PyTorch like API, from the legendary Andreaj Karpathy
- [BoolGrad](https://github.com/mlsquare/boolgrad) backprop on Boolean Computational Graph, based on micrograd.

### Post-class:

- [GRU Analysis](https://www.youtube.com/watch?v=0oaSSenz_kg)
-[Blog](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - The unreasonable effectiveness of RNNs by Anreaj Karpathy



### RNN Papers and Advancements

- [Transformers are Multi-state RNNs](https://arxiv.org/abs/2401.06104)
- [Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201)
- [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517)
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)

### Theory

- tbd
- tbd


### Additional References

- Saebastian Raschk's course, [youtube playlist](https://youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&si=fMA1c-lNpbOENP0b), [Book](https://github.com/rasbt/python-machine-learning-book-3rd-edition)
- tbd


## Notes


Sure, here is the content converted into Markdown format:

# Linear Models

1. **Line:**
   $x_i = \beta_0 + \beta_1 t + e_i$
   - Example: Section 3.7 from a textbook by S. M. Kay.

2. **De-trended:**
    $x_i = \mu + e_i$
   - Example: Section 3.3 from S. M. Kay.

3. **Curve:**
   $x_i = \beta_0 + \beta_1 t + \beta_2 t^2 + e_i$
   - Example: Section 4.4 from S. M. Kay.

---

# Fourier Analysis

$x_i = \sum_{k=1}^{M} a_k \cos \left( \frac{2 \pi k i}{N} \right) + \sum_{k=1}^{M} b_k \sin \left( \frac{2 \pi k i}{N} \right) + e_i$

where $e_i \sim N(0,1) \) for \( i = 1 \ldots N$.

---

# Dynamical Models: **Kalman Filters**

Consider the AR-signal again:


$x_i = \mu + e_i$

A more accurate model is:


$x_i = \mu_i + e_i$
- The signal is changing with time.

Another model:

$S_t = a S_{t-1} + e_t$
- For $t = 1, 2, \ldots, N$
- This is an **auto-regressive model** with $\text{Var}(e_t) = \sigma_e^2$

---

## AR(1) System

$S(t) \sim N(\mu_s, \sigma_s^2) \quad \text{for} \quad t \ge 0$

- Initial condition not satisfied (steady-state not yet reached).

Example: 
- Parameters: $\( a = 0.9, \sigma_s^2 = 0.1, \mu_s = 5, \sigma_e^2 = 1 \)$
- Question: What does this signal look like?

### Notes:

$$
S_0 = aS(t) + u_0 \\
S_1 = aS_0 + u_1 \\
\vdots
S_n = aS(t) + \sum_{k=0}^{n} e(n-k)
$$