# KANs {.unnumbered}

## Materials:
Date: TBD

### Pre-work:

1. Refresh ML foundations.
2. Read "The 100 page ML book" by Andiry Burkov. Chapters accessible [here](https://themlbook.com/wiki/doku.php)
3. [FFNs](./L01.qmd) 
3. [CNNs](./L02.qmd) 



### In-Class
1. [KAN: Kolmogorov-Arnold Networks](https://arxiv.org/abs/2404.19756)
2. Implementation [PyKAN](https://github.com/KindXiaoming/pykan)
3. Implementation [Efficient-KAN](https://github.com/Blealtan/efficient-kan)
4. [KAN 2.0: Kolmogorov-Arnold Networks Meet Science](https://arxiv.org/abs/2408.10205)
    
### Lab
1. simple functions (tbd)
2. PDE solver (tbd)
3. MLPS vs KAN (tbd)

### Post-class:
1. \[paper\] [Chebyshev KAN](https://arxiv.org/abs/2405.07200v1) Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation. Implementation [ChebyKAN](https://github.com/SynodicMonth/ChebyKAN) 
2. \[paper\] [Wav-KAN](https://arxiv.org/abs/2405.12832) Wav-KAN: Wavelet Kolmogorov-Arnold Networks
3. \[paper\] [Survey of KANs](https://arxiv.org/abs/2407.11075) A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)
4. \[git\] [KAN GPT](https://github.com/AdityaNG/kan-gpt)
5. \[git\] [Fast KAN](https://github.com/ZiyaoLi/fast-kan) Replace B-splines with RBF to make it 3.3x faster than [Efficient-KAN](https://github.com/Blealtan/efficient-kan) 
6. \[paper\][Kolmogorov-Arnold Convolutions: Design Principles and Empirical Studies](https://arxiv.org/abs/2407.01092) | [code](https://github.com/IvanDrokin/torch-conv-kan)
7. \[collection\] [awesome KANs](https://github.com/mintisan/awesome-kan) - a collection of papers, codes, tutorials on KANs.



### Papers

- [KAN vs MLP ](https://arxiv.org/abs/2407.16674) KAN or MLP: A Fairer Comparison
- [KANs for PINNs](https://arxiv.org/abs/2406.11045) Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks

### Theory

- 1957-[On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables](https://cs.uwaterloo.ca/~y328yu/classics/Kolmogorov57.pdf) : The original Kolmogorov Arnold paper
- 1957-[On functions of three variables](https://cs.uwaterloo.ca/~y328yu/classics/Arnold57.pdf)
- 2009-[On a constructive proof of Kolmogorovâ€™s superposition theorem](https://ins.uni-bonn.de/media/public/publication-media/remonkoe.pdf?pk=82)
- 2021-[The Kolmogorov-Arnold representation theorem revisited](https://arxiv.org/abs/2007.15884)
- 2021-[The Kolmogorov Superposition Theorem can Break the Curse of Dimension When Approximating High Dimensional Functions](https://arxiv.org/pdf/2112.09963)

### References
- R01: [CS6910: Deep Learning](http://www.cse.iitm.ac.in/~miteshk/CS6910.html) Mitesh Khapra @ IIT Madras
- R02:  [CS7015: Deep Learning](https://www.cse.iitm.ac.in/~miteshk/CS7015_2019.html) Mitesh Khapra @ IIT Madras) Mitesh Khapra @ IIT Madras (earlier version of CS6910)
- R03: [LING 574: Deep Learning for NLP](https://www.shane.st/teaching/574/spr24/) Shane Steinhard-Threlkeld @ University of Washington
- R04: [Dive into Deep Learning](https://d2l.ai/index.html)
- R05: [Understanding Deep Learning](https://udlbook.github.io/udlbook/)
- R06: [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- R07: [Deep Learning for Computer Vision](https://archive.nptel.ac.in/noc/courses/noc20/SEM2/noc20-cs88/) NPTEL course by Prof. Vineeth N Balasubramanian