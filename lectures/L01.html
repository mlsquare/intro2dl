<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.55">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="mlsquare">

<title>FFNs – Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../lectures/L02.html" rel="next">
<link href="../course.html" rel="prev">
<link href="../logo.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/L01.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/L01.html">FFNs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlsquare/intro2dl" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Course</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L01.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">FFNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CNNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KANs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RNNs</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lab</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/00-01-FFN-Classification-Iris.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">FFN Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/00-02-FFN-Regression-Friedman2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">FFNs Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-01-KAN-Intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KAN Intro</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-02-KAN-Splines.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KAN with Splines</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-03-KAN-RBFs.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KANs with RBFs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-04-KAN-Chebyshev.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KANs with Chebyshev</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-05-KAN-Wavelets.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KANs with Wavelets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/02-07-KAN-PDE.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">KAN PDE Solver</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#materials" id="toc-materials" class="nav-link active" data-scroll-target="#materials">Materials:</a>
  <ul class="collapse">
  <li><a href="#pre-work" id="toc-pre-work" class="nav-link" data-scroll-target="#pre-work">Pre-work:</a></li>
  <li><a href="#in-class" id="toc-in-class" class="nav-link" data-scroll-target="#in-class">In-Class</a></li>
  <li><a href="#lab" id="toc-lab" class="nav-link" data-scroll-target="#lab">Lab</a></li>
  <li><a href="#post-class" id="toc-post-class" class="nav-link" data-scroll-target="#post-class">Post-class:</a></li>
  <li><a href="#papers" id="toc-papers" class="nav-link" data-scroll-target="#papers">Papers</a></li>
  </ul></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes</a>
  <ul class="collapse">
  <li><a href="#linear-model" id="toc-linear-model" class="nav-link" data-scroll-target="#linear-model">Linear Model</a></li>
  <li><a href="#generalized-linear-model" id="toc-generalized-linear-model" class="nav-link" data-scroll-target="#generalized-linear-model">Generalized Linear Model</a></li>
  <li><a href="#vector-generalized-linear-model" id="toc-vector-generalized-linear-model" class="nav-link" data-scroll-target="#vector-generalized-linear-model">Vector Generalized Linear Model</a></li>
  <li><a href="#deep-vector-generalized-linear-model" id="toc-deep-vector-generalized-linear-model" class="nav-link" data-scroll-target="#deep-vector-generalized-linear-model">Deep Vector Generalized Linear Model</a></li>
  <li><a href="#universal-approximation" id="toc-universal-approximation" class="nav-link" data-scroll-target="#universal-approximation">Universal Approximation</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#additional-references" id="toc-additional-references" class="nav-link" data-scroll-target="#additional-references">Additional References</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mlsquare/intro2dl/edit/main/lectures/L01.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlsquare/intro2dl/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/L01.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/L01.html">FFNs</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">FFNs</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="materials" class="level2">
<h2 class="anchored" data-anchor-id="materials">Materials:</h2>
<p>Date: Saturday, 07-Sep-2024, 1.30pm, IST.</p>
<section id="pre-work" class="level3">
<h3 class="anchored" data-anchor-id="pre-work">Pre-work:</h3>
<ol type="1">
<li>Refresh ML foundations.</li>
<li>Read “The 100 page ML book” by Andiry Burkov. Chapters accessible <a href="https://themlbook.com/wiki/doku.php">here</a></li>
</ol>
</section>
<section id="in-class" class="level3">
<h3 class="anchored" data-anchor-id="in-class">In-Class</h3>
<ol type="1">
<li>History of Deep Learning
<ul>
<li>Lecture-1 <a href="http://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Handout/Lecture1.pdf">pdf</a>, <a href="https://iitm-pod.slides.com/arunprakash_ai/lecture-1-briefhistoryofdl/fullscreen">Interactive slides</a> [R01]</li>
<li>Lecture-1 <a href="https://www.shane.st/teaching/574/spr24/slides/1_Intro.pdf">pdf</a> [R02]</li>
</ul></li>
<li>Visualizations
<ul>
<li><a href="https://udlbook.github.io/udlfigures/">Interactive Figures</a> to visualize NNs [R05]</li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap4.html">Representational Power of NNs</a> from [R06]</li>
</ul></li>
<li>Neural Networks Motivation
<ul>
<li>McCulloch-Pitts Neuron, Perceptron <a href="http://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Handout/Lecture2.pdf">Lecture-2:pdf</a> [R01]</li>
<li>Digital Logic Modeling by Perceptron <a href="https://www.shane.st/teaching/574/spr24/slides/4_NN.pdf">Neural Networks:pdf</a> [R03]</li>
</ul></li>
<li>FFNs
<ul>
<li><a href="https://www.shane.st/teaching/574/spr24/slides/6_FF-class-lm.pdf">FFNs for Classification and Language Modeling: pdf</a></li>
</ul></li>
</ol>
</section>
<section id="lab" class="level3">
<h3 class="anchored" data-anchor-id="lab">Lab</h3>
<ol type="1">
<li><a href="./../notebooks/00-01-FFN-Classification-Iris.ipynbnotebook">FFN for Classification</a> on Iris data</li>
<li><a href="../notebooks/00-02-FFN-Regression-Friedman2.html">FFN for Regression</a> on Friedman2 data</li>
</ol>
</section>
<section id="post-class" class="level3">
<h3 class="anchored" data-anchor-id="post-class">Post-class:</h3>
<ol type="1">
<li><a href="http://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Handout/Lecture3.pdf">Lecture-3:pdf</a> Sigmoid Neuron, Error Surfaces, Representation Power of FFNs [R02]</li>
<li><a href="https://www.shane.st/teaching/574/spr24/slides/2_GD_WV.pdf">Gradient Descent, Word Vectors</a> [R03]</li>
<li><a href="http://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Handout/Lecture4.pdf">Lecture-4:pdf</a> FFNs and Backprop [R02]</li>
<li><a href="https://www.shane.st/teaching/574/spr24/slides/5_comp-graph.pdf">Computational Graphs, Backprop:pdf</a> [R03]</li>
<li><a href="https://mitliagkas.github.io/ift6169-2022/ift-6169-lecture-10-notes.pdf">Lecture</a> - Expressivity and UAT</li>
<li>Neural Networks Review from R07
<ul>
<li>Neural Networks: A Review <a href="https://www.youtube.com/watch?v=47d0M3UAXNc">part-1: youtube</a>, <a href="https://www.youtube.com/watch?v=OKVn7q20dEY">part-2: youtube</a></li>
<li>FFNs and Backpopr <a href="https://www.youtube.com/watch?v=8sjbwfHdqW8">part-1: youtube</a>, <a href="https://www.youtube.com/watch?v=XV20CvRsIJU">part-2: youtube</a></li>
</ul></li>
</ol>
</section>
<section id="papers" class="level3">
<h3 class="anchored" data-anchor-id="papers">Papers</h3>
<ol type="1">
<li><a href="https://hal.science/hal-03753170/">Universal Approximation Theorem</a> original paper by Cybenko <a href="https://web.njit.edu/~usman/courses/cs675_fall18/10.1.1.441.7873.pdf">pdf</a></li>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/0893608089900208">Multilayer FFNs are universal approximators</a> Hornik, Stinchcombe, and White</li>
<li><a href="https://arxiv.org/abs/1509.08101">Representation Benefits of Deep FFNs</a> Telagarsky</li>
</ol>
</section>
</section>
<section id="notes" class="level2">
<h2 class="anchored" data-anchor-id="notes">Notes</h2>
<section id="linear-model" class="level3">
<h3 class="anchored" data-anchor-id="linear-model">Linear Model</h3>
<p>Consider the following regression problem <span class="math display">\[y^{[i]} \equiv f(x^{[i]}) + e^{[i]} \equiv \phi(x^{[i]}) + e^{[i]}, i \in \left\{1,\dots,N\right\}\]</span> with <span class="math inline">\(D = \{x^{[i]}, y^{[i]}\}_{i=1}^{N}\)</span> representing all the data available to fit (train) the model <span class="math inline">\(f(x)\)</span>. Suppose that <span class="math inline">\(x_1, x_2, x_3, \dots, x_{n_0}\)</span> are the <span class="math inline">\({n_0}\)</span> features available to fit the model. If we choose <span class="math inline">\(f(.)\)</span> to be a linear combination of the features, it leads us to the familiar Linear Model (or Linear Regression). In matrix notation, the regression problem is: <span class="math display">\[
\begin{array}{left}
{\bf y} = {\bf X}{\bf \beta} + {\bf \epsilon}
\end{array}
\]</span> where <span class="math display">\[
\begin{array}{left}
{\bf X}_{N \times {({n_0}+1)}} &amp;=&amp;
\begin{pmatrix} 1 &amp;  x_1^{[1]} &amp; \dots &amp; x_{n_0}^{[1]} \\
1 &amp; x_1^{[2]} &amp; \dots &amp; x_{n_0}^{[2]} \\
\vdots &amp; &amp; &amp; \vdots \\
1 &amp; x_1^{[N]} &amp; \dots &amp; x_{n_0}^{[N]}
\end{pmatrix} \\
{\bf \beta}_{{({n_0}+1)} \times 1} &amp;=&amp; [\beta_1, \beta_2, \dots, \beta_{({n_0}+1)} ]^T \\
{\bf y}_{N \times 1} &amp;=&amp; [y^{[1]}, y^{[2]}, \dots, y^{[N]} ]^T \\
\end{array}
\]</span> This is the classic Linear Regression setup. To recast this in a familiar Deep Learning notation, we rewrite the above as: <span class="math display">\[
\begin{array}{left}
{\bf y} = {\bf X}{\bf w} + {\bf  b} + {\bf \epsilon}
\end{array}
\]</span> where <span class="math inline">\({\bf  b}\)</span> represents the <span class="math inline">\({n_0} \times 1\)</span> bias (or intercept) term, <span class="math inline">\({\bf w}\)</span> is the weight matrix (regression coefficients) and <span class="math inline">\({\bf X}\)</span> is the set of all <span class="math inline">\(N \times (n_0+1)\)</span> features excluding the column of ones (which was included to model the intercept/ bias term).</p>
<p>The prediction <span class="math inline">\({\bf \hat{y}}\)</span> is typically the conditional expectation <span class="math inline">\({\bf \hat{y}| {\bf X} } = {\bf X}{\bf w} + {\bf  b}\)</span> under the zero-mean error model for <span class="math inline">\({\bf \epsilon}\)</span>, obtained by minimizing the MSE between the observed and the predicted. This is essentially a Perceptron with linear activation function, which is typically used to solve regression problems. What about binary classification (or more generally, categorical responses)?</p>
</section>
<section id="generalized-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="generalized-linear-model">Generalized Linear Model</h3>
<section id="binary-classification" class="level4">
<h4 class="anchored" data-anchor-id="binary-classification">Binary Classification</h4>
<p>Suppose <span class="math inline">\(y \in \{ 0,1\}\)</span> is a binary response, and consider the following (generative) model:</p>
<p><span class="math display">\[
\begin{array}{left}
y^{[i]} = I(h^{[i]} \ge 0) \\
h^{[i]} \equiv f(x^{[i]}) + e^{[i]}
\end{array}
\]</span></p>
<p>Notice that the output is a thresholded version (via the Indicator function) of the linear model response <span class="math inline">\(h^{[i]}\)</span>. If <span class="math inline">\(e^{[i]} \sim N(0, \sigma^2)\)</span>, then,</p>
<p><span class="math display">\[
\begin{array}{left}
P(y^{[i]} = 1) \equiv \pi^{[i]}= \sigma \left( f(x^{[i]}) \right)  \equiv \frac{ \exp(f(x^{[i]})}{1+ \exp(f(x^{[i]}))}
\end{array}
\]</span></p>
<p>If we choose <span class="math inline">\(f(x^{[i]}) \equiv {\bf x}{\bf w} + {\bf  b}\)</span>, where <span class="math inline">\({\bf x}\)</span> is <span class="math inline">\(1 \times {n_0}\)</span> vector of inputs, we recover the Perceptron with sigmoid activation, which is exactly the Logistic regression model, belonging to the much larger class of Generalized Linear Models (GLMs). In the place of <span class="math inline">\(\sigma\)</span>, one can choose any CDF. In particular, if we choose the CDF of the standard Normal distribution, we recover the <a href="https://en.wikipedia.org/wiki/Probit_model"><em>Probit</em></a> model. This Perceptron (Logistic regression) is the most recognizable building block of MLP, a schematic of which is shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./../figs/FFNs-GLMs.drawio.png" class="img-fluid figure-img"></p>
<figcaption>Perceptron = GLM</figcaption>
</figure>
</div>
</section>
<section id="multiclass-classification" class="level4">
<h4 class="anchored" data-anchor-id="multiclass-classification">Multiclass Classification</h4>
<p>The extension of the logistic regression to multiclass is straightforward but we need a different way of writing the models. The Binary Classification problem can written in the following equivalent fashion: <span class="math display">\[
\begin{array}{left}
y^{[i]} \sim Bin(\pi^{[i]}, 1)\\
\pi^{[i]} = \sigma \left( f(x^{[i]}) \right) \equiv \frac{ \exp(f(x^{[i]})} {1+ \exp(f(x^{[i]}) }
\end{array}
\]</span> In the GLM nomenclature, it is customary to link <span class="math inline">\(\pi^{[i]}\)</span> with <span class="math inline">\(f(x^{[i]})\)</span> as follows: <span class="math display">\[\log\left(\frac{\pi^{[i]}}{1-\pi^{[i]}}\right) = f(x^{[i]})\]</span> It is like <span class="math inline">\(f(x^{[i]})\)</span> is now modeling the difference in the class-specific responses. To make this explicit, suppose, <span class="math inline">\(f_k(x^{[i]})\)</span> is the class-specific latent response (whose thresholded version yields the binary response as noted before). Then, by specification, <span class="math inline">\(f(x^{[i]}) =  f_1(x^{[i]}) - f_0(x^{[i]})\)</span>. It follows then that, <span class="math display">\[
\begin{array}{left}
\pi^{[i]} &amp;=&amp; \frac{ \exp(f(x^{[i]})} {1+ \exp(f(x^{[i]}) } \\
&amp;=&amp; \frac{ \exp(f_1(x^{[i]}) - f_0(x^{[i]})) }  {1+ \exp(f_1(x^{[i]}) - f_0(x^{[i]})) } \\
&amp;=&amp; \frac{ \exp(f_1(x^{[i]})} {\exp(f_1(x^{[i]}) + f_0(x^{[i]})) } \\
&amp;\equiv&amp; \text{softmax}(f_1(x^{[i]}))
\end{array}
\]</span></p>
</section>
</section>
<section id="vector-generalized-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="vector-generalized-linear-model">Vector Generalized Linear Model</h3>
<p>Now we are in a position to extend the Binary Classification problem to the Multiclass setting, where the scales are nominal (i.e., the class labels have no ordering, and labels could have been shuffled with no consequence).</p>
<p>Suppose we have <span class="math inline">\({n_L}\)</span>-classes with <span class="math inline">\(\pi_k^{[i]}\)</span> representing the i-th example being drawn from the class <span class="math inline">\(k\)</span> with <span class="math inline">\(\sum_{k=1}^{{n_L}} \pi_k^{[i]} = 1\)</span>. The following generative model is a plausible model: <span class="math display">\[
\begin{array}{left}
y^{[i]} &amp;\sim&amp; \text{Multinomial}(\pi_k^{[i]}, 1)\\
\pi_k^{[i]} &amp;=&amp; \frac{\exp(f_k(x^{[i]})}{ \sum_k \exp(f_k(x^{[i]})}
&amp;\equiv&amp; \text{softmax}(f_k(x^{[i]}))
\end{array}
\]</span></p>
<p>Notice that, we have one hidden/ latent functional for each class <span class="math inline">\(f_k(x^{[i]})\)</span> - the normalized version of which models the class probabilities in the <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial</a> distribution. In general, in the above model, all <span class="math inline">\(f_k(.)\)</span> are not identifiable. In fact, under the simplex constraint (i.e., class probabilities must add up to to one), <span class="math inline">\(\sum_k f_k(.)  =0\)</span>, which essentially is saying that, to model <span class="math inline">\(n_L\)</span> responses, we can not have <span class="math inline">\(n_L\)</span> independent (unconstrained) latent responses but only <span class="math inline">\(n_L-1\)</span> are identifiable.</p>
<p>As a check, if we place this simplex constraint on the logits for the Binary Classification, we can recover standard logistic regression model, as shown below: <span class="math display">\[
\begin{array}{left}
\pi^{[i]} &amp;=&amp; \frac{ \exp(f_1(x^{[i]})} {\exp(f_1(x^{[i]}) + f_0(x^{[i]})) } \\
&amp;=&amp; \frac{\exp(2 f_1(x^{[i]})-1)} {1+ \exp(2f_1(x^{[i]}-1))}   \because f_1(x^{[i]}) + f_0(x^{[i]}) =0 \\
&amp;=&amp; \frac{\exp(f(x^{[i]})} {1+ \exp(f(x^{[i]}))}  \text{ with }   f(x^{[i]}) = 2f_1(x^{[i]})-1 \\
&amp;\equiv&amp; \sigma (f(x^{[i]}))
\end{array}
\]</span></p>
<p><em>Digression: In the Deep Learning context, such explicit constraints are ignored. When training the models with SGD, different tricks which are empirically proven such as Layer Normalization, Batch Normalization etc are used. Their effect is to enforce identifiability constraints via what look like hacks but in reality, they are fixes for the structural issues in the models themselves.</em></p>
<p>When the constrains are removed, and <span class="math inline">\(f_k(x)= {\bf x}{\bf w_k} + {\bf  b_k}\)</span> is a linear model, we get a Vector Generalized Linear Model (<a href="https://en.wikipedia.org/wiki/Vector_generalized_linear_model">VGLM</a>). VGLAM is shown as a (shallow) network below. <img src="./../figs/FFNs-VGLMs.drawio.png" class="img-fluid quarto-figure quarto-figure-center" alt="Nueral Network with no hidden layers = VGLM"></p>
<p>Specifically, ignoring the bias terms for simplicity, <span class="math display">\[
\begin{array}{left}
y_k^{[i]} \equiv \psi(x^{[i]}) = \sigma \left( \sum_{j=1}^{n_0} w_{j,k} x_j^{[i]} \right) \forall k=1,2,\dots n_L\\
\end{array}
\]</span> Here <span class="math inline">\(\sigma(.)\)</span> is element-wise operation. So far we discussed the shallow Perceptron for regression, and classification. It is often helpful to write the expression in matrix notation as follows: <span class="math display">\[
\begin{array}{left}
\bf{y}  = \sigma \left( xW+b \right)
\end{array}
\]</span> where we have added the bias term <span class="math inline">\(b\)</span>, a <span class="math inline">\(n_L\times 1\)</span> vector, <span class="math inline">\(x\)</span> is <span class="math inline">\(1 \times n_0\)</span> input vector, <span class="math inline">\(W\)</span> is a <span class="math inline">\(n_0 \times n_L\)</span> weight matrix. How can we add depth?</p>
</section>
<section id="deep-vector-generalized-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="deep-vector-generalized-linear-model">Deep Vector Generalized Linear Model</h3>
<p>At the heart of the GLM for Multiclass, we have <span class="math inline">\(f_k\)</span> which is a map from <span class="math inline">\({n_0}\)</span>-dimensional input to <span class="math inline">\(R\)</span>, i.e., <span class="math inline">\(f_k(): R^{n_0} \rightarrow R\)</span>. If we have <span class="math inline">\({n_L}\)</span> such functions, we effectively have <span class="math inline">\(f: R^{n_0} \rightarrow R^{n_L}\)</span>. We can stack such blocks to get a composition as follows:</p>
<p><span class="math display">\[
f(\bf{x})= \sigma\left( \dots \left( \sigma\left(x W_0 + b_0\right)W_1 + b_1 \right) \dots \right) W_{n_{L-1}} + b_{n_{L-1}}
\]</span></p>
<p>The above is the popular Multilayer Perceptron or MLP as it is known. Shown below is the network without the bias terms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./../figs/FFNs-Deep-VGLMs.drawio.png" class="img-fluid figure-img"></p>
<figcaption>MLP = Deep VGLM</figcaption>
</figure>
</div>
</section>
<section id="universal-approximation" class="level3">
<h3 class="anchored" data-anchor-id="universal-approximation">Universal Approximation</h3>
<section id="ary-boolean-circuits" class="level4">
<h4 class="anchored" data-anchor-id="ary-boolean-circuits">2-Ary Boolean Circuits</h4>
<p>Let us consider boolean variables and just two inputs <span class="math inline">\(x_1, x_2 \in \{0,1\}\)</span>. We will model the popular logic gates using a simple Neuron: <span class="math inline">\(y = H\left(x_1 w_1 + x_2 w_2 + b\right)\)</span>, where the Heavyside step function <span class="math inline">\(H(x) = 1 \text{ if } x\ge 0, 0. \text{ o.w}\)</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>gate</th>
<th><span class="math inline">\(w_1\)</span></th>
<th><span class="math inline">\(w_2\)</span></th>
<th>b</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AND</td>
<td>1</td>
<td>1</td>
<td>-1.5</td>
</tr>
<tr class="even">
<td>OR</td>
<td>1</td>
<td>1</td>
<td>-0.5</td>
</tr>
<tr class="odd">
<td>NAND</td>
<td>-1</td>
<td>-1</td>
<td>1.5</td>
</tr>
<tr class="even">
<td>NOR</td>
<td>-1</td>
<td>-1</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<p>For example, the Truth Table for AND, and the corresponding output realized by the Neuron is given below:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(y_{AND}\)</span></th>
<th><span class="math inline">\(\hat{y}_{AND}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(H(-1.5)=0\)</span></td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>0</td>
<td><span class="math inline">\(H(-0.5)=0\)</span></td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
<td><span class="math inline">\(H(-0.5)=0\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
<td><span class="math inline">\(H(+0.5)=1\)</span></td>
</tr>
</tbody>
</table>
<p>Other gates like NAND, NOR and OR can also be realized by a Neuron of the form shown above. The NAND gate is called a universal gate (so does NOR) because, given enough number of NAND gates, any Truth Table can be realized consisting of only NAND gates. For example, NOT gate can be realized by setting one of the inputs to logic 1 to get the complement of the other input. Note that XOR can not be modeled by a Perceptron. Can you guess why?</p>
</section>
<section id="m-ary-boolean-circuits" class="level4">
<h4 class="anchored" data-anchor-id="m-ary-boolean-circuits">M-Ary Boolean Circuits</h4>
<p>Now we will generalize the above for M-ary AND, NAND, OR and NOR gates as follows. <span class="math display">\[
\begin{array}{left}
\hat{y}_{AND} = \text{H}(\sum (x_i-1) \,+ 0.5)\\
\hat{y}_{NAND} = \text{H}(\sum(1-x_i) \,- 0.5) \\
\hat{y}_{OR} = \text{H}(\sum x_i \,- 0.5) \\
\hat{y}_{NOR} = \text{H}(\sum -x_i \,+ 0.5) \\
\end{array}
\]</span> Verify the results yourself.</p>
</section>
<section id="m-ary-universal-boolean-circuits" class="level4">
<h4 class="anchored" data-anchor-id="m-ary-universal-boolean-circuits">M-Ary Universal Boolean Circuits</h4>
<p>Now consider M-ary inputs and our goal is to build a boolean circuit which can realize any Truth Table. The Truth Table consists of <span class="math inline">\(2^M\)</span> rows, each row corresponding to one specific state of the inputs, and contains <span class="math inline">\(M+1\)</span> columns - one column for each of the <span class="math inline">\(M\)</span> inputs and one column for the output <span class="math inline">\(y\)</span>. Truth Table can be realized in the form of a Sum-of-Products (SoP) form, a special form of Disjunctive Normal Form (see <a href="https://en.wikipedia.org/wiki/Canonical_normal_form">Canonical Forms</a> to represent Truth Tables in Boolean Algebra). Let us look at an example below with two inputs, modeling XOR gate:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(y_{XOR}\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
<td></td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
<td></td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>
<p>For each row, whose Truth value is 1, we consider the product of inputs. We take the complement of the input when it is zero. For the XOR gate, we add the 2nd row product <span class="math inline">\(\bar{x_1}x_2\)</span> to the third row term <span class="math inline">\(x_1 \bar{x_2}\)</span> to get final SoP as follows: <span class="math display">\[
y = \bar{x_1}x_2 + x_1 \bar{x_2}
\]</span> where <span class="math inline">\(\bar{x}\)</span> represents the logical complement of <span class="math inline">\(x\)</span>. Notice that each product term consists of two sub products - one that is a product of all inputs where inputs are 1s, and product of their complements where the inputs are 0. Therefore, each product in the SoP can be represented as <span class="math inline">\(AND(AND(.), NOR(.))\)</span> and refer to this as <span class="math inline">\(AANOR\)</span> as contraction. For example, the first term in the SoP for XOR is <span class="math inline">\(AND(AND(x_1), NOR(x_2)) \equiv \bar{x_1}x_2\)</span>. We will exploit this to succinctly represent the SoP: <span class="math display">\[
\begin{array}{left}
y &amp;=&amp; \sum_{i \in \{ i' \text{ s. t } y^[i']=1\} } AND( AND( x^{i}_{p \in A}), NOR(x^{i}_{q \in \bar{A}})) \\
A^{[i]} &amp;=&amp; \{ p \text{ s.t } x_p^{[i]} = 1 \} \\
\bar{A}^{[i]} &amp;=&amp; \{ p \text{ s.t } x_p^{[i]} = 0\}
\end{array}
\]</span> The outer sum in the SoP is nothing but an OR gate operating on the innards. Therefore, to realize an SoP boolean network, we need an N-ary OR gate (where N can be at most <span class="math inline">\(2^M\)</span> where M is the number of inputs or input dimension). Then the inner gate is <span class="math inline">\(AANOR \equiv AND(AND(.), NOR(.))\)</span>. Can we realize such as a gate with a Neuron?</p>
</section>
<section id="mlp-with-1-hidden-layer" class="level4">
<h4 class="anchored" data-anchor-id="mlp-with-1-hidden-layer">MLP with 1-hidden layer</h4>
<p>Observe that, the AANOR gate is hot only when all <span class="math inline">\(x \in A\)</span> are 1s and <span class="math inline">\(x \in A^c\)</span> are 0s. To clarity, the AANOR gate is hot for exactly one of the rows of the <span class="math inline">\(2^M\)</span> rows. Exploiting this fact, we can simplify the Truth Table as follows:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Case</th>
<th><span class="math inline">\(x \in A\)</span></th>
<th><span class="math inline">\(x \in \bar{A}\)</span></th>
<th><span class="math inline">\(y_{AANOR}\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Case-1</td>
<td>All 1s</td>
<td>All 0s</td>
<td>1</td>
<td></td>
</tr>
<tr class="even">
<td>Case-2</td>
<td>All 1s</td>
<td>At least one 1</td>
<td>0</td>
<td></td>
</tr>
<tr class="odd">
<td>Case-3</td>
<td>At least one 0</td>
<td>Any</td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>
<p>Verify that cases are mutually exclusive and exhaustive. Based on our idea about AND and NOR gates designed earlier, let us hypothesize the following Neuron for the AANOR gate as follows: <span class="math display">\[
\begin{array}{left}
y &amp;=&amp; H( \sum_{i \in A} (x_i-1) -0.5 + \sum_{i \in \bar{A}} -x_i + 0.5 + b) \\
&amp;=&amp; H( \sum_{i \in A} (x_i-1) + \sum_{i \in \bar{A}} -x_i + b)
\end{array}
\]</span> where <span class="math inline">\(b\)</span> is to be found out, which we do next.</p>
<p>Case-1: In this case, substituting the specific x’s, we get the inequality <span class="math inline">\(b &gt; 0\)</span>, which ensures that <span class="math inline">\(H(.)=1\)</span> in this case.</p>
<p>Case-2: All x’s in A are 1s and at least one x in <span class="math inline">\(\bar{A}\)</span> is 1, implying, we want, <span class="math inline">\(\max \left( \sum_{i \in \bar{A}} -x_i + b \right) &lt; 0\)</span>. It is <span class="math inline">\(-1+b\)</span> and is achieved when exactly all but one x’s in <span class="math inline">\(\bar{A}\)</span> are zero. Therefore, we get, <span class="math inline">\(1-b&lt; 0\)</span></p>
<p>Case-3: At least one x in A is 0, which means, we need <span class="math inline">\(\max \left( \sum_{i \in A} (x_i-1) + \sum_{i \in \bar{A}} -x_i + b \right) &lt; 0\)</span>. It is <span class="math inline">\(-1+b\)</span> and is achieved when exactly one x in A is zero, and all x’s in <span class="math inline">\(\bar{A}\)</span> are zero. Therefore, we get, <span class="math inline">\(b-1 &lt; 0\)</span> as in Case-2. We can choose <span class="math inline">\(b=0.5\)</span> which satisfies both the inequalities <span class="math inline">\(0 &lt; b &lt; 1\)</span>. The AANOR gate can now be realized as:</p>
<p><span class="math display">\[
\begin{array}{left}
y &amp;=&amp; H( \sum_{i \in A} (x_i-1) + \sum_{i \in \bar{A}} -x_i + 0.5)
\end{array}
\]</span></p>
<p>Now that we can realize any AANOR gate, and OR gate is something we have already seen, any M-ary Boolean Truth Table can be realized as <span class="math display">\[
\begin{array}{left}
y &amp;=&amp; OR(\{ AANOR(x^{[i]} \}), i \in \{ i' \text{ s. t } y^[i']=1\}
\end{array}
\]</span></p>
<p>Above can be seen a composition of Boolean gates: <span class="math display">\[
\text{ inputs &gt; hidden (AANOR)  &gt; output (OR) }
\]</span> which is indeed an MLP with 1-hidden layer in hindsight, as illustrated in the figure below. <img src="./../figs/FFNs-SOPs.drawio.png" class="img-fluid quarto-figure quarto-figure-center" alt="SOP = MLP with 1-hidden layer"></p>
<p>Effectively, we exploited the fact that any M-ary Truth Table can be expressed in SOP form, and we have constructed a 1-hidden layer MLP which can exactly model the SoP.</p>
<p>Let us verify this circuit for XOR gate which we know can not be modeled by single Neuron but can be modeled by a 1-hidden layer MLP.</p>
<p><span class="math display">\[
\begin{array}{left}
h_1 &amp;=&amp;  H\left( (x_1-1) - x_2 + 0.5 \right) \\
h_2 &amp;=&amp;  H\left( -x_1 + (x_2-1) + 0.5 \right) \\
y &amp;=&amp;  h_1 + h_2 - 0.5\\
\end{array}
\]</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(y_{XOR}\)</span></th>
<th><span class="math inline">\(h_1\)</span></th>
<th><span class="math inline">\(h_2\)</span></th>
<th><span class="math inline">\(y=OR(h_1, h_2)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Indeed, we recovered the XOR gate. Of course, we are operating on Boolean inputs. The continuous analogue of what we derived for MLPs with 1-hidden layer are due to the early works of Cybenko’s <a href="https://hal.science/hal-03753170/">Universal Approximation Theorem</a> <a href="https://web.njit.edu/~usman/courses/cs675_fall18/10.1.1.441.7873.pdf">pdf</a>. A nice illustration of the Universal Approximation abilities of MLPs can be found here - <a href="http://neuralnetworksanddeeplearning.com/chap4.html">Representational Power of NNs</a></p>
</section>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li>Logistic regression is a member of Generalized Linear Models (GLMs)</li>
<li>GLMs are a member of Vector Generalized Linear Models (VGLMs)</li>
<li>Logistic regression = Perceptron with sigmoid activation</li>
<li>MLP with an input and output layer (with multiple outputs) = VGLM</li>
<li>MLPs are Deep VGLMS</li>
<li>Universal Approximation Ability of MLPs
<ul>
<li>Any M-ary Boolean circuit can be represented in Sum-of-Products form</li>
<li>AND, OR, NAND, NOR gates can be realized by Perceptrons.</li>
<li>Sum-of-Products can be realized by an MLP with 1-hidden layer.</li>
<li>(Therefore) MLP with 1-hidden layer is a Universal Approximator of (any) M-Boolean circuit</li>
</ul></li>
</ul>
</section>
<section id="additional-references" class="level3">
<h3 class="anchored" data-anchor-id="additional-references">Additional References</h3>
<ul>
<li><a href="https://www.amazon.in/Generalized-Chapman-Monographs-Statistics-Probability/dp/0412317605">Generalized Linear Models</a>, a classic, by McCullagh and Nelder.</li>
<li><a href="https://www.amazon.in/Categorical-Analysis-Wiley-Probability-Statistics/dp/0470463635">Categorical Data Analysis</a>, a classic, by Alen Agresti. His others books are nice too.</li>
<li><a href="https://www.amazon.in/Vector-Generalized-Additive-Springer-Statistics/dp/1493928171">Vector Generalized Linear and Additive Moldes</a> T. W. Yee</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mlsquare\.github\.io\/intro2dl");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../course.html" class="pagination-link" aria-label="Course">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Course</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../lectures/L02.html" class="pagination-link" aria-label="CNNs">
        <span class="nav-page-text">CNNs</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>course under prep.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlsquare/intro2dl/edit/main/lectures/L01.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlsquare/intro2dl/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This course material was built with <a href="https://quarto.org/">Quarto</a> with <a href="https://github.com/hadley/r4ds">r4ds</a> theme.</p>
</div>
  </div>
</footer>




</body></html>