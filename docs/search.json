[
  {
    "objectID": "notebooks/00-01-FFN-Classification-Iris.html",
    "href": "notebooks/00-01-FFN-Classification-Iris.html",
    "title": "FFN Classification",
    "section": "",
    "text": "A notebook to apply an FFN (Feed Forward Neural Network) to classify the flower species type. We will use the the famous Iris dataset (which is now the equivalent of the hellow world dataset in the Data Science World)\nBased on - Kaggle Notebook for Iris Classiifcation - PyTorch for Iris Dataset - Iris Classification\nLoad, Visualize, Summarise Data\nsklearn comes with Iris dataset. We will load it, and do some basic visualization. It is always a good idea to “look” at the data before (blindly) running any models.\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots(figsize=(5,5))\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\nWe see that, 1. there are four features, and it is a three class classification problem 2. Using two features (sepal length, and sepal width), it is clear that, a perceptron will not be able separate _versicolor from virginica (data is not linearly separable) class. 3. But setosa can be separated from the remaining two.\nLet us look at the basic descriptions of the data.\n\n\nprint('feature name',iris.feature_names)\nprint('features type of data',type(iris.data))\nprint('features shape',iris.data.shape)\nprint('feature name',iris.target_names)\nprint('target type of data',type(iris.target))\nprint('target shape',iris.target.shape)\n\nfeature name ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nfeatures type of data &lt;class 'numpy.ndarray'&gt;\nfeatures shape (150, 4)\nfeature name ['setosa' 'versicolor' 'virginica']\ntarget type of data &lt;class 'numpy.ndarray'&gt;\ntarget shape (150,)\n\n\n\nprint('target labels',iris.target)\n\ntarget labels [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n\n\nHa. In the original dataset, the data is organized by class. If we naively prepare the mini batches (sequentially), model will only see data corresponding to only one class. This will be pretty problematic to get proper gradient signals. We should shuffle the data s.t diversity in the mini batches is maintained.\nQuestions\nImagine you split the data into two batches. One containing only say class 0, and other contains only class 1. During training, the model sees these two batches cyclically. Will the model ever converge.\n\nWill it converge when the data is linearly separable?\nWill it converge when the data is not linearly separable?\nDoes having a balanced class representation in every mini batch helps? Which way does it?\nWhat will be the impact of learning rate when alternating between sets of samples of one class during gradient descent?\n\nLet us get back to checking the data, this time, from huggingace datasets itself. Later down the line, it may be useful to learn how to work with datasets library from HuggingFace. It has deep integrations with PyTorch.\n\nfrom datasets import Dataset\nimport pandas as pd\ndf = pd.read_csv(\"hf://datasets/scikit-learn/iris/Iris.csv\")\ndf = pd.DataFrame(df)\ndf.head()\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\ndf['Species'].unique()\n\narray(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)\n\n\nInterestingly, the first column is ids., which is not useful for us. May be, a perfect system can simply memory the indices and spit out the correct classes.\nAnd we need to map the Iris types into numerical codes for models to work with. In the torch, we can supply integers representing the classes, and we do not have to explicitly pass one-hot coded labels.\n\n# transform species to numerics\ndf.loc[df.Species=='Iris-setosa', 'Target'] = 0\ndf.loc[df.Species=='Iris-versicolor', 'Target'] = 1\ndf.loc[df.Species=='Iris-virginica', 'Target'] = 2\nprint(df.Target.unique())\ndf.head()\n\n[0. 1. 2.]\n\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nTarget\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0.0\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0.0\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0.0\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0.0\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0.0\n\n\n\n\n\n\n\n\n# drop the Id columns from the dataframe\ndf.drop(['Id'],axis=1,inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nTarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0.0\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = df[df.columns[0:4]].values\ny = df.Target.values\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\n\n\n# dip test:  check that data is shuffled\nprint(y_train)\n\n[2. 1. 2. 2. 0. 0. 0. 2. 1. 2. 0. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 2. 0. 1.\n 1. 0. 1. 0. 1. 1.]\n\n\nQuestions\nAbove (visualluy inspecting data) is not a rigorous way (and repeatable way) to test if the data is shuffled (randomly). For numerical labels like integers, in the multi-class or binary class classification problems, which statistical test is suitable to flag if the data grouped?\n\n# scale the features to roughly have zero mean and unit variance\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nIt is always a good practice to scale the data (features).\n\nWhat might happen if the different features are on different scales?\nDoes it pose any problems for the optimizer (gradient descent)?\nDoes it cause any problems w.r.t interpretation of the feature importance?\n\nSuppose instead of\n\ncreate train, test splits\nlearn the scaling transformation on train data\nscale both train and test data\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nyou do the following\n\nlearn the scaling transformation on whole data before split\nand then create train, test splits\n\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\nWhat happens? Should we do this?\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)\n\nLet us define a FFN (or MLP) with two hidden layers. Suppose \\(x\\) is a \\(B \\times 4\\) vector, we have two hidden layers of 64 dimensions each, and we have three outputs (one for each class), then, \\[\nh_1 = ReLU(x W_{1} +b_{1}) \\\\\nh_2 = ReLU(h_1 W_{2} +b_{2}) \\\\\ny =  h_2 W_{out} + b_{out} \\\\\n\\]\nwhere \\[\nx \\text{ is } B \\times 4 \\\\\nW_{1} \\text{ is } 4 \\times 64 \\\\\nW_{2} \\text{ is } 64 \\times 64 \\\\\nW_{out} \\text{ is } 64 \\times 3 \\\\\nb_{1} \\text{ is } 1 \\times 64 \\\\\nb_{2} \\text{ is } 1 \\times 64 \\\\\nb_{out} \\text{ is } 1 \\times 3 \\\\\ny \\text{ is } B \\times 3 \\\\\n\\]\nIn $xW +b $, \\(b\\) is broadcast over all rows and \\(B\\) is the batch size.\n\\(ReLU(x) = x \\text{ if } x \\ge 0 \\text{ and } 0 \\text{ o.w }\\)\nQuestion\nWhat is the total number of parameters if input dimension is \\(p^{in}\\), output dimension is \\(p^{out}\\) and each hidden layer is of size \\(p^{h}_{i}\\) for the i-th hidden layer and there \\(d\\) such layers?\n\nclass MLP(nn.Module):\n    # define nn\n    def __init__(self, input_dim=4, output_dim=3, hidden_dim = [128,64]):\n        super(MLP, self).__init__()\n        self.h1 = nn.Linear(input_dim, hidden_dim[0])\n        self.h2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.out = nn.Linear(hidden_dim[1], output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        X = self.relu(self.h1(X))\n        X = self.relu(self.h2(X))\n        X = self.out(X)\n        return X\n\nWe have built a Neural Network with one input layer, two hidden layers, and one output layer.\nNote, the last output layer is a linear layer. Even though we are modeling a 3-class problem, output layer is still linear, and not softmax. Is this fine?\n\ninput_dim = 4 # No. of features\noutput_dim = 3 # No. of outputs\nhidden_dim = [64, 64] # No. of perceptrons in 1st hidden layer and 2nd hidden layer\nmodel = MLP(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim) # instantiate the model\n\n\n# inspect the model for a given batch size\nfrom torchinfo import summary\nsummary(model, input_size=(10, 4))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [10, 3]                   --\n├─Linear: 1-1                            [10, 64]                  320\n├─ReLU: 1-2                              [10, 64]                  --\n├─Linear: 1-3                            [10, 64]                  4,160\n├─ReLU: 1-4                              [10, 64]                  --\n├─Linear: 1-5                            [10, 3]                   195\n==========================================================================================\nTotal params: 4,675\nTrainable params: 4,675\nNon-trainable params: 0\nTotal mult-adds (M): 0.05\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.02\nEstimated Total Size (MB): 0.03\n==========================================================================================\n\n\n\nlearning_rate = 0.01\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n\nIn the above train_network block, we have not used batches. Entire train data is passed at once. So, one epoch is one complete pass through the data.\nExercise\nModify the training loop to pass over mini batches.\n\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\n\n\ntrain_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 0.0001, Test Loss: 0.1433\nEpoch 100/1000, Train Loss: 0.0000, Test Loss: 0.1390\nEpoch 150/1000, Train Loss: 0.0000, Test Loss: 0.1403\nEpoch 200/1000, Train Loss: 0.0000, Test Loss: 0.1430\nEpoch 250/1000, Train Loss: 0.0000, Test Loss: 0.1450\nEpoch 300/1000, Train Loss: 0.0000, Test Loss: 0.1467\nEpoch 350/1000, Train Loss: 0.0000, Test Loss: 0.1484\nEpoch 400/1000, Train Loss: 0.0000, Test Loss: 0.1500\nEpoch 450/1000, Train Loss: 0.0000, Test Loss: 0.1514\nEpoch 500/1000, Train Loss: 0.0000, Test Loss: 0.1525\nEpoch 550/1000, Train Loss: 0.0000, Test Loss: 0.1533\nEpoch 600/1000, Train Loss: 0.0000, Test Loss: 0.1540\nEpoch 650/1000, Train Loss: 0.0000, Test Loss: 0.1547\nEpoch 700/1000, Train Loss: 0.0000, Test Loss: 0.1554\nEpoch 750/1000, Train Loss: 0.0000, Test Loss: 0.1562\nEpoch 800/1000, Train Loss: 0.0000, Test Loss: 0.1569\nEpoch 850/1000, Train Loss: 0.0000, Test Loss: 0.1577\nEpoch 900/1000, Train Loss: 0.0000, Test Loss: 0.1584\nEpoch 950/1000, Train Loss: 0.0000, Test Loss: 0.1592\nEpoch 1000/1000, Train Loss: 0.0000, Test Loss: 0.1598\n\n\n\nplt.figure(figsize=(4,4))\nplt.plot(train_losses, label='train loss')\nplt.plot(test_losses, label='test loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\npredictions_train = []\npredictions_test =  []\nwith torch.no_grad():\n    predictions_train = model(X_train)\n    predictions_test = model(X_test)\n\n\nprint(predictions_train.shape)\nprint(type(predictions_train))\n\nprint(y_train.shape)\nprint(type(y_train))\n\ntorch.Size([30, 3])\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([30])\n&lt;class 'torch.Tensor'&gt;\n\n\n\ndef get_accuracy_multiclass(pred_arr,original_arr):\n    if len(pred_arr)!=len(original_arr):\n        return False\n    pred_arr = pred_arr.numpy()\n    original_arr = original_arr.numpy()\n    final_pred= []\n    # we will get something like this in the pred_arr [32.1680,12.9350,-58.4877]\n    # so will be taking the index of that argument which has the highest value here 32.1680 which corresponds to 0th index\n    for i in range(len(pred_arr)):\n        final_pred.append(np.argmax(pred_arr[i]))\n    final_pred = np.array(final_pred)\n    count = 0\n    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy\n    for i in range(len(original_arr)):\n        if final_pred[i] == original_arr[i]:\n            count+=1\n    return count/len(final_pred)\n\nNotice that the model predictions were of size (batch_size, output_dim) and we have to take argmax of the model predictions to produce the class labels. The predictions are in the logit space (recall that the output layer is linear and not softmax).\n\ntrain_acc = get_accuracy_multiclass(predictions_train,y_train)\ntest_acc  = get_accuracy_multiclass(predictions_test,y_test)\nprint(f\"Training Accuracy: {round(train_acc*100,3)}\")\nprint(f\"Test Accuracy: {round(test_acc*100,3)}\")\n\nTraining Accuracy: 100.0\nTest Accuracy: 95.0",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>FFN Classification</span>"
    ]
  },
  {
    "objectID": "notebooks/00-02-FFN-Regression-Friedman2.html",
    "href": "notebooks/00-02-FFN-Regression-Friedman2.html",
    "title": "FFNs Regression",
    "section": "",
    "text": "A notebook to apply an FFN (Feed Forward Neural Network) to regress. We will use Friedman2 dataset from sklearn\nBased on - Kaggle Notebook for Iris Classiifcation - PyTorch for Iris Dataset - Iris Classification\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_friedman2\nX, y = make_friedman2(n_samples = 200, random_state=42, noise=0.5)\nprint(X.shape)\nprint(y.shape)\n\n(200, 4)\n(200,)\n\n\n\nplt.figure(figsize=(5,5))\nplt.scatter(X[:,0],y)\nplt.show()\nplt.hist(y)\n\n\n\n\n\n\n\n\n(array([64., 29., 30., 19., 23., 20.,  5.,  2.,  5.,  3.]),\n array([  13.8686335 ,  177.60233879,  341.33604407,  505.06974936,\n         668.80345464,  832.53715992,  996.27086521, 1160.00457049,\n        1323.73827578, 1487.47198106, 1651.20568635]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n# split and scale.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# Friedman2 response variable has huge dynamic range. The maximum value seems above above 1500.\nscaler = StandardScaler()\n\nprint(y_train.shape)\ny_tmp = y_train.reshape(-1, 1)\ny_train = scaler.fit_transform(y_tmp)\nprint(y_train.shape)\n\ny_tmp = y_test.reshape(-1, 1)\ny_test = scaler.transform(y_tmp)\nprint(y_test.shape)\n\n(160,)\n(160, 1)\n(40, 1)\n\n\nExercise\nRun the regression without scaling the response variable! What will be expected?\n\n# load the libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\n# convert numpy arrays into torch tensors\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.FloatTensor(y_train)\ny_test = torch.FloatTensor(y_test)\n\n\n# define the model.\n# the model is exactly the same as the model we saw in earlier\nclass MLP(nn.Module):\n    # define nn\n    def __init__(self, input_dim=4, output_dim=3, hidden_dim = [128,64]):\n        super(MLP, self).__init__()\n        self.input = nn.Linear(input_dim, hidden_dim[0])\n        self.hidden = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.out = nn.Linear(hidden_dim[1], output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        X = self.relu(self.input(X))\n        X = self.relu(self.hidden(X))\n        X = self.out(X)\n        return X\n\n\ninput_dim = 4\noutput_dim = 1\nhidden_dim = [64, 64]\nmodel = MLP(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim)\n\n\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\nSince it is a regression problem, we have used MSE loss. Practically, this is the only change we have to make so far. And of course, how to evaluate the model perdictions has to change!\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n\n\nimport numpy as np\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\ntrain_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 0.0040, Test Loss: 0.0068\nEpoch 100/1000, Train Loss: 0.0010, Test Loss: 0.0037\nEpoch 150/1000, Train Loss: 0.0005, Test Loss: 0.0032\nEpoch 200/1000, Train Loss: 0.0003, Test Loss: 0.0028\nEpoch 250/1000, Train Loss: 0.0002, Test Loss: 0.0026\nEpoch 300/1000, Train Loss: 0.0001, Test Loss: 0.0025\nEpoch 350/1000, Train Loss: 0.0001, Test Loss: 0.0025\nEpoch 400/1000, Train Loss: 0.0001, Test Loss: 0.0024\nEpoch 450/1000, Train Loss: 0.0016, Test Loss: 0.0037\nEpoch 500/1000, Train Loss: 0.0000, Test Loss: 0.0024\nEpoch 550/1000, Train Loss: 0.0000, Test Loss: 0.0023\nEpoch 600/1000, Train Loss: 0.0008, Test Loss: 0.0030\nEpoch 650/1000, Train Loss: 0.0000, Test Loss: 0.0023\nEpoch 700/1000, Train Loss: 0.0000, Test Loss: 0.0022\nEpoch 750/1000, Train Loss: 0.0000, Test Loss: 0.0022\nEpoch 800/1000, Train Loss: 0.0002, Test Loss: 0.0024\nEpoch 850/1000, Train Loss: 0.0000, Test Loss: 0.0022\nEpoch 900/1000, Train Loss: 0.0000, Test Loss: 0.0022\nEpoch 950/1000, Train Loss: 0.0000, Test Loss: 0.0021\nEpoch 1000/1000, Train Loss: 0.0000, Test Loss: 0.0024\n\n\n\nplt.figure(figsize=(5,5))\nplt.plot(train_losses, label='train loss')\nplt.plot(test_losses, label='test loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\npredictions_train = []\npredictions_test =  []\nwith torch.no_grad():\n    predictions_train = model(X_train)\n    predictions_test = model(X_test)\n\nprint(predictions_train.shape)\nprint(type(predictions_train))\n\nprint(y_train.shape)\nprint(type(y_train))\n\ntorch.Size([160, 1])\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([160, 1])\n&lt;class 'torch.Tensor'&gt;\n\n\n\nyt = y_test.numpy()\nprint(type(yt))\nprint(yt.shape)\n\nyh = predictions_test.numpy()\nprint(type(yh))\nprint(yh.shape)\n\n&lt;class 'numpy.ndarray'&gt;\n(40, 1)\n&lt;class 'numpy.ndarray'&gt;\n(40, 1)\n\n\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import root_mean_squared_error as rmse\n\nprint('mse is: ', mse(yt, yh))\nprint('rmse is: ', rmse(yt, yh))\n\nmse is:  0.00236044\nrmse is:  0.04858436\n\n\n\nplt.scatter(yt,yh)\n\n\n\n\n\n\n\n\n\nresiduals = yt-yh\nplt.hist(residuals)\nplt.show()\n\nimport pandas as pd\npd.DataFrame(residuals).plot(kind='density')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompared to FFNs with Classification, for regression, we used MSE loss, keeping all else the same (code wise). While making predictions, we just used the outputs of the model as is.",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFNs Regression</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Welcome\nDear Faculty, Students and Learners\nSee the course page for recent information on Lectures, Labs, Resources etc..",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "Deep Learning",
    "section": "Announcements",
    "text": "Announcements\n\n[07-Sep-2024] L01 added.\n[21-Aug-2024] Course website up",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Learning",
    "section": "Overview",
    "text": "Overview\nPrereqs\n\nUndergraduate level exposure to Linear Algebra, Calculus\nAbility to read Python code\nBasic exposure to ML/DL\n\nPart-1: A Mathematical Introduction to Deep Learning Models\n\nTopics\n\nFeed Forward Neural Networks (FFNs)\nConvolution Neural Networks (CNNs)\nRecurrent Neural Networks (RNNs)\nTransformers\n\n\nPart-2: A Mathematical Introduction to Deep Generative Models\n\nTopics\n\nVariational Auto Encoders (VAEs)\nGenerative Adversarial Networks (GANs)\nFlow Networks\nDiffusion Models",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "When I started reading papers about Deep Learning, typically published in conferences, I had hard time following them. The key modeling details are presented in textual form with accompanied by architecture (block) diagrams. If the paper had any Math in them it was for proofs but not for explaining the models or the expressions were restricted to highlighting key contributions (eg. loss function).\nDespite this information, it was hard for me to understand them to be able to reproduce. I have to look at source code to see how they are implemented and go back to the paper and read again, and repeat this process. This was probably due to the my formal training Statistics. I always start with the model (expressed as equations). This unlearning took a long time. I am assuming that folks with training in Maths/ Applied Maths will also have hard time reading papers in the ML space for the same reason - there is no precision.\nIn this course, we will discuss different architectures (models) in Deep Learning using Mathematical language as much as possible (for the sake of precision) and follow-them up with implementation in code.\nThe intended audience is those with math background, that wants to appreciate modern deep learning models. We will not get into “why” deep learning works or their applications. In the resources section, I will provide ample references for those interested that wants to explore further.\n\nDisclaimer\nThis course is by no means a replacement of any other resources available. Hopefully, the content and views presented complement the current practice of MLOps, readers and students benefit from it.\nopenly,\nThe Saddle Point",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Course",
    "section": "",
    "text": "Syllabus & Schedule",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#references",
    "href": "course.html#references",
    "title": "Course",
    "section": "References",
    "text": "References\n\n[course] CS6910, Prof. Mitesh Khapra’s CS6910 Deep Learning at IIT-M\n[course] CS236 Prof. Stefano Emron’s course on Deep Generative Modeling at Stanford Fall’23\n[Book] Deep Generative Modeling, Jakub Tomxzak\n[Book] Understanding Deep Learning, Simon Prince\n[Book] Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville\n\nSoma S Dhavala\nDiscussion Anchor",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "lectures/L01.html",
    "href": "lectures/L01.html",
    "title": "FFNs",
    "section": "",
    "text": "Materials:\nDate: Saturday, 07-Sep-2024, 1.30pm",
    "crumbs": [
      "Lectures",
      "FFNs"
    ]
  },
  {
    "objectID": "lectures/L01.html#materials",
    "href": "lectures/L01.html#materials",
    "title": "FFNs",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\n\n\n\nIn-Class\n\nHistory of Deep Learning\n\nLecture-1 pdf, Interactive slides [R01]\nLecture-1 pdf [R02]\n\nVisualizations\n\nInteractive Figures to visualize NNs [R05]\nRepresentational Power of NNs from [R06]\n\nNeural Networks Motivation\n\nMcCulloch-Pitts Neuron, Perceptron Lecture-2:pdf [R01]\nDigital Logic Modeling by Perceptron Neural Networks:pdf [R03]\n\nFFNs\n\nFFNs for Classification and Language Modeling: pdf\n\n\n\n\nLab\n\nFFN for Classification on Iris data\nFFN for Regression on Friedman2 data\n\n\n\nPost-class:\n\nLecture-3:pdf Sigmoid Neuron, Error Surfaces, Representation Power of FFNs [R02]\nGradient Descent, Word Vectors [R03]\nLecture-4:pdf FFNs and Backprop [R02]\nComputational Graphs, Backprop:pdf [R03]\nLecture - Expressivity and UAT\nNeural Networks Review from R07\n\nNeural Networks: A Review part-1: youtube, part-2: youtube\nFFNs and Backpopr part-1: youtube, part-2: youtube\n\n\n\n\nPapers\n\nUniversal Approximation Theorem original paper by Cybenko pdf\nMultilayer FFNs are universal approximators Hornik, Stinchcombe, and White\nRepresentation Benefits of Deep FFNs Telagarsky\n\n\n\nReferences\n\nR01: CS6910: Deep Learning Mitesh Khapra @ IIT Madras (earlier version of CS6910)\nR02: CS7015: Deep Learning Mitesh Khapra @ IIT Madras) Mitesh Khapra @ IIT Madras (earlier version of CS6910)\nR03: LING 574: Deep Learning for NLP Shane Steinhard-Threlkeld @ University of Washington\nR04: Dive into Deep Learning\nR05: Understanding Deep Learning\nR06: Neural Networks and Deep Learning\nR07: Deep Learning for Computer Vision NPTEL course by Prof. Vineeth N Balasubramanian",
    "crumbs": [
      "Lectures",
      "FFNs"
    ]
  },
  {
    "objectID": "lectures/L02.html",
    "href": "lectures/L02.html",
    "title": "CNNs",
    "section": "",
    "text": "Materials:\nDate: TBD",
    "crumbs": [
      "Lectures",
      "CNNs"
    ]
  },
  {
    "objectID": "lectures/L02.html#materials",
    "href": "lectures/L02.html#materials",
    "title": "CNNs",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nFFNs\n\n\n\nIn-Class\n\nCNNs Intro\n\nChapter 7[R05] covers topics like Invariance, Channels, Convolution Operation, Padding, Stride, Pooling.\nCNNs[R02]\n\nVisualizations\n\nCNNs[R02]\n\nCNN Architectures\n\nChapter 8[R04] covers landmark CNN architectures such as AlexNet, VGG, ResNet, DenseNet.\n\n\n\n\nLab\n\n1d CNNs (tbd)\n2d CNNs (tbd)\n\n\n\nPost-class:\n\n[notebook] ID Convolution R05\n[notebook] ID Convolution for MNSIT R05\n[notebook] 2D Convolution R05\n[notebook] CNN for MNIST R05\n[youtube] Lectures from R07\n\nCNNs: An Intro Part-1, part-2\nBackprop in CNNS youtube\nEvolution of CNNs for Image Classification part-1, part-2\nRecent Architectures youtube [as of 2021]\nCNNs for Object Detection Part-1, Part-2 Part-3\nCNNs for Segmentation youtube\n\n\n\n\nPapers\n\nAlexNet - ImageNet Classification with Deep Convolutional Neural Networks\nZFNet Visualizing and Understanding Convolutional Networks\nVGGNet Very Deep Convolutional Networks for Large-Scale Image Recognition\nGooLeNet Going Deeper with Convolutions\nResNet Deep Residual Learning for Image Recognition\nRCNN Rich feature hierarchies for accurate object detection and semantic segmentation -Faster-RCNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\nYOLO You Only Look Once: Unified, Real-Time Object Detection\nUNet U-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nReferences\n\nR01: CS6910: Deep Learning Mitesh Khapra @ IIT Madras\nR02: CS7015: Deep Learning Mitesh Khapra @ IIT Madras) Mitesh Khapra @ IIT Madras (earlier version of CS6910)\nR03: LING 574: Deep Learning for NLP Shane Steinhard-Threlkeld @ University of Washington\nR04: Dive into Deep Learning\nR05: Understanding Deep Learning\nR06: Neural Networks and Deep Learning\nR07: Deep Learning for Computer Vision NPTEL course by Prof. Vineeth N Balasubramanian",
    "crumbs": [
      "Lectures",
      "CNNs"
    ]
  },
  {
    "objectID": "lectures/L03.html",
    "href": "lectures/L03.html",
    "title": "KANs",
    "section": "",
    "text": "Materials:\nDate: TBD",
    "crumbs": [
      "Lectures",
      "KANs"
    ]
  },
  {
    "objectID": "lectures/L03.html#materials",
    "href": "lectures/L03.html#materials",
    "title": "KANs",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nFFNs\nCNNs\n\n\n\nIn-Class\n\nKAN: Kolmogorov-Arnold Networks\nImplementation PyKAN\nImplementation Efficient-KAN\nKAN 2.0: Kolmogorov-Arnold Networks Meet Science\n\n\n\nLab\n\nsimple functions (tbd)\nPDE solver (tbd)\nMLPS vs KAN (tbd)\n\n\n\nPost-class:\n\n[paper] Chebyshev KAN Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation. Implementation ChebyKAN\n[paper] Wav-KAN Wav-KAN: Wavelet Kolmogorov-Arnold Networks\n[paper] Survey of KANs A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)\n[git] KAN GPT\n[git] Fast KAN Replace B-splines with RBF to make it 3.3x faster than Efficient-KAN\n[paper]Kolmogorov-Arnold Convolutions: Design Principles and Empirical Studies | code\n[collection] awesome KANs - a collection of papers, codes, tutorials on KANs.\n\n\n\nPapers\n\nKAN vs MLP KAN or MLP: A Fairer Comparison\nKANs for PINNs Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks\n\n\n\nTheory\n\n1957-On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables : The original Kolmogorov Arnold paper\n1957-On functions of three variables\n2009-On a constructive proof of Kolmogorov’s superposition theorem\n2021-The Kolmogorov-Arnold representation theorem revisited\n2021-The Kolmogorov Superposition Theorem can Break the Curse of Dimension When Approximating High Dimensional Functions\n\n\n\nReferences\n\nR01: CS6910: Deep Learning Mitesh Khapra @ IIT Madras\nR02: CS7015: Deep Learning Mitesh Khapra @ IIT Madras) Mitesh Khapra @ IIT Madras (earlier version of CS6910)\nR03: LING 574: Deep Learning for NLP Shane Steinhard-Threlkeld @ University of Washington\nR04: Dive into Deep Learning\nR05: Understanding Deep Learning\nR06: Neural Networks and Deep Learning\nR07: Deep Learning for Computer Vision NPTEL course by Prof. Vineeth N Balasubramanian",
    "crumbs": [
      "Lectures",
      "KANs"
    ]
  }
]