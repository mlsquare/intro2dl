[
  {
    "objectID": "notebooks/00-01-FFN-Classification-Iris.html",
    "href": "notebooks/00-01-FFN-Classification-Iris.html",
    "title": "FFN Classification",
    "section": "",
    "text": "A notebook to apply an FFN (Feed Forward Neural Network) to classify the flower species type. We will use the the famous Iris dataset (which is now the equivalent of the hellow world dataset in the Data Science World)\nBased on - Kaggle Notebook for Iris Classiifcation - PyTorch for Iris Dataset - Iris Classification\nLoad, Visualize, Summarise Data\nsklearn comes with Iris dataset. We will load it, and do some basic visualization. It is always a good idea to “look” at the data before (blindly) running any models.\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots(figsize=(5,5))\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\nWe see that, 1. there are four features, and it is a three class classification problem 2. Using two features (sepal length, and sepal width), it is clear that, a perceptron will not be able separate _versicolor from virginica (data is not linearly separable) class. 3. But setosa can be separated from the remaining two.\nLet us look at the basic descriptions of the data.\n\n\nprint('feature name',iris.feature_names)\nprint('features type of data',type(iris.data))\nprint('features shape',iris.data.shape)\nprint('feature name',iris.target_names)\nprint('target type of data',type(iris.target))\nprint('target shape',iris.target.shape)\n\nfeature name ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nfeatures type of data &lt;class 'numpy.ndarray'&gt;\nfeatures shape (150, 4)\nfeature name ['setosa' 'versicolor' 'virginica']\ntarget type of data &lt;class 'numpy.ndarray'&gt;\ntarget shape (150,)\n\n\n\nprint('target labels',iris.target)\n\ntarget labels [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n\n\nHa. In the original dataset, the data is organized by class. If we naively prepare the mini batches (sequentially), model will only see data corresponding to only one class. This will be pretty problematic to get proper gradient signals. We should shuffle the data s.t diversity in the mini batches is maintained.\nQuestions\nImagine you split the data into two batches. One containing only say class 0, and other contains only class 1. During training, the model sees these two batches cyclically. Will the model ever converge.\n\nWill it converge when the data is linearly separable?\nWill it converge when the data is not linearly separable?\nDoes having a balanced class representation in every mini batch helps? Which way does it?\nWhat will be the impact of learning rate when alternating between sets of samples of one class during gradient descent?\n\nLet us get back to checking the data, this time, from huggingace datasets itself. Later down the line, it may be useful to learn how to work with datasets library from HuggingFace. It has deep integrations with PyTorch.\n\nfrom datasets import Dataset\nimport pandas as pd\ndf = pd.read_csv(\"hf://datasets/scikit-learn/iris/Iris.csv\")\ndf = pd.DataFrame(df)\ndf.head()\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\ndf['Species'].unique()\n\narray(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)\n\n\nInterestingly, the first column is ids., which is not useful for us. May be, a perfect system can simply memory the indices and spit out the correct classes.\nAnd we need to map the Iris types into numerical codes for models to work with. In the torch, we can supply integers representing the classes, and we do not have to explicitly pass one-hot coded labels.\n\n# transform species to numerics\ndf.loc[df.Species=='Iris-setosa', 'Target'] = 0\ndf.loc[df.Species=='Iris-versicolor', 'Target'] = 1\ndf.loc[df.Species=='Iris-virginica', 'Target'] = 2\nprint(df.Target.unique())\ndf.head()\n\n[0. 1. 2.]\n\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nTarget\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0.0\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0.0\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0.0\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0.0\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0.0\n\n\n\n\n\n\n\n\n# drop the Id columns from the dataframe\ndf.drop(['Id'],axis=1,inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nTarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0.0\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = df[df.columns[0:4]].values\ny = df.Target.values\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\n\n\n# dip test:  check that data is shuffled\nprint(y_train)\n\n[2. 1. 2. 2. 0. 0. 0. 2. 1. 2. 0. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 2. 0. 1.\n 1. 0. 1. 0. 1. 1.]\n\n\nQuestions\nAbove (visualluy inspecting data) is not a rigorous way (and repeatable way) to test if the data is shuffled (randomly). For numerical labels like integers, in the multi-class or binary class classification problems, which statistical test is suitable to flag if the data grouped?\n\n# scale the features to roughly have zero mean and unit variance\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nIt is always a good practice to scale the data (features).\n\nWhat might happen if the different features are on different scales?\nDoes it pose any problems for the optimizer (gradient descent)?\nDoes it cause any problems w.r.t interpretation of the feature importance?\n\nSuppose instead of\n\ncreate train, test splits\nlearn the scaling transformation on train data\nscale both train and test data\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nyou do the following\n\nlearn the scaling transformation on whole data before split\nand then create train, test splits\n\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\nWhat happens? Should we do this?\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)\n\nLet us define a FFN (or MLP) with two hidden layers. Suppose \\(x\\) is a \\(B \\times 4\\) vector, we have two hidden layers of 64 dimensions each, and we have three outputs (one for each class), then, \\[\nh_1 = ReLU(x W_{1} +b_{1}) \\\\\nh_2 = ReLU(h_1 W_{2} +b_{2}) \\\\\ny =  h_2 W_{out} + b_{out} \\\\\n\\]\nwhere \\[\nx \\text{ is } B \\times 4 \\\\\nW_{1} \\text{ is } 4 \\times 64 \\\\\nW_{2} \\text{ is } 64 \\times 64 \\\\\nW_{out} \\text{ is } 64 \\times 3 \\\\\nb_{1} \\text{ is } 1 \\times 64 \\\\\nb_{2} \\text{ is } 1 \\times 64 \\\\\nb_{out} \\text{ is } 1 \\times 3 \\\\\ny \\text{ is } B \\times 3 \\\\\n\\]\nIn $xW +b $, \\(b\\) is broadcast over all rows and \\(B\\) is the batch size.\n\\(ReLU(x) = x \\text{ if } x \\ge 0 \\text{ and } 0 \\text{ o.w }\\)\nQuestion\nWhat is the total number of parameters if input dimension is \\(p^{in}\\), output dimension is \\(p^{out}\\) and each hidden layer is of size \\(p^{h}_{i}\\) for the i-th hidden layer and there \\(d\\) such layers?\n\nclass MLP(nn.Module):\n    # define nn\n    def __init__(self, input_dim=4, output_dim=3, hidden_dim = [128,64]):\n        super(MLP, self).__init__()\n        self.h1 = nn.Linear(input_dim, hidden_dim[0])\n        self.h2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.out = nn.Linear(hidden_dim[1], output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        X = self.relu(self.h1(X))\n        X = self.relu(self.h2(X))\n        X = self.out(X)\n        return X\n\nWe have built a Neural Network with one input layer, two hidden layers, and one output layer.\nNote, the last output layer is a linear layer. Even though we are modeling a 3-class problem, output layer is still linear, and not softmax. Is this fine?\n\ninput_dim = 4 # No. of features\noutput_dim = 3 # No. of outputs\nhidden_dim = [64, 64] # No. of perceptrons in 1st hidden layer and 2nd hidden layer\nmodel = MLP(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim) # instantiate the model\n\n\n# inspect the model for a given batch size\nfrom torchinfo import summary\nsummary(model, input_size=(10, 4))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [10, 3]                   --\n├─Linear: 1-1                            [10, 64]                  320\n├─ReLU: 1-2                              [10, 64]                  --\n├─Linear: 1-3                            [10, 64]                  4,160\n├─ReLU: 1-4                              [10, 64]                  --\n├─Linear: 1-5                            [10, 3]                   195\n==========================================================================================\nTotal params: 4,675\nTrainable params: 4,675\nNon-trainable params: 0\nTotal mult-adds (M): 0.05\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.02\nEstimated Total Size (MB): 0.03\n==========================================================================================\n\n\n\nlearning_rate = 0.01\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n\nIn the above train_network block, we have not used batches. Entire train data is passed at once. So, one epoch is one complete pass through the data.\nExercise\nModify the training loop to pass over mini batches.\n\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\n\n\ntrain_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 0.0001, Test Loss: 0.1433\nEpoch 100/1000, Train Loss: 0.0000, Test Loss: 0.1390\nEpoch 150/1000, Train Loss: 0.0000, Test Loss: 0.1403\nEpoch 200/1000, Train Loss: 0.0000, Test Loss: 0.1430\nEpoch 250/1000, Train Loss: 0.0000, Test Loss: 0.1450\nEpoch 300/1000, Train Loss: 0.0000, Test Loss: 0.1467\nEpoch 350/1000, Train Loss: 0.0000, Test Loss: 0.1484\nEpoch 400/1000, Train Loss: 0.0000, Test Loss: 0.1500\nEpoch 450/1000, Train Loss: 0.0000, Test Loss: 0.1514\nEpoch 500/1000, Train Loss: 0.0000, Test Loss: 0.1525\nEpoch 550/1000, Train Loss: 0.0000, Test Loss: 0.1533\nEpoch 600/1000, Train Loss: 0.0000, Test Loss: 0.1540\nEpoch 650/1000, Train Loss: 0.0000, Test Loss: 0.1547\nEpoch 700/1000, Train Loss: 0.0000, Test Loss: 0.1554\nEpoch 750/1000, Train Loss: 0.0000, Test Loss: 0.1562\nEpoch 800/1000, Train Loss: 0.0000, Test Loss: 0.1569\nEpoch 850/1000, Train Loss: 0.0000, Test Loss: 0.1577\nEpoch 900/1000, Train Loss: 0.0000, Test Loss: 0.1584\nEpoch 950/1000, Train Loss: 0.0000, Test Loss: 0.1592\nEpoch 1000/1000, Train Loss: 0.0000, Test Loss: 0.1598\n\n\n\nplt.figure(figsize=(4,4))\nplt.plot(train_losses, label='train loss')\nplt.plot(test_losses, label='test loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\npredictions_train = []\npredictions_test =  []\nwith torch.no_grad():\n    predictions_train = model(X_train)\n    predictions_test = model(X_test)\n\n\nprint(predictions_train.shape)\nprint(type(predictions_train))\n\nprint(y_train.shape)\nprint(type(y_train))\n\ntorch.Size([30, 3])\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([30])\n&lt;class 'torch.Tensor'&gt;\n\n\n\ndef get_accuracy_multiclass(pred_arr,original_arr):\n    if len(pred_arr)!=len(original_arr):\n        return False\n    pred_arr = pred_arr.numpy()\n    original_arr = original_arr.numpy()\n    final_pred= []\n    # we will get something like this in the pred_arr [32.1680,12.9350,-58.4877]\n    # so will be taking the index of that argument which has the highest value here 32.1680 which corresponds to 0th index\n    for i in range(len(pred_arr)):\n        final_pred.append(np.argmax(pred_arr[i]))\n    final_pred = np.array(final_pred)\n    count = 0\n    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy\n    for i in range(len(original_arr)):\n        if final_pred[i] == original_arr[i]:\n            count+=1\n    return count/len(final_pred)\n\nNotice that the model predictions were of size (batch_size, output_dim) and we have to take argmax of the model predictions to produce the class labels. The predictions are in the logit space (recall that the output layer is linear and not softmax).\n\ntrain_acc = get_accuracy_multiclass(predictions_train,y_train)\ntest_acc  = get_accuracy_multiclass(predictions_test,y_test)\nprint(f\"Training Accuracy: {round(train_acc*100,3)}\")\nprint(f\"Test Accuracy: {round(test_acc*100,3)}\")\n\nTraining Accuracy: 100.0\nTest Accuracy: 95.0",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>FFN Classification</span>"
    ]
  },
  {
    "objectID": "notebooks/00-02-FFN-Regression-Friedman2.html",
    "href": "notebooks/00-02-FFN-Regression-Friedman2.html",
    "title": "FFNs Regression",
    "section": "",
    "text": "A notebook to apply an FFN (Feed Forward Neural Network) to regress. We will use Friedman2 dataset from sklearn\nBased on - Kaggle Notebook for Iris Classiifcation - PyTorch for Iris Dataset - Iris Classification\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_friedman2\nX, y = make_friedman2(n_samples = 200, random_state=42, noise=0.5)\nprint(X.shape)\nprint(y.shape)\n\n(200, 4)\n(200,)\n\n\n\nplt.figure(figsize=(5,5))\nplt.scatter(X[:,0],y)\nplt.show()\nplt.hist(y)\n\n\n\n\n\n\n\n\n(array([64., 29., 30., 19., 23., 20.,  5.,  2.,  5.,  3.]),\n array([  13.8686335 ,  177.60233879,  341.33604407,  505.06974936,\n         668.80345464,  832.53715992,  996.27086521, 1160.00457049,\n        1323.73827578, 1487.47198106, 1651.20568635]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n# split and scale.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# Friedman2 response variable has huge dynamic range. The maximum value seems above above 1500.\nscaler = StandardScaler()\n\nprint(y_train.shape)\ny_tmp = y_train.reshape(-1, 1)\ny_train = scaler.fit_transform(y_tmp)\nprint(y_train.shape)\n\ny_tmp = y_test.reshape(-1, 1)\ny_test = scaler.transform(y_tmp)\nprint(y_test.shape)\n\n(160,)\n(160, 1)\n(40, 1)\n\n\nExercise\nRun the regression without scaling the response variable! What will be expected?\n\n# load the libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\n# convert numpy arrays into torch tensors\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.FloatTensor(y_train)\ny_test = torch.FloatTensor(y_test)\n\n\n# define the model.\n# the model is exactly the same as the model we saw in earlier\nclass MLP(nn.Module):\n    # define nn\n    def __init__(self, input_dim=4, output_dim=3, hidden_dim = [128,64]):\n        super(MLP, self).__init__()\n        self.input = nn.Linear(input_dim, hidden_dim[0])\n        self.hidden = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.out = nn.Linear(hidden_dim[1], output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        X = self.relu(self.input(X))\n        X = self.relu(self.hidden(X))\n        X = self.out(X)\n        return X\n\n\ninput_dim = 4\noutput_dim = 1\nhidden_dim = [64, 64]\nmodel = MLP(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim)\n\n\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\nSince it is a regression problem, we have used MSE loss. Practically, this is the only change we have to make so far. And of course, how to evaluate the model perdictions has to change!\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n\n\nimport numpy as np\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\ntrain_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 0.0040, Test Loss: 0.0068\nEpoch 100/1000, Train Loss: 0.0010, Test Loss: 0.0037\nEpoch 150/1000, Train Loss: 0.0005, Test Loss: 0.0032\nEpoch 200/1000, Train Loss: 0.0003, Test Loss: 0.0028\nEpoch 250/1000, Train Loss: 0.0002, Test Loss: 0.0026\nEpoch 300/1000, Train Loss: 0.0001, Test Loss: 0.0025\nEpoch 350/1000, Train Loss: 0.0001, Test Loss: 0.0025\nEpoch 400/1000, Train Loss: 0.0001, Test Loss: 0.0024\nEpoch 450/1000, Train Loss: 0.0016, Test Loss: 0.0037\nEpoch 500/1000, Train Loss: 0.0000, Test Loss: 0.0024\nEpoch 550/1000, Train Loss: 0.0000, Test Loss: 0.0023\nEpoch 600/1000, Train Loss: 0.0008, Test Loss: 0.0030\nEpoch 650/1000, Train Loss: 0.0000, Test Loss: 0.0023\nEpoch 700/1000, Train Loss: 0.0000, Test Loss: 0.0022\nEpoch 750/1000, Train Loss: 0.0000, Test Loss: 0.0022\nEpoch 800/1000, Train Loss: 0.0002, Test Loss: 0.0024\nEpoch 850/1000, Train Loss: 0.0000, Test Loss: 0.0022\nEpoch 900/1000, Train Loss: 0.0000, Test Loss: 0.0022\nEpoch 950/1000, Train Loss: 0.0000, Test Loss: 0.0021\nEpoch 1000/1000, Train Loss: 0.0000, Test Loss: 0.0024\n\n\n\nplt.figure(figsize=(5,5))\nplt.plot(train_losses, label='train loss')\nplt.plot(test_losses, label='test loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\npredictions_train = []\npredictions_test =  []\nwith torch.no_grad():\n    predictions_train = model(X_train)\n    predictions_test = model(X_test)\n\nprint(predictions_train.shape)\nprint(type(predictions_train))\n\nprint(y_train.shape)\nprint(type(y_train))\n\ntorch.Size([160, 1])\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([160, 1])\n&lt;class 'torch.Tensor'&gt;\n\n\n\nyt = y_test.numpy()\nprint(type(yt))\nprint(yt.shape)\n\nyh = predictions_test.numpy()\nprint(type(yh))\nprint(yh.shape)\n\n&lt;class 'numpy.ndarray'&gt;\n(40, 1)\n&lt;class 'numpy.ndarray'&gt;\n(40, 1)\n\n\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import root_mean_squared_error as rmse\n\nprint('mse is: ', mse(yt, yh))\nprint('rmse is: ', rmse(yt, yh))\n\nmse is:  0.00236044\nrmse is:  0.04858436\n\n\n\nplt.scatter(yt,yh)\n\n\n\n\n\n\n\n\n\nresiduals = yt-yh\nplt.hist(residuals)\nplt.show()\n\nimport pandas as pd\npd.DataFrame(residuals).plot(kind='density')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompared to FFNs with Classification, for regression, we used MSE loss, keeping all else the same (code wise). While making predictions, we just used the outputs of the model as is.",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFNs Regression</span>"
    ]
  },
  {
    "objectID": "notebooks/02-01-KAN-Intro.html",
    "href": "notebooks/02-01-KAN-Intro.html",
    "title": "KAN Intro",
    "section": "",
    "text": "We will use example provided in the official implementation of the KAN paper. Install the library with\npip install pykan\nFunction Fitting\nWe will take Example-1 provided in the PyKAN repo.\nThe function is \\[\nf(x) = \\sin(\\pi x_1) + x_2^2 \\\\\nx_1, x_2 \\sim U[-1,1]\n\\]\n\nimport torch\nimport matplotlib.pyplot as plt\n\ntorch.set_default_dtype(torch.float64)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device is: ',device)\n\nfrom kan.utils import create_dataset\nf = lambda x: torch.exp(torch.sin(torch.pi*x[:,[0]]) + x[:,[1]]**2)\ndataset = create_dataset(f, n_var=2, device=device)\n\nprint('train input data shape', dataset['train_input'].shape)\nprint('train label data shape', dataset['train_label'].shape)\n\ndevice is:  cpu\ntrain input data shape torch.Size([1000, 2])\ntrain label data shape torch.Size([1000, 1])\n\n\n\n# create a KAN: 2D inputs, 1D output, and 5 hidden neurons. cubic spline (k=3), 5 grid intervals (grid=5).\nfrom kan import KAN\nmodel = KAN(width=[2,5,1], grid=3, k=3, seed=42, device=device)\n\ncheckpoint directory created: ./model\nsaving model version 0.0\n\n\n\n# plot KAN at initialization\nmodel(dataset['train_input']);\nmodel.plot()\n\n\n\n\n\n\n\n\n\n# train the model\nmodel.fit(dataset, opt=\"LBFGS\", steps=50, lamb=0.001);\n\n| train_loss: 1.88e-02 | test_loss: 1.78e-02 | reg: 6.80e+00 | : 100%|█| 50/50 [00:13&lt;00:00,  3.79it\n\n\nsaving model version 0.1\n\n\n\n\n\n\n# visualise the trained model\nmodel.plot()\n\n\n\n\n\n\n\n\n\n# prune the model, and revisualize\nmodel = model.prune()\nmodel.plot()\n\nsaving model version 0.2\n\n\n\n\n\n\n\n\n\n\n# contnue retraining based on the pruned model\nmodel.fit(dataset, opt=\"LBFGS\", steps=50);\n\n| train_loss: 1.79e-02 | test_loss: 1.72e-02 | reg: 7.66e+00 | : 100%|█| 50/50 [00:03&lt;00:00, 13.81it\n\n\nsaving model version 0.3\n\n\n\n\n\n\nmode = \"auto\" # \"manual\"\n\nif mode == \"manual\":\n    # manual mode\n    model.fix_symbolic(0,0,0,'sin');\n    model.fix_symbolic(0,1,0,'x^2');\n    model.fix_symbolic(1,0,0,'exp');\nelif mode == \"auto\":\n    # automatic mode\n    lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']\n    model.auto_symbolic(lib=lib)\n\nfixing (0,0,0) with sin, r2=0.9998313640557426, c=2\nfixing (0,1,0) with x^2, r2=0.9999907195889106, c=2\nfixing (1,0,0) with exp, r2=0.9999810964976568, c=2\nsaving model version 0.4\n\n\n\n# continue retrained and fit the symbolic functions that were found earlier\nmodel.fit(dataset, opt=\"LBFGS\", steps=50);\n\n| train_loss: 9.43e-12 | test_loss: 5.75e-12 | reg: 0.00e+00 | : 100%|█| 50/50 [00:01&lt;00:00, 27.50it\n\n\nsaving model version 0.5\n\n\n\n\n\n\n# plot the symbolic model\nfrom kan.utils import ex_round\nex_round(model.symbolic_formula()[0][0],4)\n\n\\(\\displaystyle 1.0 e^{1.0 x_{2}^{2} + 1.0 \\sin{\\left(3.1416 x_{1} \\right)}}\\)\n\n\n\nX = dataset['train_input']\nn  = 1000\nX[:,0] = torch.linspace(-1,1,steps=n)\nX[:,1] = torch.zeros(n)\ny = f(X)\ny = y[:,0].detach().numpy()\nyh = model.forward(X)\n\nyh = yh[:,0].detach().numpy()\nplt.plot(y,color='blue')\nplt.plot(yh, color='red')\n\n\n\n\n\n\n\n\n\nplt.plot(y-yh)\nplt.ylim(-1,1)\n\n\n\n\n\n\n\n\nIndeed, the correct function was recovered, exactly. This would have been very hard for an MLP. Let us how a off-the-shelf (meaning no tuning) MLP will work out of the box. We will copy an MLP from this regression notebook.\n\nimport torch.nn as nn\n\n# define the model.\n# the model is exactly the same as the model we saw in earlier\nclass MLP(nn.Module):\n    # define nn\n    def __init__(self, input_dim=4, output_dim=3, hidden_dim = [128,64]):\n        super(MLP, self).__init__()\n        self.input = nn.Linear(input_dim, hidden_dim[0])\n        self.hidden = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.out = nn.Linear(hidden_dim[1], output_dim)\n        self.relu = nn.ReLU()\n        self.silu = nn.SiLU()\n\n    def forward(self, X):\n        X = self.silu(self.input(X))\n        X = self.silu(self.hidden(X))\n        X = self.out(X)\n        return X\n\n\ninput_dim = 2\noutput_dim = 1\nhidden_dim = [64, 64]\nmlp = MLP(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim)\n\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(mlp.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        model.train()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        model.eval()\n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n    return model, train_losses, test_losses\n\n\n\nimport numpy as np\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\nX_train = dataset['train_input']\ny_train = dataset['train_label']\nX_test = dataset['test_input']\ny_test = dataset['test_label']\nmlp, train_losses, test_losses = train_network(mlp,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 2.0488, Test Loss: 1.8635\nEpoch 100/1000, Train Loss: 2.0481, Test Loss: 1.8814\nEpoch 150/1000, Train Loss: 2.0478, Test Loss: 1.8812\nEpoch 200/1000, Train Loss: 2.0476, Test Loss: 1.8800\nEpoch 250/1000, Train Loss: 2.0474, Test Loss: 1.8780\nEpoch 300/1000, Train Loss: 2.0468, Test Loss: 1.8737\nEpoch 350/1000, Train Loss: 2.0453, Test Loss: 1.8607\nEpoch 400/1000, Train Loss: 2.0437, Test Loss: 1.8382\nEpoch 450/1000, Train Loss: 2.0425, Test Loss: 1.8111\nEpoch 500/1000, Train Loss: 2.0418, Test Loss: 1.7928\nEpoch 550/1000, Train Loss: 2.0415, Test Loss: 1.7889\nEpoch 600/1000, Train Loss: 2.0413, Test Loss: 1.7895\nEpoch 650/1000, Train Loss: 2.0411, Test Loss: 1.7912\nEpoch 700/1000, Train Loss: 2.0411, Test Loss: 1.7929\nEpoch 750/1000, Train Loss: 2.0410, Test Loss: 1.7938\nEpoch 800/1000, Train Loss: 2.0410, Test Loss: 1.7942\nEpoch 850/1000, Train Loss: 2.0409, Test Loss: 1.7945\nEpoch 900/1000, Train Loss: 2.0408, Test Loss: 1.7956\nEpoch 950/1000, Train Loss: 2.0406, Test Loss: 1.7987\nEpoch 1000/1000, Train Loss: 2.0403, Test Loss: 1.8046\n\n\n\nn = len(X_train)\nX[:,0] = torch.linspace(-1,1,steps=n)\nX[:,1] = torch.zeros(n)\ny = f(X)\ny = y[:,0].detach().numpy()\nyh = mlp(X)\n\nyh = yh[:,0].detach().numpy()\nplt.plot(y,color='blue')\nplt.plot(yh, color='red')\n\n\n\n\n\n\n\n\nThe MLP has hard time learning this function. Architectures search and tuning hyperparams is needed.",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>KAN Intro</span>"
    ]
  },
  {
    "objectID": "notebooks/02-02-KAN-Splines.html",
    "href": "notebooks/02-02-KAN-Splines.html",
    "title": "KAN with Splines",
    "section": "",
    "text": "We will use example provided in the official implementation of the KAN paper. Install the library with\npip install pykan\nFunction Fitting We will take some interesting test functions from Wavelet Regression notebook.\nThe Doppler function is \\[\nf(x) = \\sqrt{x(1-x)} \\sin(\\frac{2.1\\pi}{x+0.05}) \\\\\nx \\sim U[0,1]\n\\]\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom kan.utils import create_dataset\n\ntorch.set_default_dtype(torch.float64)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device is: ',device)\n\nf = lambda x: torch.sqrt(x[:,[0]]*(1-x[:,[0]]))*torch.sin((2*np.pi)/(x[:,[0]]+.1))\n\ndataset = create_dataset(f, n_var=1, device=device, ranges=[0,1])\n\nprint('train input data shape', dataset['train_input'].shape)\nprint('train label data shape', dataset['train_label'].shape)\n\nplt.scatter(dataset['train_input'],dataset['train_label'])\n\ndevice is:  cpu\ntrain input data shape torch.Size([1000, 1])\ntrain label data shape torch.Size([1000, 1])\n\n\n\n\n\n\n\n\n\n\n# create a KAN: 1D inputs, 1D output, and 5 hidden neurons. cubic spline (k=3), 5 grid intervals (grid=5).\nfrom kan import KAN\nmodel = KAN(width=[1,3,1], grid=5, k=3, seed=0, device=device)\n\n# plot KAN at initialization\nmodel(dataset['train_input'])\nmodel.plot()\n\n# train the model\nmodel.fit(dataset, opt=\"LBFGS\", steps=50, lamb=0.001);\n\ncheckpoint directory created: ./model\nsaving model version 0.0\n\n\n| train_loss: 8.76e-02 | test_loss: 9.21e-02 | reg: 7.97e+00 | : 100%|█| 50/50 [00:09&lt;00:00,  5.53it\n\n\nsaving model version 0.1\n\n\n\n\n\n\n\n\n\n\n# let us look at the recontruction\nX = dataset['train_input']\nn  = 1000\nX[:,0] = torch.linspace(0,1,steps=n)\ny = f(X)\ny = y[:,0].detach().numpy()\nyh = model.forward(X)\n\nyh = yh[:,0].detach().numpy()\nplt.plot(y,color='blue')\nplt.plot(yh, color='red')\nplt.show()\nplt.plot(yh-y, color='black')\nplt.ylim(-1,1)\nplt.show()",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>KAN with Splines</span>"
    ]
  },
  {
    "objectID": "notebooks/02-03-KAN-RBFs.html",
    "href": "notebooks/02-03-KAN-RBFs.html",
    "title": "KANs with RBFs",
    "section": "",
    "text": "Replace B-Splines with Radial Basis Functions. They should be faster to compute. Following base code is taken from fast-KAN\n\n# Copyright 2024 Li, Ziyao\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom typing import *\n\nclass SplineLinear(nn.Linear):\n    def __init__(self, in_features: int, out_features: int, init_scale: float = 0.1, **kw) -&gt; None:\n        self.init_scale = init_scale\n        super().__init__(in_features, out_features, bias=False, **kw)\n\n    def reset_parameters(self) -&gt; None:\n        nn.init.trunc_normal_(self.weight, mean=0, std=self.init_scale)\n\nclass RadialBasisFunction(nn.Module):\n    def __init__(\n        self,\n        grid_min: float = -2.,\n        grid_max: float = 2.,\n        num_grids: int = 8,\n        denominator: float = None,  # larger denominators lead to smoother basis\n    ):\n        super().__init__()\n        self.grid_min = grid_min\n        self.grid_max = grid_max\n        self.num_grids = num_grids\n        grid = torch.linspace(grid_min, grid_max, num_grids)\n        self.grid = torch.nn.Parameter(grid, requires_grad=False)\n        self.denominator = denominator or (grid_max - grid_min) / (num_grids - 1)\n\n    def forward(self, x):\n        return torch.exp(-((x[..., None] - self.grid) / self.denominator) ** 2)\n\nclass FastKANLayer(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        grid_min: float = -2.,\n        grid_max: float = 2.,\n        num_grids: int = 8,\n        use_base_update: bool = True,\n        use_layernorm: bool = False,\n        base_activation = F.silu,\n        spline_weight_init_scale: float = 0.1,\n    ) -&gt; None:\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.layernorm = None\n        if use_layernorm:\n            assert input_dim &gt; 1, \"Do not use layernorms on 1D inputs. Set `use_layernorm=False`.\"\n            self.layernorm = nn.LayerNorm(input_dim)\n        self.rbf = RadialBasisFunction(grid_min, grid_max, num_grids)\n        self.spline_linear = SplineLinear(input_dim * num_grids, output_dim, spline_weight_init_scale)\n        self.use_base_update = use_base_update\n        if use_base_update:\n            self.base_activation = base_activation\n            self.base_linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x, use_layernorm=True):\n        if self.layernorm is not None and use_layernorm:\n            spline_basis = self.rbf(self.layernorm(x))\n        else:\n            spline_basis = self.rbf(x)\n        ret = self.spline_linear(spline_basis.view(*spline_basis.shape[:-2], -1))\n        if self.use_base_update:\n            base = self.base_linear(self.base_activation(x))\n            ret = ret + base\n        return ret\n\n    def plot_curve(\n        self,\n        input_index: int,\n        output_index: int,\n        num_pts: int = 1000,\n        num_extrapolate_bins: int = 2\n    ):\n        '''this function returns the learned curves in a FastKANLayer.\n        input_index: the selected index of the input, in [0, input_dim) .\n        output_index: the selected index of the output, in [0, output_dim) .\n        num_pts: num of points sampled for the curve.\n        num_extrapolate_bins (N_e): num of bins extrapolating from the given grids. The curve \n            will be calculate in the range of [grid_min - h * N_e, grid_max + h * N_e].\n        '''\n        ng = self.rbf.num_grids\n        h = self.rbf.denominator\n        assert input_index &lt; self.input_dim\n        assert output_index &lt; self.output_dim\n        w = self.spline_linear.weight[\n            output_index, input_index * ng : (input_index + 1) * ng\n        ]   # num_grids,\n        x = torch.linspace(\n            self.rbf.grid_min - num_extrapolate_bins * h,\n            self.rbf.grid_max + num_extrapolate_bins * h,\n            num_pts\n        )   # num_pts, num_grids\n        with torch.no_grad():\n            y = (w * self.rbf(x.to(w.dtype))).sum(-1)\n        return x, y\n\n\nclass FastKAN(nn.Module):\n    def __init__(\n        self,\n        layers_hidden: List[int],\n        grid_min: float = -2.,\n        grid_max: float = 2.,\n        num_grids: int = 8,\n        use_base_update: bool = True,\n        base_activation = F.silu,\n        spline_weight_init_scale: float = 0.1,\n    ) -&gt; None:\n        super().__init__()\n        self.layers = nn.ModuleList([\n            FastKANLayer(\n                in_dim, out_dim,\n                grid_min=grid_min,\n                grid_max=grid_max,\n                num_grids=num_grids,\n                use_base_update=use_base_update,\n                base_activation=base_activation,\n                spline_weight_init_scale=spline_weight_init_scale,\n            ) for in_dim, out_dim in zip(layers_hidden[:-1], layers_hidden[1:])\n        ])\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nFunction Fitting\nWe will take some interesting test functions from Wavelet Regression notebook.\nThe Doppler function is \\[\nf(x) = x(1-x) \\sin(\\frac{2.1\\pi}{x+0.05}) \\\\\nx \\sim U[0,1]\n\\]\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom kan.utils import create_dataset\n\ntorch.set_default_dtype(torch.float64)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device is: ',device)\n\nf = lambda x: x[:,[0]]*(1-x[:,[0]])*torch.sin((2*np.pi)/(x[:,[0]]+.1))\n\ndataset = create_dataset(f, n_var=1, device=device, ranges=[0,1])\n\nprint('train input data shape', dataset['train_input'].shape)\nprint('train label data shape', dataset['train_label'].shape)\n\nplt.scatter(dataset['train_input'],dataset['train_label'])\n\ndevice is:  cpu\ntrain input data shape torch.Size([1000, 1])\ntrain label data shape torch.Size([1000, 1])\n\n\n\n\n\n\n\n\n\n\n# Define model\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# create a KAN: 1D inputs, 1D output, and 5 hidden neurons.\nmodel =FastKAN([1,1,5,5],grid_min=0,grid_max=1,num_grids=5)\nmodel.to(device)\n\n# Define loss\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        model.train()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        model.eval()\n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n    return model, train_losses, test_losses\n\n\n\nimport numpy as np\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\nX_train = dataset['train_input']\ny_train = dataset['train_label']\nX_test = dataset['test_input']\ny_test = dataset['test_label']\nmlp, train_losses, test_losses = train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\n/opt/miniconda3/envs/ai839/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1000, 1])) that is different to the input size (torch.Size([1000, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n\n\nEpoch 50/1000, Train Loss: 0.0138, Test Loss: 0.0137\nEpoch 100/1000, Train Loss: 0.0074, Test Loss: 0.0070\nEpoch 150/1000, Train Loss: 0.0062, Test Loss: 0.0059\nEpoch 200/1000, Train Loss: 0.0060, Test Loss: 0.0057\nEpoch 250/1000, Train Loss: 0.0059, Test Loss: 0.0057\nEpoch 300/1000, Train Loss: 0.0058, Test Loss: 0.0055\nEpoch 350/1000, Train Loss: 0.0058, Test Loss: 0.0054\nEpoch 400/1000, Train Loss: 0.0058, Test Loss: 0.0054\nEpoch 450/1000, Train Loss: 0.0057, Test Loss: 0.0053\nEpoch 500/1000, Train Loss: 0.0057, Test Loss: 0.0053\nEpoch 550/1000, Train Loss: 0.0057, Test Loss: 0.0053\nEpoch 600/1000, Train Loss: 0.0056, Test Loss: 0.0052\nEpoch 650/1000, Train Loss: 0.0057, Test Loss: 0.0053\nEpoch 700/1000, Train Loss: 0.0059, Test Loss: 0.0053\nEpoch 750/1000, Train Loss: 0.0056, Test Loss: 0.0052\nEpoch 800/1000, Train Loss: 0.0055, Test Loss: 0.0052\nEpoch 850/1000, Train Loss: 0.0055, Test Loss: 0.0051\nEpoch 900/1000, Train Loss: 0.0054, Test Loss: 0.0050\nEpoch 950/1000, Train Loss: 0.0055, Test Loss: 0.0050\nEpoch 1000/1000, Train Loss: 0.0055, Test Loss: 0.0051\n\n\n\n# let us look at the recontruction\nX = dataset['train_input']\nn  = 1000\nX[:,0] = torch.linspace(0,1,steps=n)\ny = f(X)\ny = y[:,0].detach().numpy()\nyh = model.forward(X)\n\nyh = yh[:,0].detach().numpy()\nplt.plot(y,color='blue')\nplt.plot(yh, color='red')\nplt.show()\nplt.plot(yh-y, color='black')\nplt.ylim(-1,1)\nplt.show()",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>KANs with RBFs</span>"
    ]
  },
  {
    "objectID": "notebooks/02-04-KAN-Chebyshev.html",
    "href": "notebooks/02-04-KAN-Chebyshev.html",
    "title": "KANs with Chebyshev",
    "section": "",
    "text": "Replace B-Splines with Radial Basis Functions. They should be faster to compute. Following base code is taken from fast-KAN\n\nimport torch\nimport torch.nn as nn\n\n\n# This is inspired by Kolmogorov-Arnold Networks but using Chebyshev polynomials instead of splines coefficients\nclass ChebyKANLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, degree):\n        super(ChebyKANLayer, self).__init__()\n        self.inputdim = input_dim\n        self.outdim = output_dim\n        self.degree = degree\n\n        self.cheby_coeffs = nn.Parameter(torch.empty(input_dim, output_dim, degree + 1))\n        nn.init.normal_(self.cheby_coeffs, mean=0.0, std=1 / (input_dim * (degree + 1)))\n        self.register_buffer(\"arange\", torch.arange(0, degree + 1, 1))\n\n    def forward(self, x):\n        # Since Chebyshev polynomial is defined in [-1, 1]\n        # We need to normalize x to [-1, 1] using tanh\n        x = torch.tanh(x)\n        # View and repeat input degree + 1 times\n        x = x.view((-1, self.inputdim, 1)).expand(\n            -1, -1, self.degree + 1\n        )  # shape = (batch_size, inputdim, self.degree + 1)\n        # Apply acos\n        x = x.acos()\n        # Multiply by arange [0 .. degree]\n        x *= self.arange\n        # Apply cos\n        x = x.cos()\n        # Compute the Chebyshev interpolation\n        y = torch.einsum(\n            \"bid,iod-&gt;bo\", x, self.cheby_coeffs\n        )  # shape = (batch_size, outdim)\n        y = y.view(-1, self.outdim)\n        return y\n\nFunction Fitting\nWe will take some interesting test functions from Wavelet Regression notebook.\nThe Doppler function is \\[\nf(x) = x(1-x) \\sin(\\frac{2.1\\pi}{x+0.05}) \\\\\nx \\sim U[0,1]\n\\]\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom kan.utils import create_dataset\n\ntorch.set_default_dtype(torch.float64)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device is: ',device)\n\nf = lambda x: x[:,[0]]*(1-x[:,[0]])*torch.sin((2*np.pi)/(x[:,[0]]+.1))\n\ndataset = create_dataset(f, n_var=1, device=device, ranges=[0,1])\n\nprint('train input data shape', dataset['train_input'].shape)\nprint('train label data shape', dataset['train_label'].shape)\n\nplt.scatter(dataset['train_input'],dataset['train_label'])\n\ndevice is:  cpu\ntrain input data shape torch.Size([1000, 1])\ntrain label data shape torch.Size([1000, 1])\n\n\n\n\n\n\n\n\n\n\n# Define model\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nclass ChebyKAN(nn.Module):\n    def __init__(self):\n        super(ChebyKAN, self).__init__()\n        self.chebykan1 = ChebyKANLayer(1, 5, 4)\n        self.ln1 = nn.LayerNorm(5) # To avoid gradient vanishing caused by tanh\n        self.chebykan2 = ChebyKANLayer(5, 5, 4)\n        self.ln2 = nn.LayerNorm(5)\n        self.chebykan3 = ChebyKANLayer(5, 1, 4)\n\n    def forward(self, x):\n        x = self.chebykan1(x)\n        x = self.ln1(x)\n        x = self.chebykan2(x)\n        x = self.ln2(x)\n        x = self.chebykan3(x)\n        return x\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# create a KAN: 1D inputs, 1D output, and 5 hidden neurons.\nmodel =ChebyKAN()\nmodel.to(device)\n\n# Define loss\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        model.train()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        model.eval()\n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n    return model, train_losses, test_losses\n\n\n\nimport numpy as np\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\nX_train = dataset['train_input']\ny_train = dataset['train_label']\nX_test = dataset['test_input']\ny_test = dataset['test_label']\nmlp, train_losses, test_losses = train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 0.0079, Test Loss: 0.0079\nEpoch 100/1000, Train Loss: 0.0049, Test Loss: 0.0046\nEpoch 150/1000, Train Loss: 0.0041, Test Loss: 0.0037\nEpoch 200/1000, Train Loss: 0.0027, Test Loss: 0.0025\nEpoch 250/1000, Train Loss: 0.0018, Test Loss: 0.0016\nEpoch 300/1000, Train Loss: 0.0014, Test Loss: 0.0014\nEpoch 350/1000, Train Loss: 0.0085, Test Loss: 0.0093\nEpoch 400/1000, Train Loss: 0.0013, Test Loss: 0.0013\nEpoch 450/1000, Train Loss: 0.0007, Test Loss: 0.0008\nEpoch 500/1000, Train Loss: 0.0005, Test Loss: 0.0005\nEpoch 550/1000, Train Loss: 0.0004, Test Loss: 0.0005\nEpoch 600/1000, Train Loss: 0.0006, Test Loss: 0.0006\nEpoch 650/1000, Train Loss: 0.0002, Test Loss: 0.0002\nEpoch 700/1000, Train Loss: 0.0002, Test Loss: 0.0003\nEpoch 750/1000, Train Loss: 0.0001, Test Loss: 0.0001\nEpoch 800/1000, Train Loss: 0.0001, Test Loss: 0.0002\nEpoch 850/1000, Train Loss: 0.0001, Test Loss: 0.0001\nEpoch 900/1000, Train Loss: 0.0001, Test Loss: 0.0001\nEpoch 950/1000, Train Loss: 0.0001, Test Loss: 0.0001\nEpoch 1000/1000, Train Loss: 0.0003, Test Loss: 0.0003\n\n\n\n# let us look at the recontruction\nX = dataset['train_input']\nn  = 1000\nX[:,0] = torch.linspace(0,1,steps=n)\ny = f(X)\ny = y[:,0].detach().numpy()\nyh = model.forward(X)\n\nyh = yh[:,0].detach().numpy()\nplt.plot(y,color='blue')\nplt.plot(yh, color='red')\nplt.show()\nplt.plot(yh-y, color='black')\nplt.ylim(-1,1)\nplt.show()",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>KANs with Chebyshev</span>"
    ]
  },
  {
    "objectID": "notebooks/02-05-KAN-Wavelets.html",
    "href": "notebooks/02-05-KAN-Wavelets.html",
    "title": "KANs with Wavelets",
    "section": "",
    "text": "Replace B-Splines with Wavelets Following base code is taken from WavKAN and this notebooks on Wavelet Regression\n\n'''This is a sample code for the simulations of the paper:\nBozorgasl, Zavareh and Chen, Hao, Wav-KAN: Wavelet Kolmogorov-Arnold Networks (May, 2024)\n\nhttps://arxiv.org/abs/2405.12832\nand also available at:\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835325\nWe used efficient KAN notation and some part of the code:https://github.com/Blealtan/efficient-kan\n\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport math\n\nclass KANLinear(nn.Module):\n    def __init__(self, in_features, out_features, wavelet_type='mexican_hat'):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.wavelet_type = wavelet_type\n\n        # Parameters for wavelet transformation\n        self.scale = nn.Parameter(torch.ones(out_features, in_features))\n        self.translation = nn.Parameter(torch.zeros(out_features, in_features))\n\n        # Linear weights for combining outputs\n        #self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.weight1 = nn.Parameter(torch.Tensor(out_features, in_features)) #not used; you may like to use it for wieghting base activation and adding it like Spl-KAN paper\n        self.wavelet_weights = nn.Parameter(torch.Tensor(out_features, in_features))\n\n        nn.init.kaiming_uniform_(self.wavelet_weights, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.weight1, a=math.sqrt(5))\n\n        # Base activation function #not used for this experiment\n        self.base_activation = nn.SiLU()\n\n        # Batch normalization\n        self.bn = nn.BatchNorm1d(out_features)\n\n    def wavelet_transform(self, x):\n        if x.dim() == 2:\n            x_expanded = x.unsqueeze(1)\n        else:\n            x_expanded = x\n\n        translation_expanded = self.translation.unsqueeze(0).expand(x.size(0), -1, -1)\n        scale_expanded = self.scale.unsqueeze(0).expand(x.size(0), -1, -1)\n        x_scaled = (x_expanded - translation_expanded) / scale_expanded\n\n        # Implementation of different wavelet types\n        if self.wavelet_type == 'mexican_hat':\n            term1 = ((x_scaled ** 2)-1)\n            term2 = torch.exp(-0.5 * x_scaled ** 2)\n            wavelet = (2 / (math.sqrt(3) * math.pi**0.25)) * term1 * term2\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n        elif self.wavelet_type == 'morlet':\n            omega0 = 5.0  # Central frequency\n            real = torch.cos(omega0 * x_scaled)\n            envelope = torch.exp(-0.5 * x_scaled ** 2)\n            wavelet = envelope * real\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n            \n        elif self.wavelet_type == 'dog':\n            # Implementing Derivative of Gaussian Wavelet \n            dog = -x_scaled * torch.exp(-0.5 * x_scaled ** 2)\n            wavelet = dog\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n        elif self.wavelet_type == 'meyer':\n            # Implement Meyer Wavelet here\n            # Constants for the Meyer wavelet transition boundaries\n            v = torch.abs(x_scaled)\n            pi = math.pi\n\n            def meyer_aux(v):\n                return torch.where(v &lt;= 1/2,torch.ones_like(v),torch.where(v &gt;= 1,torch.zeros_like(v),torch.cos(pi / 2 * nu(2 * v - 1))))\n\n            def nu(t):\n                return t**4 * (35 - 84*t + 70*t**2 - 20*t**3)\n            # Meyer wavelet calculation using the auxiliary function\n            wavelet = torch.sin(pi * v) * meyer_aux(v)\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n        elif self.wavelet_type == 'shannon':\n            # Windowing the sinc function to limit its support\n            pi = math.pi\n            sinc = torch.sinc(x_scaled / pi)  # sinc(x) = sin(pi*x) / (pi*x)\n\n            # Applying a Hamming window to limit the infinite support of the sinc function\n            window = torch.hamming_window(x_scaled.size(-1), periodic=False, dtype=x_scaled.dtype, device=x_scaled.device)\n            # Shannon wavelet is the product of the sinc function and the window\n            wavelet = sinc * window\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n            #You can try many more wavelet types ...\n        else:\n            raise ValueError(\"Unsupported wavelet type\")\n\n        return wavelet_output\n\n    def forward(self, x):\n        wavelet_output = self.wavelet_transform(x)\n        #You may like test the cases like Spl-KAN\n        #wav_output = F.linear(wavelet_output, self.weight)\n        #base_output = F.linear(self.base_activation(x), self.weight1)\n\n        base_output = F.linear(x, self.weight1)\n        combined_output =  wavelet_output #+ base_output \n\n        # Apply batch normalization\n        return self.bn(combined_output)\n\nclass KAN(nn.Module):\n    def __init__(self, layers_hidden, wavelet_type='mexican_hat'):\n        super(KAN, self).__init__()\n        self.layers = nn.ModuleList()\n        for in_features, out_features in zip(layers_hidden[:-1], layers_hidden[1:]):\n            self.layers.append(KANLinear(in_features, out_features, wavelet_type))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nFunction Fitting\nWe will take some interesting test functions from Wavelet Regression notebook.\nThe Doppler function is \\[\nf(x) = x(1-x) \\sin(\\frac{2.1\\pi}{x+0.05}) \\\\\nx \\sim U[0,1]\n\\]\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom kan.utils import create_dataset\n\ntorch.set_default_dtype(torch.float64)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device is: ',device)\n\nf = lambda x: x[:,[0]]*(1-x[:,[0]])*torch.sin((2*np.pi)/(x[:,[0]]+.1))\n\ndataset = create_dataset(f, n_var=1, device=device, ranges=[0,1])\n\nprint('train input data shape', dataset['train_input'].shape)\nprint('train label data shape', dataset['train_label'].shape)\n\nplt.scatter(dataset['train_input'],dataset['train_label'])\n\ndevice is:  cpu\ntrain input data shape torch.Size([1000, 1])\ntrain label data shape torch.Size([1000, 1])\n\n\n\n\n\n\n\n\n\n\n# Define model\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# create a KAN: 1D inputs, 1D output, and 5 hidden neurons.\nmodel = KAN([1, 32, 1], wavelet_type='mexican_hat')\nmodel.to(device)\n\n# Define loss\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        model.train()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        model.eval()\n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n    return model, train_losses, test_losses\n\n\n\nimport numpy as np\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\nX_train = dataset['train_input']\ny_train = dataset['train_label']\nX_test = dataset['test_input']\ny_test = dataset['test_label']\nmlp, train_losses, test_losses = train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 0.2128, Test Loss: 0.2410\nEpoch 100/1000, Train Loss: 0.0273, Test Loss: 0.0248\nEpoch 150/1000, Train Loss: 0.0039, Test Loss: 0.0032\nEpoch 200/1000, Train Loss: 0.0020, Test Loss: 0.0018\nEpoch 250/1000, Train Loss: 0.0014, Test Loss: 0.0018\nEpoch 300/1000, Train Loss: 0.0012, Test Loss: 0.0014\nEpoch 350/1000, Train Loss: 0.0011, Test Loss: 0.0014\nEpoch 400/1000, Train Loss: 0.0010, Test Loss: 0.0013\nEpoch 450/1000, Train Loss: 0.0009, Test Loss: 0.0014\nEpoch 500/1000, Train Loss: 0.0009, Test Loss: 0.0020\nEpoch 550/1000, Train Loss: 0.0009, Test Loss: 0.0035\nEpoch 600/1000, Train Loss: 0.0008, Test Loss: 0.0038\nEpoch 650/1000, Train Loss: 0.0008, Test Loss: 0.0026\nEpoch 700/1000, Train Loss: 0.0008, Test Loss: 0.0027\nEpoch 750/1000, Train Loss: 0.0009, Test Loss: 0.0010\nEpoch 800/1000, Train Loss: 0.0008, Test Loss: 0.0010\nEpoch 850/1000, Train Loss: 0.0008, Test Loss: 0.0035\nEpoch 900/1000, Train Loss: 0.0008, Test Loss: 0.0039\nEpoch 950/1000, Train Loss: 0.0010, Test Loss: 0.0027\nEpoch 1000/1000, Train Loss: 0.0008, Test Loss: 0.0019\n\n\n\n# let us look at the recontruction\nX = dataset['train_input']\nn  = 1000\nX[:,0] = torch.linspace(0,1,steps=n)\ny = f(X)\ny = y[:,0].detach().numpy()\nyh = model.forward(X)\n\nyh = yh[:,0].detach().numpy()\nplt.plot(y,color='blue')\nplt.plot(yh, color='red')\nplt.show()\nplt.plot(yh-y, color='black')\nplt.ylim(-1,1)\nplt.show()",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KANs with Wavelets</span>"
    ]
  },
  {
    "objectID": "notebooks/02-07-KAN-PDE.html",
    "href": "notebooks/02-07-KAN-PDE.html",
    "title": "KAN PDE Solver",
    "section": "",
    "text": "We will use the example provided in the official implementation of the KAN paper. Install the library with\npip install pykan\n2D Poisson PDE\nWe will take Example-6 provided in the PyKAN repo.\nWe aim to solve a 2D poisson equation \\(\\nabla^2 f(x,y) = -2\\pi^2{\\rm sin}(\\pi x){\\rm sin}(\\pi y)\\), with boundary condition \\(f(-1,y)=f(1,y)=f(x,-1)=f(x,1)=0\\). The ground truth solution is \\(f(x,y)={\\rm sin}(\\pi x){\\rm sin}(\\pi y)\\).\n\nfrom kan import *\nimport matplotlib.pyplot as plt\nfrom torch import autograd\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ndim = 2\nnp_i = 30 # number of interior points (along each dimension)\nnp_b = 30 # number of boundary points (along each dimension)\nranges = [-1, 1]\n\nmodel = KAN(width=[2,2,2,1,1], grid=5, k=3, seed=1, device=device)\n\ndef batch_jacobian(func, x, create_graph=False):\n    # x in shape (Batch, Length)\n    def _func_sum(x):\n        return func(x).sum(dim=0)\n    return autograd.functional.jacobian(_func_sum, x, create_graph=create_graph).permute(1,0,2)\n\n# define solution\nsol_fun = lambda x: torch.sin(torch.pi*x[:,[0]])*torch.sin(torch.pi*x[:,[1]])\nsource_fun = lambda x: -2*torch.pi**2 * torch.sin(torch.pi*x[:,[0]])*torch.sin(torch.pi*x[:,[1]])\n\n# interior\nsampling_mode = 'random' # 'radnom' or 'mesh'\n\nx_mesh = torch.linspace(ranges[0],ranges[1],steps=np_i)\ny_mesh = torch.linspace(ranges[0],ranges[1],steps=np_i)\nX, Y = torch.meshgrid(x_mesh, y_mesh, indexing=\"ij\")\nif sampling_mode == 'mesh':\n    #mesh\n    x_i = torch.stack([X.reshape(-1,), Y.reshape(-1,)]).permute(1,0)\nelse:\n    #random\n    x_i = torch.rand((np_i**2,2))*2-1\n    \nx_i = x_i.to(device)\n\n# boundary, 4 sides\nhelper = lambda X, Y: torch.stack([X.reshape(-1,), Y.reshape(-1,)]).permute(1,0)\nxb1 = helper(X[0], Y[0])\nxb2 = helper(X[-1], Y[0])\nxb3 = helper(X[:,0], Y[:,0])\nxb4 = helper(X[:,0], Y[:,-1])\nx_b = torch.cat([xb1, xb2, xb3, xb4], dim=0)\n\nx_b = x_b.to(device)\n\nsteps = 20\nalpha = 0.01\nlog = 1\n\ndef train():\n    optimizer = LBFGS(model.parameters(), lr=0.1, history_size=10, line_search_fn=\"strong_wolfe\", tolerance_grad=1e-32, tolerance_change=1e-32, tolerance_ys=1e-32)\n\n    pbar = tqdm(range(steps), desc='description', ncols=100)\n\n    for _ in pbar:\n        def closure():\n            global pde_loss, bc_loss\n            optimizer.zero_grad()\n            # interior loss\n            sol = sol_fun(x_i)\n            sol_D1_fun = lambda x: batch_jacobian(model, x, create_graph=True)[:,0,:]\n            sol_D1 = sol_D1_fun(x_i)\n            sol_D2 = batch_jacobian(sol_D1_fun, x_i, create_graph=True)[:,:,:]\n            lap = torch.sum(torch.diagonal(sol_D2, dim1=1, dim2=2), dim=1, keepdim=True)\n            source = source_fun(x_i)\n            pde_loss = torch.mean((lap - source)**2)\n\n            # boundary loss\n            bc_true = sol_fun(x_b)\n            bc_pred = model(x_b)\n            bc_loss = torch.mean((bc_pred-bc_true)**2)\n\n            loss = alpha * pde_loss + bc_loss\n            loss.backward()\n            return loss\n\n        if _ % 5 == 0 and _ &lt; 50:\n            model.update_grid_from_samples(x_i)\n\n        optimizer.step(closure)\n        sol = sol_fun(x_i)\n        loss = alpha * pde_loss + bc_loss\n        l2 = torch.mean((model(x_i) - sol)**2)\n\n        if _ % log == 0:\n            pbar.set_description(\"pde loss: %.2e | bc loss: %.2e | l2: %.2e \" % (pde_loss.cpu().detach().numpy(), bc_loss.cpu().detach().numpy(), l2.cpu().detach().numpy()))\n\ntrain()\n\ncpu\ncheckpoint directory created: ./model\nsaving model version 0.0\n\n\npde loss: 4.65e+01 | bc loss: 5.23e-02 | l2: 1.43e-01 : 100%|███████| 20/20 [00:34&lt;00:00,  1.73s/it]\n\n\n\nmodel.plot(beta=10)\n\n\n\n\n\n\n\n\n\nlib = ['x','sin','log','exp']\nmodel.auto_symbolic(lib=lib)\n\nfixing (0,0,0) with x, r2=0.9569580554962158, c=1\nfixing (0,0,1) with sin, r2=0.9994773268699646, c=2\nfixing (0,1,0) with x, r2=0.9826741814613342, c=1\nfixing (0,1,1) with x, r2=0.9958211779594421, c=1\nfixing (1,0,0) with x, r2=0.8470919728279114, c=1\nfixing (1,0,1) with x, r2=0.9101454019546509, c=1\nfixing (1,1,0) with x, r2=0.8957751989364624, c=1\nfixing (1,1,1) with x, r2=0.924821674823761, c=1\nfixing (2,0,0) with x, r2=0.9645843505859375, c=1\nfixing (2,1,0) with x, r2=0.9815171360969543, c=1\nfixing (3,0,0) with x, r2=0.06022500991821289, c=1\nsaving model version 0.1\n\n\n\nformula = model.symbolic_formula()\nprint(formula)\n\n([0.000678440416089527*x_1 - 0.0932934275961874*x_2 - 0.0355230525977442*sin(3.26248002052307*x_1 - 3.20023989677429) + 0.131764609939269], [x_1, x_2])\n\n\nIt is not quite there. TBD: Let us plot the solution (and see how it differs)",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>KAN PDE Solver</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Welcome\nDear Faculty, Students and Learners\nSee the course page for recent information on Lectures, Labs, Resources etc..",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "Deep Learning",
    "section": "Announcements",
    "text": "Announcements\n\n[22-Sep-2024] Notes added to L03.\n[21-Sep-2024] L02, L03 added.\n[07-Sep-2024] L01 added.\n[21-Aug-2024] Course website up",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Learning",
    "section": "Overview",
    "text": "Overview\nPrereqs\n\nUndergraduate level exposure to Linear Algebra, Calculus\nAbility to read Python code\nBasic exposure to ML/DL\n\nPart-1: A Mathematical Introduction to Deep Learning Models\n\nTopics\n\nFeed Forward Neural Networks (FFNs)\nConvolution Neural Networks (CNNs)\nKolmogorov-Arnold Networks (KANs)\nRecurrent Neural Networks (RNNs)\nTransformers\nGraph Neural Networks (GNNs)\nSelective Structured State Space Models (S4)\n\n\nPart-2: A Mathematical Introduction to Deep Generative Models\n\nTopics\n\nVariational Auto Encoders (VAEs)\nGenerative Adversarial Networks (GANs)\nFlow Networks\nDiffusion Models",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "When I started reading papers about Deep Learning, typically published in conferences, I had hard time following them. The key modeling details are presented in textual form with accompanied by architecture (block) diagrams. If the paper had any Math in them it was for proofs but not for explaining the models or the expressions were restricted to highlighting key contributions (eg. loss function).\nDespite this information, it was hard for me to understand them to be able to reproduce. I have to look at source code to see how they are implemented and go back to the paper and read again, and repeat this process. This was probably due to the my formal training Statistics. I always start with the model (expressed as equations). This unlearning took a long time. I am assuming that folks with training in Maths/ Applied Maths will also have hard time reading papers in the ML space for the same reason - there is no precision.\nIn this course, we will discuss different architectures (models) in Deep Learning using Mathematical language as much as possible (for the sake of precision) and follow-them up with implementation in code.\nThe intended audience is those with math background, that wants to appreciate modern deep learning models. We will not get into “why” deep learning works or their applications. In the resources section, I will provide ample references for those interested that wants to explore further.\n\nDisclaimer\nThis course is by no means a replacement of any other resources available. Hopefully, the content and views presented complement the current practice of MLOps, readers and students benefit from it.\nopenly,\nThe Saddle Point",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Course",
    "section": "",
    "text": "Syllabus & Schedule",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#references",
    "href": "course.html#references",
    "title": "Course",
    "section": "References",
    "text": "References\n\n[course] CS6910, Prof. Mitesh Khapra’s CS6910 Deep Learning at IIT-M\n[course] CS236 Prof. Stefano Emron’s course on Deep Generative Modeling at Stanford Fall’23\n[Book] Deep Generative Modeling, Jakub Tomxzak\n[Book] Understanding Deep Learning, Simon Prince\n[Book] Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville\n\n\nReferences\n\nR01: CS6910: Deep Learning Prof. Mitesh Khapra @ IIT Madras\nR02: CS7015: Deep Learning Prof. Mitesh Khapra @ IIT Madras(earlier version of CS6910)\nR03: LING 574: Deep Learning for NLP Prof. Shane Steinert-Threlkeld @ University of Washington\nR04: Dive into Deep Learning Alex Smola et al\nR05: Understanding Deep Learning Prof. Simon Prince\nR06: Neural Networks and Deep Learning Michael Nielsen\nR07: Deep Learning for Computer Vision NPTEL course by Prof. Vineeth N Balasubramanian\nR08: CS236 Prof. Stefano Emron’s course on Deep Generative Modeling at Stanford Fall’23\nR09: Deep Generative Modeling, Jakub Tomxzak\nR10 Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville\n\nSoma S Dhavala\nDiscussion Anchor",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "lectures/L01.html",
    "href": "lectures/L01.html",
    "title": "FFNs",
    "section": "",
    "text": "Materials:\nDate: Saturday, 07-Sep-2024, 1.30pm, IST.",
    "crumbs": [
      "Lectures",
      "FFNs"
    ]
  },
  {
    "objectID": "lectures/L01.html#materials",
    "href": "lectures/L01.html#materials",
    "title": "FFNs",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\n\n\n\nIn-Class\n\nHistory of Deep Learning\n\nLecture-1 pdf, Interactive slides [R01]\nLecture-1 pdf [R02]\n\nVisualizations\n\nInteractive Figures to visualize NNs [R05]\nRepresentational Power of NNs from [R06]\n\nNeural Networks Motivation\n\nMcCulloch-Pitts Neuron, Perceptron Lecture-2:pdf [R01]\nDigital Logic Modeling by Perceptron Neural Networks:pdf [R03]\n\nFFNs\n\nFFNs for Classification and Language Modeling: pdf\n\n\n\n\nLab\n\nFFN for Classification on Iris data\nFFN for Regression on Friedman2 data\n\n\n\nPost-class:\n\nLecture-3:pdf Sigmoid Neuron, Error Surfaces, Representation Power of FFNs [R02]\nGradient Descent, Word Vectors [R03]\nLecture-4:pdf FFNs and Backprop [R02]\nComputational Graphs, Backprop:pdf [R03]\nLecture - Expressivity and UAT\nNeural Networks Review from R07\n\nNeural Networks: A Review part-1: youtube, part-2: youtube\nFFNs and Backpopr part-1: youtube, part-2: youtube\n\n\n\n\nPapers\n\nUniversal Approximation Theorem original paper by Cybenko pdf\nMultilayer FFNs are universal approximators Hornik, Stinchcombe, and White\nRepresentation Benefits of Deep FFNs Telagarsky",
    "crumbs": [
      "Lectures",
      "FFNs"
    ]
  },
  {
    "objectID": "lectures/L01.html#notes",
    "href": "lectures/L01.html#notes",
    "title": "FFNs",
    "section": "Notes",
    "text": "Notes\n\nLinear Model\nConsider the following regression problem \\[y^{[i]} \\equiv f(x^{[i]}) + e^{[i]} \\equiv \\phi(x^{[i]}) + e^{[i]}, i \\in \\left\\{1,\\dots,N\\right\\}\\] with \\(D = \\{x^{[i]}, y^{[i]}\\}_{i=1}^{N}\\) representing all the data available to fit (train) the model \\(f(x)\\). Suppose that \\(x_1, x_2, x_3, \\dots, x_{n_0}\\) are the \\({n_0}\\) features available to fit the model. If we choose \\(f(.)\\) to be a linear combination of the features, it leads us to the familiar Linear Model (or Linear Regression). In matrix notation, the regression problem is: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf \\beta} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\[\n\\begin{array}{left}\n{\\bf X}_{N \\times {({n_0}+1)}} &=&\n\\begin{pmatrix} 1 &  x_1^{[1]} & \\dots & x_{n_0}^{[1]} \\\\\n1 & x_1^{[2]} & \\dots & x_{n_0}^{[2]} \\\\\n\\vdots & & & \\vdots \\\\\n1 & x_1^{[N]} & \\dots & x_{n_0}^{[N]}\n\\end{pmatrix} \\\\\n{\\bf \\beta}_{{({n_0}+1)} \\times 1} &=& [\\beta_1, \\beta_2, \\dots, \\beta_{({n_0}+1)} ]^T \\\\\n{\\bf y}_{N \\times 1} &=& [y^{[1]}, y^{[2]}, \\dots, y^{[N]} ]^T \\\\\n\\end{array}\n\\] This is the classic Linear Regression setup. To recast this in a familiar Deep Learning notation, we rewrite the above as: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf w} + {\\bf  b} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\({\\bf  b}\\) represents the \\({n_0} \\times 1\\) bias (or intercept) term, \\({\\bf w}\\) is the weight matrix (regression coefficients) and \\({\\bf X}\\) is the set of all \\(N \\times (n_0+1)\\) features excluding the column of ones (which was included to model the intercept/ bias term).\nThe prediction \\({\\bf \\hat{y}}\\) is typically the conditional expectation \\({\\bf \\hat{y}| {\\bf X} } = {\\bf X}{\\bf w} + {\\bf  b}\\) under the zero-mean error model for \\({\\bf \\epsilon}\\), obtained by minimizing the MSE between the observed and the predicted. This is essentially a Perceptron with linear activation function, which is typically used to solve regression problems. What about binary classification (or more generally, categorical responses)?\n\n\nGeneralized Linear Model\n\nBinary Classification\nSuppose \\(y \\in \\{ 0,1\\}\\) is a binary response, and consider the following (generative) model:\n\\[\n\\begin{array}{left}\ny^{[i]} = I(h^{[i]} \\ge 0) \\\\\nh^{[i]} \\equiv f(x^{[i]}) + e^{[i]}\n\\end{array}\n\\]\nNotice that the output is a thresholded version (via the Indicator function) of the linear model response \\(h^{[i]}\\). If \\(e^{[i]} \\sim N(0, \\sigma^2)\\), then,\n\\[\n\\begin{array}{left}\nP(y^{[i]} = 1) \\equiv \\pi^{[i]}= \\sigma \\left( f(x^{[i]}) \\right)  \\equiv \\frac{ \\exp(f(x^{[i]})}{1+ \\exp(f(x^{[i]}))}\n\\end{array}\n\\]\nIf we choose \\(f(x^{[i]}) \\equiv {\\bf x}{\\bf w} + {\\bf  b}\\), where \\({\\bf x}\\) is \\(1 \\times {n_0}\\) vector of inputs, we recover the Perceptron with sigmoid activation, which is exactly the Logistic regression model, belonging to the much larger class of Generalized Linear Models (GLMs). In the place of \\(\\sigma\\), one can choose any CDF. In particular, if we choose the CDF of the standard Normal distribution, we recover the Probit model. This Perceptron (Logistic regression) is the most recognizable building block of MLP, a schematic of which is shown below.\n\n\n\nPerceptron = GLM\n\n\n\n\nMulticlass Classification\nThe extension of the logistic regression to multiclass is straightforward but we need a different way of writing the models. The Binary Classification problem can written in the following equivalent fashion: \\[\n\\begin{array}{left}\ny^{[i]} \\sim Bin(\\pi^{[i]}, 1)\\\\\n\\pi^{[i]} = \\sigma \\left( f(x^{[i]}) \\right) \\equiv \\frac{ \\exp(f(x^{[i]})} {1+ \\exp(f(x^{[i]}) }\n\\end{array}\n\\] In the GLM nomenclature, it is customary to link \\(\\pi^{[i]}\\) with \\(f(x^{[i]})\\) as follows: \\[\\log\\left(\\frac{\\pi^{[i]}}{1-\\pi^{[i]}}\\right) = f(x^{[i]})\\] It is like \\(f(x^{[i]})\\) is now modeling the difference in the class-specific responses. To make this explicit, suppose, \\(f_k(x^{[i]})\\) is the class-specific latent response (whose thresholded version yields the binary response as noted before). Then, by specification, \\(f(x^{[i]}) =  f_1(x^{[i]}) - f_0(x^{[i]})\\). It follows then that, \\[\n\\begin{array}{left}\n\\pi^{[i]} &=& \\frac{ \\exp(f(x^{[i]})} {1+ \\exp(f(x^{[i]}) } \\\\\n&=& \\frac{ \\exp(f_1(x^{[i]}) - f_0(x^{[i]})) }  {1+ \\exp(f_1(x^{[i]}) - f_0(x^{[i]})) } \\\\\n&=& \\frac{ \\exp(f_1(x^{[i]})} {\\exp(f_1(x^{[i]}) + f_0(x^{[i]})) } \\\\\n&\\equiv& \\text{softmax}(f_1(x^{[i]}))\n\\end{array}\n\\]\n\n\n\nVector Generalized Linear Model\nNow we are in a position to extend the Binary Classification problem to the Multiclass setting, where the scales are nominal (i.e., the class labels have no ordering, and labels could have been shuffled with no consequence).\nSuppose we have \\({n_L}\\)-classes with \\(\\pi_k^{[i]}\\) representing the i-th example being drawn from the class \\(k\\) with \\(\\sum_{k=1}^{{n_L}} \\pi_k^{[i]} = 1\\). The following generative model is a plausible model: \\[\n\\begin{array}{left}\ny^{[i]} &\\sim& \\text{Multinomial}(\\pi_k^{[i]}, 1)\\\\\n\\pi_k^{[i]} &=& \\frac{\\exp(f_k(x^{[i]})}{ \\sum_k \\exp(f_k(x^{[i]})}\n&\\equiv& \\text{softmax}(f_k(x^{[i]}))\n\\end{array}\n\\]\nNotice that, we have one hidden/ latent functional for each class \\(f_k(x^{[i]})\\) - the normalized version of which models the class probabilities in the Multinomial distribution. In general, in the above model, all \\(f_k(.)\\) are not identifiable. In fact, under the simplex constraint (i.e., class probabilities must add up to to one), \\(\\sum_k f_k(.)  =0\\), which essentially is saying that, to model \\(n_L\\) responses, we can not have \\(n_L\\) independent (unconstrained) latent responses but only \\(n_L-1\\) are identifiable.\nAs a check, if we place this simplex constraint on the logits for the Binary Classification, we can recover standard logistic regression model, as shown below: \\[\n\\begin{array}{left}\n\\pi^{[i]} &=& \\frac{ \\exp(f_1(x^{[i]})} {\\exp(f_1(x^{[i]}) + f_0(x^{[i]})) } \\\\\n&=& \\frac{\\exp(2 f_1(x^{[i]})-1)} {1+ \\exp(2f_1(x^{[i]}-1))}   \\because f_1(x^{[i]}) + f_0(x^{[i]}) =0 \\\\\n&=& \\frac{\\exp(f(x^{[i]})} {1+ \\exp(f(x^{[i]}))}  \\text{ with }   f(x^{[i]}) = 2f_1(x^{[i]})-1 \\\\\n&\\equiv& \\sigma (f(x^{[i]}))\n\\end{array}\n\\]\nDigression: In the Deep Learning context, such explicit constraints are ignored. When training the models with SGD, different tricks which are empirically proven such as Layer Normalization, Batch Normalization etc are used. Their effect is to enforce identifiability constraints via what look like hacks but in reality, they are fixes for the structural issues in the models themselves.\nWhen the constrains are removed, and \\(f_k(x)= {\\bf x}{\\bf w_k} + {\\bf  b_k}\\) is a linear model, we get a Vector Generalized Linear Model (VGLAM). VGLAM is shown as a (shallow) network below. \nSpecifically, ignoring the bias terms for simplicity, \\[\n\\begin{array}{left}\ny_k^{[i]} \\equiv \\psi(x^{[i]}) = \\sigma \\left( \\sum_{j=1}^{n_0} w_{j,k} x_j^{[i]} \\right) \\forall k=1,2,\\dots n_L\\\\\n\\end{array}\n\\] Here \\(\\sigma(.)\\) is element-wise operation. So far we discussed the shallow Perceptron for regression, and classification. It is often helpful to write the expression in matrix notation as follows: \\[\n\\begin{array}{left}\n\\bf{y}  = \\sigma \\left( xW+b \\right)\n\\end{array}\n\\] where we have added the bias term \\(b\\), a \\(n_L\\times 1\\) vector, \\(x\\) is \\(1 \\times n_0\\) input vector, \\(W\\) is a \\(n_0 \\times n_L\\) weight matrix. How can we add depth?\n\n\nDeep Vector Generalized Linear Model\nAt the heart of the GLM for Multiclass, we have \\(f_k\\) which is a map from \\({n_0}\\)-dimensional input to \\(R\\), i.e., \\(f_k(): R^{n_0} \\rightarrow R\\). If we have \\({n_L}\\) such functions, we effectively have \\(f: R^{n_0} \\rightarrow R^{n_L}\\). We can stack such blocks to get a composition as follows:\n\\[\nf(\\bf{x})= \\sigma\\left( \\dots \\left( \\sigma\\left(x W_0 + b_0\\right)W_1 + b_1 \\right) \\dots \\right) W_{n_{L-1}} + b_{n_{L-1}}\n\\]\nThe above is the popular Multilayer Perceptron or MLP as it is known. Shown below is the network without the bias terms.\n\n\n\nMLP = Deep VGLM\n\n\n\n\nUniversal Approximation\n\n2-Ary Boolean Circuits\nLet us consider boolean variables and just two inputs \\(x_1, x_2 \\in \\{0,1\\}\\). We will model the popular logic gates using a simple Neuron: \\(y = H\\left(x_1 w_1 + x_2 w_2 + b\\right)\\), where the Heavyside step function \\(H(x) = 1 \\text{ if } x\\ge 0, 0. \\text{ o.w}\\)\n\n\n\ngate\n\\(w_1\\)\n\\(w_2\\)\nb\n\n\n\n\nAND\n1\n1\n-1.5\n\n\nOR\n1\n1\n-0.5\n\n\nNAND\n-1\n-1\n1.5\n\n\nNOR\n-1\n-1\n0.5\n\n\n\nFor example, the Truth Table for AND, and the corresponding output realized by the Neuron is given below:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{AND}\\)\n\\(\\hat{y}_{AND}\\)\n\n\n\n\n0\n0\n0\n\\(H(-1.5)=0\\)\n\n\n0\n1\n0\n\\(H(-0.5)=0\\)\n\n\n1\n0\n0\n\\(H(-0.5)=0\\)\n\n\n1\n1\n1\n\\(H(+0.5)=1\\)\n\n\n\nOther gates like NAND, NOR and OR can also be realized by a Neuron of the form shown above. The NAND gate is called a universal gate (so does NOR) because, given enough number of NAND gates, any Truth Table can be realized consisting of only NAND gates. For example, NOT gate can be realized by setting one of the inputs to logic 1 to get the complement of the other input. Note that XOR can not be modeled by a perceptron. Can you guess why?\n\n\nM-Ary Boolean Circuits\nNow we will generalize the above for M-ary AND, NAND, OR and NOR gates as follows. \\[\n\\begin{array}{left}\n\\hat{y}_{AND} = \\text{H}(\\sum (x_i-1) \\,+ 0.5)\\\\\n\\hat{y}_{NAND} = \\text{H}(\\sum(1-x_i) \\,- 0.5) \\\\\n\\hat{y}_{OR} = \\text{H}(\\sum x_i \\,- 0.5) \\\\\n\\hat{y}_{NOR} = \\text{H}(\\sum -x_i \\,+ 0.5) \\\\\n\\end{array}\n\\] Verify the results yourself.\n\n\nM-Ary Universal Boolean Circuits\nNow consider M-ary inputs and our goal is to build a boolean circuit which can realize any Truth Table. The Truth Table consists of \\(2^M\\) rows, each row corresponding to one specific state of the inputs, and contains \\(M+1\\) columns - one column for each of the \\(M\\) inputs and one column for the output \\(y\\). Truth Table can be realized in the form of a Sum-of-Products (SoP) form, a special form of Disjunctive Normal Form (see Canonical Forms to represent Truth Tables in Boolean Algebra). Let us look at an example below with two inputs, modeling XOR gate:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{XOR}\\)\n\n\n\n\n\n0\n0\n0\n\n\n\n0\n1\n1\n\n\n\n1\n0\n1\n\n\n\n1\n1\n0\n\n\n\n\nFor each row, whose Truth value is 1, we consider the product of inputs. We take the complement of the input when it is zero. For the XOR gate, we add the 2nd row product \\(\\bar{x_1}x_2\\) to the third row term \\(x_1 \\bar{x_2}\\) to get final SoP as follows: \\[\ny = \\bar{x_1}x_2 + x_1 \\bar{x_2}\n\\] where \\(\\bar{x}\\) represents the logical complement of \\(x\\). Notice that each product term consists of two sub products - one that is a product of all inputs where inputs are 1s, and product of their complements where the inputs are 0. Therefore, each product in the SoP can be represented as \\(AND(AND(.), NOR(.))\\) and refer to this as \\(AANOR\\) as contraction. For example, the first term in the SoP for XOR is \\(AND(AND(x_1), NOR(x_2)) \\equiv \\bar{x_1}x_2\\). We will exploit this to succinctly represent the SoP: \\[\n\\begin{array}{left}\ny &=& \\sum_{i \\in \\{ i' \\text{ s. t } y^[i']=1\\} } AND( AND( x^{i}_{p \\in A}), NOR(x^{i}_{q \\in \\bar{A}})) \\\\\nA^{[i]} &=& \\{ p \\text{ s.t } x_p^{[i]} = 1 \\} \\\\\n\\bar{A}^{[i]} &=& \\{ p \\text{ s.t } x_p^{[i]} = 0\\}\n\\end{array}\n\\] The outer sum in the SoP is nothing but an OR gate operating on the innards. Therefore, to realize an SoP boolean network, we need an N-ary OR gate (where N can be at most \\(2^M\\) where M is the number of inputs or input dimension). Then the inner gate is \\(AANOR \\equiv AND(AND(.), NOR(.))\\). Can we realize such as a gate with a Neuron?\n\n\nMLP with 1-hidden layer\nObserve that, the AANOR gate is hot only when all \\(x \\in A\\) are 1s and \\(x \\in A^c\\) are 0s. To clarity, the AANOR gate is hot for exactly one of the rows of the \\(2^M\\) rows. Exploiting this fact, we can simplify the Truth Table as follows:\n\n\n\nCase\n\\(x \\in A\\)\n\\(x \\in \\bar{A}\\)\n\\(y_{AANOR}\\)\n\n\n\n\n\nCase-1\nAll 1s\nAll 0s\n1\n\n\n\nCase-2\nAt least one 0\nAny\n0\n\n\n\n\nVerify that cases are mutually exclusive and exhaustive. Based on our idea about AND and NOR gates designed earlier, let us hypothesize the following Neuron for the AANOR gate as follows: \\[\n\\begin{array}{left}\ny &=& H( \\sum_{i \\in A} (x_i-1) -0.5 + \\sum_{i \\in \\bar{A}} -x_i + 0.5 + b) \\\\\n&=& H( \\sum_{i \\in A} (x_i-1) + \\sum_{i \\in \\bar{A}} -x_i + b)\n\\end{array}\n\\] where \\(b\\) is to found out, which we do next.\nCase-1: In this case, substituting the specific x’s, we get the inequality $b &gt; 0, which ensures that \\(H(.)=1\\) in this case.\nCase-2: At least one x in A is 0, which means, we need \\(\\max \\left( \\sum_{i \\in A} (x_i-1) + \\sum_{i \\in \\bar{A}} -x_i + b \\right) &lt; 0\\). It is \\(-1+b\\) and is achieved when exactly one x in A is zero, and all x’s in \\(\\bar{A}\\) are zero. Therefore, we get, \\(b-1 &lt; 0\\). We can choose \\(b=0.5\\) which satisfies both the inequalities \\(0 &lt; b &lt; 1\\). The AANOR gate can now be realized as:\n\\[\n\\begin{array}{left}\ny &=& H( \\sum_{i \\in A} (x_i-1) + \\sum_{i \\in \\bar{A}} -x_i + 0.5)\n\\end{array}\n\\]\nNow that we can realize any AANOR gate, and OR gate is something we have already seen, any M-ary Boolean Truth Table can be realized as \\[\n\\begin{array}{left}\ny &=& OR(\\{ AANOR(x^{[i]} \\}), i \\in \\{ i' \\text{ s. t } y^[i']=1\\}\n\\end{array}\n\\]\nAbove can be seen a composition of Boolean gates: \\[\n\\text{ inputs &gt; ANOR &gt; OR }\n\\] which is indeed an MLP with 1-hidden layer in hindsight, as illustrated in the figure below. \nEffectively, we exploited the fact that any M-ary Truth Table can be expressed in SOP form, and we have constructed a 1-hidden layer MLP which can exactly model the SoP.\nLet us verify this circuit for XOR gate which we know can not be modeled by single Neuron but can be nmodeled by a 1-hidden layer MLP.\n\\[\n\\begin{array}{left}\nh_1 &=&  H\\left( (x_1-1) - x_2 + 0.5 \\right) \\\\\nh_2 &=&  H\\left( -x_1 + (x_2-1) + 0.5 \\right) \\\\\ny &=&  h_1 + h_2 - 0.5\\\\\n\\end{array}\n\\]\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{XOR}\\)\n\\(h_1\\)\n\\(h_2\\)\n\\(y=OR(h_1, h_2)\\)\n\n\n\n\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n1\n\n\n1\n0\n1\n1\n0\n1\n\n\n1\n1\n0\n0\n0\n1\n\n\n\nIndeed, we recovered the XOR gate. Of course, we are operating on Boolean inputs. The continuous analogue of what we derived for MLPs with 1-hidden layer are due to the early works of Cybenko’s Universal Approximation Theorempdf. A nice illustration of the Universal Approximation abilities of MLPs can be found here - Representational Power of NNs\n\n\n\nSummary\n\nLogistic regression is a member of Generalized Linear Models (GLMs)\nGeneralized Linear Models is a member of Vector Generalized Linear Models (VGLMs)\nLogistic regression = Perceptron with sigmoid activation\nMLP with an input and output layer (with multiple outputs) = VGLM\nMLPs are Deep VGLMS\nUniversal Approximation Ability of MLPs\n\nAny M-ary Boolean circuit can be represented in Sum-of-Products form\nAND, OR, NAND, NOR gates can be realized by Percetrons.\nSum-of-Products can be realized by an MLP with 1-hidden layer.\nMLP with 1-hidden layer are Universal Approximators of M-ary Boolean circuits.",
    "crumbs": [
      "Lectures",
      "FFNs"
    ]
  },
  {
    "objectID": "lectures/L02.html",
    "href": "lectures/L02.html",
    "title": "CNNs",
    "section": "",
    "text": "Materials:\nDate: TBD",
    "crumbs": [
      "Lectures",
      "CNNs"
    ]
  },
  {
    "objectID": "lectures/L02.html#materials",
    "href": "lectures/L02.html#materials",
    "title": "CNNs",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nFFNs\n\n\n\nIn-Class\n\nCNNs Intro\n\nChapter 7[R05] covers topics like Invariance, Channels, Convolution Operation, Padding, Stride, Pooling.\nCNNs[R02]\n\nVisualizations\n\nCNNs[R02]\n\nCNN Architectures\n\nChapter 8[R04] covers landmark CNN architectures such as AlexNet, VGG, ResNet, DenseNet.\n\n\n\n\nLab\n\n1d CNNs (tbd)\n2d CNNs (tbd)\n\n\n\nPost-class:\n\n[notebook] ID Convolution R05\n[notebook] ID Convolution for MNSIT R05\n[notebook] 2D Convolution R05\n[notebook] CNN for MNIST R05\n[youtube] Lectures from R07\n\nCNNs: An Intro Part-1, part-2\nBackprop in CNNS youtube\nEvolution of CNNs for Image Classification part-1, part-2\nRecent Architectures youtube [as of 2021]\nCNNs for Object Detection Part-1, Part-2 Part-3\nCNNs for Segmentation youtube\n\n\n\n\nPapers\n\nAlexNet - ImageNet Classification with Deep Convolutional Neural Networks\nZFNet Visualizing and Understanding Convolutional Networks\nVGGNet Very Deep Convolutional Networks for Large-Scale Image Recognition\nGooLeNet Going Deeper with Convolutions\nResNet Deep Residual Learning for Image Recognition\nRCNN Rich feature hierarchies for accurate object detection and semantic segmentation -Faster-RCNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\nYOLO You Only Look Once: Unified, Real-Time Object Detection\nUNet U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "crumbs": [
      "Lectures",
      "CNNs"
    ]
  },
  {
    "objectID": "lectures/L03.html",
    "href": "lectures/L03.html",
    "title": "KANs",
    "section": "",
    "text": "Materials:\nDate: Saturday, 21-Sep-2024, 1.30pm, IST.",
    "crumbs": [
      "Lectures",
      "KANs"
    ]
  },
  {
    "objectID": "lectures/L03.html#materials",
    "href": "lectures/L03.html#materials",
    "title": "KANs",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nFFNs\nCNNs\n\n\n\nIn-Class\n\nKAN: Kolmogorov-Arnold Networks\nImplementation PyKAN\nImplementation Efficient-KAN\nKAN 2.0: Kolmogorov-Arnold Networks Meet Science\n\n\n\nLab\n\nFit a function from PyKAN simple functions\nSplines from PyKAN on a Doppler function.\nRBFs from Fast-KAN on a Doppler function\nChebyshev Polynomials from ChebyKAN on a Doppler function\nWavelets from WavKAN on a Doppler function\nPDE solver\nWavelets with IWT\nWavelet Regression in Python\n\n\n\nPost-class:\n\n[paper] Chebyshev KAN Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation. Implementation ChebyKAN\n[paper] Survey of KANs A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)\n[code] Fast KAN Replace B-splines with RBF to make it 3.3x faster than Efficient-KAN\n[paper]Kolmogorov-Arnold Convolutions: Design Principles and Empirical Studies | code\n[collection] awesome KANs - a collection of papers, codes, tutorials on KANs.\n\n\n\nKAN Papers and Advancements\n\nKAN vs MLP KAN or MLP: A Fairer Comparison\nKANs for PINNs Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks\nWav-KAN Wav-KAN: Wavelet Kolmogorov-Arnold Networks\nKAN GPT\n\n\n\nTheory\n\n1957-On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables : The original Kolmogorov Arnold paper\n1957-On functions of three variables\n2009-On a constructive proof of Kolmogorov’s superposition theorem\n2021-The Kolmogorov-Arnold representation theorem revisited\n2021-The Kolmogorov Superposition Theorem can Break the Curse of Dimension When Approximating High Dimensional Functions\n\n\n\nAdditional References\n\n[Book] Essential Wavelets for Statistical Applications and Data Analysis\n[Book] All of Nonparametric Statistics - Chapter 9\n[Book] Functional Data Analysis Chapter 3 on Fourier basis, Splines, Wavelets and Polynomials\n[Book] Functional Data Analysis with R and MATLAB - Chapter 2 on specifying basis functions\n[Book] Generalized Additive Models by Hastie and Tibshirani. paper\n[Tutorial] Functional Regression\n[Tutorial] Functional Data Analysis: An Introduction and Recent Developments",
    "crumbs": [
      "Lectures",
      "KANs"
    ]
  },
  {
    "objectID": "lectures/L03.html#notes",
    "href": "lectures/L03.html#notes",
    "title": "KANs",
    "section": "Notes",
    "text": "Notes\nThe following (written in blue) is taken mostly verbatim from hellokan.ipynb, the authors of KAN.\n\nKolmogorov-Arnold representation theorem\n\nKolmogorov-Arnold representation theorem states that if \\(f\\) is a multivariate continuous function on a bounded domain, then it can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. More specifically, for a smooth \\(f : [0,1]^{n} \\to \\mathbb{R}\\), \\[f(x) = f(x_1,...,x_{n})=\\sum_{q=1}^{2{n}+1}\\Phi_q(\\sum_{p=1}^{n} \\phi_{q,p}(x_p))\\] where \\(\\phi_{q,p}:[0,1]\\to\\mathbb{R}\\) and \\(\\Phi_q:\\mathbb{R}\\to\\mathbb{R}\\). In a sense, they showed that the only true multivariate function is addition, since every other function can be written using univariate functions and sum. However, this 2-Layer width-\\((2{n}+1)\\) Kolmogorov-Arnold representation may not be smooth due to its limited expressive power. We augment its expressive power by generalizing it to arbitrary depths and widths.\n\n\n\nKolmogorov-Arnold Network (KAN)\n\nThe Kolmogorov-Arnold representation can be written in matrix form \\[f(x)={\\bf \\Phi}_{\\rm out}\\circ{\\bf \\Phi}_{\\rm in}\\circ {\\bf x}\\] where \\[{\\bf \\Phi}_{\\rm in}= \\begin{pmatrix} \\phi_{1,1}(\\cdot) & \\cdots & \\phi_{1,n}(\\cdot) \\\\ \\vdots & & \\vdots \\\\ \\phi_{2n+1,1}(\\cdot) & \\cdots & \\phi_{2n+1,n}(\\cdot) \\end{pmatrix},\\quad {\\bf \\Phi}_{\\rm out}=\\begin{pmatrix} \\Phi_1(\\cdot) & \\cdots & \\Phi_{2n+1}(\\cdot)\\end{pmatrix}\\] We notice that both \\({\\bf \\Phi}_{\\rm in}\\) and \\({\\bf \\Phi}_{\\rm out}\\) are special cases of the following function matrix \\({\\bf \\Phi}\\) (with \\(n_{\\rm in}\\) inputs, and \\(n_{\\rm out}\\) outputs), we call a Kolmogorov-Arnold layer: \\[{\\bf \\Phi}= \\begin{pmatrix} \\phi_{1,1}(\\cdot) & \\cdots & \\phi_{1,n_{\\rm in}}(\\cdot) \\\\ \\vdots & & \\vdots \\\\ \\phi_{n_{\\rm out},1}(\\cdot) & \\cdots & \\phi_{n_{\\rm out},n_{\\rm in}}(\\cdot) \\end{pmatrix}\\] \\({\\bf \\Phi}_{\\rm in}\\) corresponds to \\(n_{\\rm in}=n, n_{\\rm out}=2n+1\\), and \\({\\bf \\Phi}_{\\rm out}\\) corresponds to \\(n_{\\rm in}=2n+1, n_{\\rm out}=1\\). After defining the layer, we can construct a Kolmogorov-Arnold network simply by stacking layers! Let’s say we have \\(L\\) layers, with the \\(l^{\\rm th}\\) layer \\({\\bf \\Phi}_l\\) have shape \\((n_{l+1}, n_{l})\\). Then the whole network is \\[{\\rm KAN}({\\bf x})={\\bf \\Phi}_{L-1}\\circ\\cdots \\circ{\\bf \\Phi}_1\\circ{\\bf \\Phi}_0\\circ {\\bf x}\\] In constrast, a Multi-Layer Perceptron is interleaved by linear layers \\({\\bf W}_l\\) and nonlinearities \\(\\sigma\\): \\[{\\rm MLP}({\\bf x})={\\bf W}_{L-1}\\circ\\sigma\\circ\\cdots\\circ {\\bf W}_1\\circ\\sigma\\circ {\\bf W}_0\\circ {\\bf x}\\] Even though cumbersome to write, but simpler to see is the following form of the KAN network. Assuming output dimension \\(n_{L}=1\\), and define \\(f(\\bf{x})\\equiv {\\rm KAN}(\\bf{x})\\): \\[\nf(\\bf{x})=\\sum_{i_{L-1}=1}^{n_{L-1}}\\phi_{L-1,i_{L},i_{L-1}}\\left(\\sum_{i_{L-2}=1}^{n_{L-2}}\\cdots\\left(\\sum_{i_2=1}^{n_2}\\phi_{2,i_3,i_2}\\left(\\sum_{i_1=1}^{n_1}\\phi_{1,i_2,i_1}\\left(\\sum_{i_0=1}^{n_0}\\phi_{0,i_1,i_0}(x_{i_0})\\right)\\right)\\right)\\cdots\\right)\n\\]\n\nThe basic ingredient is the so called learnable activation \\(\\phi_{l,j,i}\\) which maps the post-activation of \\(i\\)th neuron in layer \\(l\\) to the pre-activation of \\(j\\)th neuron in the \\(l+1\\)th layer. Effectively, it is the edge connecting two neurons on adjacent layers. But how can it be made learnable? Represent this activation function as: \\[\n\\begin{align}\n\\phi(x)=w_{b} b(x)+w_{s}{\\rm spline}(x) \\\\\nb(x)={\\rm silu}(x)=x/(1+e^{-x}) \\\\\n{\\rm spline}(x) = \\sum_i c_iB_i(x)\n\\end{align}\n\\] where \\(c_i\\)s are trainable/learnable parameters, \\(B_i\\) are the Spline basis functions (in the original KAN paper). The authors included \\(b(x)\\), a \\(SiLU\\), as a residual connection.\nThe above development of KANs is compelling but it is much more illustrative to approach it from a Nonparametric Regression point of view, and then see that KAN’s are a type of Deep Nonparametric Regressors. Then, we realize that the standard terminology like neurons, activations, pre-activations, post-activation etc., can be completely dropped.\n\n\nShallow Nonparametric Regression\nConsider the following regression problem \\[y^{[i]} \\equiv f(x^{[i]}) + e^{[i]} \\equiv \\phi(x^{[i]}) + e^{[i]}, i \\in \\left\\{1,\\dots,N\\right\\}\\] with \\(D = \\{x^{[i]}, y^{[i]}\\}_{i=1}^{N}\\) representing all the data available to fit (train) the model \\(f(x)\\). It is customary to write the model as \\(f(x)\\) instead of \\(\\phi(x)\\). It is done for compatibility with KAN. In the shallow case \\(f(x) = \\phi(x)\\) otherwise \\(\\phi(x)\\) is used to denote the building blocks and \\(f(x)\\) will be a composition of many such building blocks.\nFor a moment, w.l.o.g, assume \\(x\\) to be univariate. In a typical regression setup, one constructs features such as \\(x^2, x^3,\\dots\\), like in polynomial regression and treat this as a Linear Regression problem. We can view this standard procedure as expanding the function \\(f(x)\\) on a set of Polynomials. Seen in a more general sense, we can choose an appropriate Basis Functions to construct the feature space. For example [see Chapter 9 of All of Nonparametric Statistics] \\[f(x) \\equiv \\phi(x) =  \\sum_{i=1}^{\\infty} \\beta_i B_i(x)\\] where \\(B_1(x) \\equiv 1, B_i(x) \\equiv \\sqrt{2}\\cos((i-1)\\pi x) \\text{ for } i \\ge 2\\). See the figure below.\n\n\n\nShallow Nonparametric Regression\n\n\nIn practice, we will truncate the expansion up to some finite number of terms. For illustration, say, we choose \\(p\\) terms. Then, in matrix notation, the regression problem is: \\[\n\\begin{array}{left}\n{\\bf y} = {\\bf X}{\\bf \\beta} + {\\bf \\epsilon}\n\\end{array}\n\\] where \\[\n\\begin{array}{left}\n{\\bf X}_{N \\times p} &=&\n\\begin{pmatrix} 1 & \\sqrt{2}\\cos(\\pi x^{[1]}) & \\dots & \\sqrt{2}\\cos((p-1)\\pi x^{[1]}) \\\\\n1 & \\sqrt{2}\\cos(\\pi x^{[2]}) & \\dots & \\sqrt{2}\\cos((p-1)\\pi x^{[2]}) \\\\\n\\vdots & & & \\vdots \\\\\n1 & \\sqrt{2}\\cos(\\pi x^{[N]}) & \\dots & \\sqrt{2}\\cos((p-1)\\pi x^{[N]})\n\\end{pmatrix} \\\\\n{\\bf \\beta}_{p \\times 1} &=& [\\beta_1, \\beta_2, \\dots, \\beta_p ]^T \\\\\n{\\bf y}_{N \\times 1} &=& [y^{[1]}, y^{[2]}, \\dots, y^{[N]} ]^T \\\\\n\\end{array}\n\\] This is the classic Nonparametric Regression setup. Different choices of the basis functions lead to different design matrices \\({\\bf X}\\). Some popular choices are Splines, Chebyshev Polynomials, Legendre Polynomials, Wavelets, among others.\n\nSpecifically, the function approximation with wavelets takes the following form [see Wavelet Regression in Python for a very nice demonstration of wavelets for denoising]: \\[\\phi(x) = \\alpha\\zeta(x) + \\sum_{j=0}^{J-1}\\sum_{k=0}^{2^j-1}\\beta_{jk}\\psi_{jk}(x)\\] where \\[\\alpha=\\int_0^1\\phi(x)\\zeta(x)dx\\text{, }\\beta_{jk}=\\int_0^1\\phi(x)\\psi_{jk}(x)dx.\\] Here \\(\\alpha\\) and \\(\\beta_{jk}\\) are called the scaling coefficients and the detail coefficients, respectively. The basic idea is that the detail coefficients capture the coarser details of the function while the scaling, or smoothing, coefficients capture the overall functional form.\n\nSo, by choosing a basis function, we map the observed inputs into the feature space defined by the basis functions, and project the response onto the space spanned by the basis functions. This is basically a Linear Regression in a different function space (induced by the basis functions), commonly referred to as Nonparametric Regression.\n\nGeneralized Additive Models\nHow do we extend the above formulation to multivariate case. Say, we have \\(n_{0}\\) dimensional inputs. One obvious but non-trivial way is to choose a multivariate basis function. The other way to do that is to construct feature space based on univariate functions which we know how to approximate already. For example, take the cartesian product space as \\(f(x) \\equiv \\phi(x) = \\Pi_{p=1}^{n_{0}} \\phi_p(x_p)\\) where \\(\\phi_p\\) can be expanded like before (univariate case). Or we can construct the function \\(f(x)\\) additively as \\[\n\\begin{array}{left}\nf(x) \\equiv \\phi(x) = \\sum_{p=1}^{n_{0}} \\phi_p(x_p)\n\\end{array}\n\\] See the figure below for an illustration.\n\n\n\nKAN Neuron = GAM\n\n\nThat is, for every dimension \\(p\\) of input, there is a corresponding \\(\\phi_p\\) and we can add them to get a multivariate function. In fact, the above specification appears in the framework developed by Hastie and Tibhirani in 1986, known as Generalized Addtive Models (see paper, wiki) or GAMs as they are popularly referred to.\nIn the context of KANs, we can see this exactly as node at layer \\(l+1\\), which takes inputs from layer \\(l\\), whose edges represent the transformation of the inputs via \\(\\phi_p\\). Except for notational differences, each neuron in KAN is a GAM.\n\n\nVector Generalized Additive Models\nSo far we are dealing with single output and multiple inputs. But what if there are \\(n_{1}\\) outputs. This leads us to Vector Generalized Additive Models (VGAMs), proposed by Yee and Wild in 1996 (see paper, wiki). We can represent VGAM as:\n\\[\n\\begin{array}{left}\ny_q \\equiv f_{q,.}(x) \\equiv \\phi_{q,.}(x)  = \\sum_{p=1}^{n_{0}} \\phi_{q,p}(x_p)\n\\end{array}\n\\]\nSee the figure below.\n\n\n\nKAN Layer = VGAM\n\n\nIn effect, one KAN layer is actually a VGAM. So, given \\(D=\\{x^{[i]}, y^{[i]}\\}_{i=1}^{N}\\) with \\(x^{[i]} \\in R^{n_{0}}, y^{[i]} \\in R^{n_{1}}\\) outputs, KAN Layer and VGAM learn \\(\\phi_{q,p}\\) which are specified in terms of the basis functions. Note that, the choice of the basis functions should correspond to the domain and range of the functions being modeled. GAMS and VGAM models are trained (or leant) using back-fitting techniques. There is no reason why we can not apply backprop and fit these models using gradient descent.\n\n\n\nDeep Nonparametric Regression\nWhat remains to be done is to stack the KAN layers or VGAMs. That gets us to the model we have seen before: \\[\nf(\\bf{x})=\\sum_{i_{L-1}=1}^{n_{L-1}}\\phi_{L-1,i_{L},i_{L-1}}\\left(\\sum_{i_{L-2}=1}^{n_{L-2}}\\cdots\\left(\\sum_{i_2=1}^{n_2}\\phi_{2,i_3,i_2}\\left(\\sum_{i_1=1}^{n_1}\\phi_{1,i_2,i_1}\\left(\\sum_{i_0=1}^{n_0}\\phi_{0,i_1,i_0}(x_{i_0})\\right)\\right)\\right)\\cdots\\right)\n\\] For clarity sake, \\(x_{i_0}\\) is referring to the \\(i_0\\)th input dimension or feature, \\(\\phi_{l,q,p}(.)\\) refers to the function that maps \\(p\\)th input of layer \\(l\\) to the (before summation) \\(q\\)th output.\n\n\n\nKAN = Deep Nonparametric Regression Model\n\n\nFrom the above figure, it is clear that the edges of the KAN network are nonparametric functions \\(\\phi(x)\\) which are specified via basis functions such as Splines. Even MLPs have weights on the edges. So, the mainstream interpretation that, in KANs, the activations are on the edges and they are learnable is not very convincing. In both MLPs and KANs, the edges are learnable. For example, if we choose \\(\\phi(x) = \\beta x\\), and replace the sum with sum preceded by elementwise, fixed activations like \\(SiLU\\), we get the standard MLP network. KAN proposed by Liu et al in 2024 is a Deep Nonparametric Regression framework. Specifically, we note the following:\n\nKAN neuron is a Generalized Additive Model (GAM)\nKAN layer is a Vector GAM (VGAM)\nKAN Network is a Deep VGAM or simply put a Deep Nonparametric Regression Model\n\nThe generality of this technique comes from\n\nflexibility of the specification (multiple inputs, multiple outputs, varying depth and width, choice of basis functions) and\nthe deep learning framework itself (so we can fit almost all methods using backprop without doing any custom implementations). See for example,\n\nFast-KAN: KANs with Radial Basis Functions code\nChebyKAN: KANs with Chebyshev Polynomials paper\nWavKAN: KANs with Wavelets paper\nTorchKAN: KANs with Monomials, Legendre Polynomials code\n\n\nNot only that, the modularity of the KAN layer allows one to mix and match KAN layer with other modules such as a Transformer block or an RNN block, for example. We can replace MLP with KAN almost like a drop-in. We have already seen implementation of KANs in GPT models GTP-KAN, GPT2-KAN. They also started appearing in CNNs. For more resources see awesome-KANS.\nExplore the features of KANs such as interpretability, solving PDEs, check out this KAN Features notebook or go through the examples from PyKAN official repo.\n\n\nLimitations\nFew limitation of KANs at this time are:\n\nThey are relying on one dimensional (univariate) functions as the building blocks. This need not be efficient always. For example, consider 2d functions that have certain spatial or temporal properties. To apply KANs, we have to convert them to 1d first and then model them in KAN layers. It would be much better if we can find multivariate basis functions that can naturally deal with arbitrary dimensions. For example, to process images, 2d wavelets could be a better choice.\nIn many cases, MLPs still seem to be doing better. See KAN or MLP: A Fairer Comparison for details. KANs seem to be good at symbolic regression problems which have much more grounding in physical and other sciences.\nSpline-based KANs, as noted by others are computationally slow. Knot selection (how many and where to place them) are important hyperparameters which can affect the performance significancy. Grid refinement implemented by the KAN authors is a welcoming step in this direction but it is a hard problem.\n\nThat said, KANs are extremely interesting in the sense that:\n\nKAN is a Deep Nonparametric regression framework\nVery generic\nAfter pruning and doing some symbolic search, one can recover interpretable equations, which may be difficult to do in a typical Deep Neural Network.\nCombining the power of Wavelets, Filter Banks, Multi Resolution Analysis (MRA) in the KAN framework would be interesting to pursue.",
    "crumbs": [
      "Lectures",
      "KANs"
    ]
  },
  {
    "objectID": "lectures/L04.html",
    "href": "lectures/L04.html",
    "title": "RNNs",
    "section": "",
    "text": "Materials:\nDate: Saturday, 12-October-2024, 1.30pm, IST.",
    "crumbs": [
      "Lectures",
      "RNNs"
    ]
  },
  {
    "objectID": "lectures/L04.html#materials",
    "href": "lectures/L04.html#materials",
    "title": "RNNs",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\nFFNs\nCNNs\n\n\n\nIn-Class\n\ntbd\ntbd\n\n\n\nLab\n\ntbd\ntbd\n\n\n\nPost-class:\n\ntbd\ntbd\n\n\n\nRNN Papers and Advancements\n\ntbd\ntbd\n\n\n\nTheory\n\ntbd\ntbd\n\n\n\nAdditional References\n\ntbd\ntbd",
    "crumbs": [
      "Lectures",
      "RNNs"
    ]
  },
  {
    "objectID": "lectures/L04.html#notes",
    "href": "lectures/L04.html#notes",
    "title": "RNNs",
    "section": "Notes",
    "text": "Notes\nSure, here is the content converted into Markdown format:",
    "crumbs": [
      "Lectures",
      "RNNs"
    ]
  },
  {
    "objectID": "lectures/L04.html#ar1-system",
    "href": "lectures/L04.html#ar1-system",
    "title": "RNNs",
    "section": "AR(1) System",
    "text": "AR(1) System\n\\(S(t) \\sim N(\\mu_s, \\sigma_s^2) \\quad \\text{for} \\quad t \\ge 0\\)\n\nInitial condition not satisfied (steady-state not yet reached).\n\nExample: - Parameters: \\(\\( a = 0.9, \\sigma_s^2 = 0.1, \\mu_s = 5, \\sigma_e^2 = 1 \\)\\) - Question: What does this signal look like?\n\nNotes:\n\\[\nS_0 = aS(t) + u_0 \\\\\nS_1 = aS_0 + u_1 \\\\\n\\vdots\nS_n = aS(t) + \\sum_{k=0}^{n} e(n-k)\n\\]",
    "crumbs": [
      "Lectures",
      "RNNs"
    ]
  }
]