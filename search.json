[
  {
    "objectID": "notebooks/00-01-FFN-Classification-Iris.html",
    "href": "notebooks/00-01-FFN-Classification-Iris.html",
    "title": "FFN Classification",
    "section": "",
    "text": "A notebook to apply an FFN (Feed Forward Neural Network) to classify the flower species type. We will use the the famous Iris dataset (which is now the equivalent of the hellow world dataset in the Data Science World)\nBased on - Kaggle Notebook for Iris Classiifcation - PyTorch for Iris Dataset - Iris Classification\nLoad, Visualize, Summarise Data\nsklearn comes with Iris dataset. We will load it, and do some basic visualization. It is always a good idea to “look” at the data before (blindly) running any models.\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nimport matplotlib.pyplot as plt\n\n_, ax = plt.subplots(figsize=(5,5))\nscatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\nax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n_ = ax.legend(\n    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n)\n\n\n\n\n\n\n\n\nWe see that, 1. there are four features, and it is a three class classification problem 2. Using two features (sepal length, and sepal width), it is clear that, a perceptron will not be able separate _versicolor from virginica (data is not linearly separable) class. 3. But setosa can be separated from the remaining two.\nLet us look at the basic descriptions of the data.\n\n\nprint('feature name',iris.feature_names)\nprint('features type of data',type(iris.data))\nprint('features shape',iris.data.shape)\nprint('feature name',iris.target_names)\nprint('target type of data',type(iris.target))\nprint('target shape',iris.target.shape)\n\nfeature name ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nfeatures type of data &lt;class 'numpy.ndarray'&gt;\nfeatures shape (150, 4)\nfeature name ['setosa' 'versicolor' 'virginica']\ntarget type of data &lt;class 'numpy.ndarray'&gt;\ntarget shape (150,)\n\n\n\nprint('target labels',iris.target)\n\ntarget labels [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n\n\nHa. In the original dataset, the data is organized by class. If we naively prepare the mini batches (sequentially), model will only see data corresponding to only one class. This will be pretty problematic to get proper gradient signals. We should shuffle the data s.t diversity in the mini batches is maintained.\nA homework exercise :)\nImagine you split the data into two batches. One containing only say class 0, and other contains only class 1. During training, the model sees these two batches cyclically. Will the model ever converge.\n\nWill it converge when the data is linearly separable?\nWill it converge when the data is not linearly separable?\nDoes having a balanced class representation in every mini batch helps? Which way does it?\n\n\nfrom datasets import Dataset\nimport pandas as pd\ndf = pd.read_csv(\"hf://datasets/scikit-learn/iris/Iris.csv\")\ndf = pd.DataFrame(df)\ndf.head()\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\n\ndf['Species'].unique()\n\narray(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)\n\n\n\n# transform species to numerics\ndf.loc[df.Species=='Iris-setosa', 'Target'] = 0\ndf.loc[df.Species=='Iris-versicolor', 'Target'] = 1\ndf.loc[df.Species=='Iris-virginica', 'Target'] = 2\nprint(df.Target.unique())\ndf.head()\n\n[0. 1. 2.]\n\n\n\n\n\n\n\n\n\nId\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nTarget\n\n\n\n\n0\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0.0\n\n\n1\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0.0\n\n\n2\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0.0\n\n\n3\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0.0\n\n\n4\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0.0\n\n\n\n\n\n\n\n\ndf.drop(['Id'],axis=1,inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nSepalLengthCm\nSepalWidthCm\nPetalLengthCm\nPetalWidthCm\nSpecies\nTarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n0.0\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = df[df.columns[0:4]].values\ny = df.Target.values\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.8)\n\n\n# check that data is shuffled\nprint(y_train)\n\n[2. 0. 1. 2. 2. 2. 2. 2. 2. 0. 2. 1. 0. 1. 2. 1. 0. 1. 2. 0. 0. 1. 1. 1.\n 1. 1. 1. 0. 0. 1.]\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)\n\n\nclass MLP(nn.Module):\n    # define nn\n    def __init__(self, input_dim=4, output_dim=3, hidden_dim = [128,64]):\n        super(MLP, self).__init__()\n        self.input = nn.Linear(input_dim, hidden_dim[0])\n        self.hidden = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.out = nn.Linear(hidden_dim[1], output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        X = self.relu(self.input(X))\n        X = self.relu(self.hidden(X))\n        X = self.out(X)\n        return X\n\n\ninput_dim = 4\noutput_dim = 3\nhidden_dim = [64, 64]\nmodel = MLP(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim)\n\n\nlearning_rate = 0.01\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n\n\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\n\n\ntrain_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 0.0001, Test Loss: 0.4372\nEpoch 100/1000, Train Loss: 0.0000, Test Loss: 0.4097\nEpoch 150/1000, Train Loss: 0.0000, Test Loss: 0.3973\nEpoch 200/1000, Train Loss: 0.0000, Test Loss: 0.3885\nEpoch 250/1000, Train Loss: 0.0000, Test Loss: 0.3816\nEpoch 300/1000, Train Loss: 0.0000, Test Loss: 0.3758\nEpoch 350/1000, Train Loss: 0.0000, Test Loss: 0.3712\nEpoch 400/1000, Train Loss: 0.0000, Test Loss: 0.3676\nEpoch 450/1000, Train Loss: 0.0000, Test Loss: 0.3652\nEpoch 500/1000, Train Loss: 0.0000, Test Loss: 0.3635\nEpoch 550/1000, Train Loss: 0.0000, Test Loss: 0.3623\nEpoch 600/1000, Train Loss: 0.0000, Test Loss: 0.3614\nEpoch 650/1000, Train Loss: 0.0000, Test Loss: 0.3608\nEpoch 700/1000, Train Loss: 0.0000, Test Loss: 0.3603\nEpoch 750/1000, Train Loss: 0.0000, Test Loss: 0.3600\nEpoch 800/1000, Train Loss: 0.0000, Test Loss: 0.3597\nEpoch 850/1000, Train Loss: 0.0000, Test Loss: 0.3596\nEpoch 900/1000, Train Loss: 0.0000, Test Loss: 0.3597\nEpoch 950/1000, Train Loss: 0.0000, Test Loss: 0.3598\nEpoch 1000/1000, Train Loss: 0.0000, Test Loss: 0.3601\n\n\n\nplt.figure(figsize=(10,10))\nplt.plot(train_losses, label='train loss')\nplt.plot(test_losses, label='test loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\npredictions_train = []\npredictions_test =  []\nwith torch.no_grad():\n    predictions_train = model(X_train)\n    predictions_test = model(X_test)\n\n\nprint(predictions_train.shape)\nprint(type(predictions_train))\n\nprint(y_train.shape)\nprint(type(y_train))\n\ntorch.Size([30, 3])\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([30])\n&lt;class 'torch.Tensor'&gt;\n\n\n\ndef get_accuracy_multiclass(pred_arr,original_arr):\n    if len(pred_arr)!=len(original_arr):\n        return False\n    pred_arr = pred_arr.numpy()\n    original_arr = original_arr.numpy()\n    final_pred= []\n    # we will get something like this in the pred_arr [32.1680,12.9350,-58.4877]\n    # so will be taking the index of that argument which has the highest value here 32.1680 which corresponds to 0th index\n    for i in range(len(pred_arr)):\n        final_pred.append(np.argmax(pred_arr[i]))\n    final_pred = np.array(final_pred)\n    count = 0\n    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy\n    for i in range(len(original_arr)):\n        if final_pred[i] == original_arr[i]:\n            count+=1\n    return count/len(final_pred)\n\n\ntrain_acc = get_accuracy_multiclass(predictions_train,y_train)\ntest_acc  = get_accuracy_multiclass(predictions_test,y_test)\nprint(f\"Training Accuracy: {round(train_acc*100,3)}\")\nprint(f\"Test Accuracy: {round(test_acc*100,3)}\")\n\nTraining Accuracy: 100.0\nTest Accuracy: 92.5",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>FFN Classification</span>"
    ]
  },
  {
    "objectID": "notebooks/00-02-FFN-Regression-Friedman2.html",
    "href": "notebooks/00-02-FFN-Regression-Friedman2.html",
    "title": "FFNs Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_friedman2\nX, y = make_friedman2(n_samples = 200, random_state=42, noise=0.5)\nprint(X.shape)\nprint(y.shape)\n\n\nplt.scatter(X[:,0],y)\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n\n\nscaler = StandardScaler()\n\nprint(y_train.shape)\ny_tmp = y_train.reshape(-1, 1)\ny_train = scaler.fit_transform(y_tmp)\nprint(y_train.shape)\n\ny_tmp = y_test.reshape(-1, 1)\ny_test = scaler.transform(y_tmp)\nprint(y_test.shape)\n\n(160,)\n(160, 1)\n(40, 1)\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.FloatTensor(y_train)\ny_test = torch.FloatTensor(y_test)\n\n\nclass MLP(nn.Module):\n    # define nn\n    def __init__(self, input_dim=4, output_dim=3, hidden_dim = [128,64]):\n        super(MLP, self).__init__()\n        self.input = nn.Linear(input_dim, hidden_dim[0])\n        self.hidden = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.out = nn.Linear(hidden_dim[1], output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        X = self.relu(self.input(X))\n        X = self.relu(self.hidden(X))\n        X = self.out(X)\n        return X\n\n\ninput_dim = 4\noutput_dim = 1\nhidden_dim = [64, 64]\nmodel = MLP(input_dim=input_dim, output_dim=output_dim, hidden_dim=hidden_dim)\n\n\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n\n\ndef train_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses):\n    for epoch in range(num_epochs):\n        #clear out the gradients from the last step loss.backward()\n        optimizer.zero_grad()\n        \n        #forward feed\n        output_train = model(X_train)\n\n        #calculate the loss\n        loss_train = criterion(output_train, y_train)\n\n\n        #backward propagation: calculate gradients\n        loss_train.backward()\n\n        #update the weights\n        optimizer.step()\n        \n        output_test = model(X_test)\n        loss_test = criterion(output_test,y_test)\n\n        train_losses[epoch] = loss_train.item()\n        test_losses[epoch] = loss_test.item()\n\n        if (epoch + 1) % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n\n\nimport numpy as np\nnum_epochs = 1000\ntrain_losses = np.zeros(num_epochs)\ntest_losses  = np.zeros(num_epochs)\ntrain_network(model,optimizer,criterion,X_train,y_train,X_test,y_test,num_epochs,train_losses,test_losses)\n\nEpoch 50/1000, Train Loss: 0.0027, Test Loss: 0.0044\nEpoch 100/1000, Train Loss: 0.0007, Test Loss: 0.0018\nEpoch 150/1000, Train Loss: 0.0004, Test Loss: 0.0016\nEpoch 200/1000, Train Loss: 0.0002, Test Loss: 0.0015\nEpoch 250/1000, Train Loss: 0.0001, Test Loss: 0.0015\nEpoch 300/1000, Train Loss: 0.0001, Test Loss: 0.0016\nEpoch 350/1000, Train Loss: 0.0001, Test Loss: 0.0015\nEpoch 400/1000, Train Loss: 0.0001, Test Loss: 0.0015\nEpoch 450/1000, Train Loss: 0.0000, Test Loss: 0.0015\nEpoch 500/1000, Train Loss: 0.0000, Test Loss: 0.0015\nEpoch 550/1000, Train Loss: 0.0000, Test Loss: 0.0015\nEpoch 600/1000, Train Loss: 0.0000, Test Loss: 0.0015\nEpoch 650/1000, Train Loss: 0.0000, Test Loss: 0.0015\nEpoch 700/1000, Train Loss: 0.0001, Test Loss: 0.0015\nEpoch 750/1000, Train Loss: 0.0000, Test Loss: 0.0015\nEpoch 800/1000, Train Loss: 0.0000, Test Loss: 0.0014\nEpoch 850/1000, Train Loss: 0.0001, Test Loss: 0.0016\nEpoch 900/1000, Train Loss: 0.0000, Test Loss: 0.0014\nEpoch 950/1000, Train Loss: 0.0000, Test Loss: 0.0014\nEpoch 1000/1000, Train Loss: 0.0000, Test Loss: 0.0015\n\n\n\nplt.figure(figsize=(10,10))\nplt.plot(train_losses, label='train loss')\nplt.plot(test_losses, label='test loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\npredictions_train = []\npredictions_test =  []\nwith torch.no_grad():\n    predictions_train = model(X_train)\n    predictions_test = model(X_test)\n\nprint(predictions_train.shape)\nprint(type(predictions_train))\n\nprint(y_train.shape)\nprint(type(y_train))\n\ntorch.Size([160, 1])\n&lt;class 'torch.Tensor'&gt;\ntorch.Size([160, 1])\n&lt;class 'torch.Tensor'&gt;\n\n\n\nyt = y_test.numpy()\nprint(type(yt))\nprint(yt.shape)\n\nyh = predictions_test.numpy()\nprint(type(yh))\nprint(yh.shape)\n\n&lt;class 'numpy.ndarray'&gt;\n(40, 1)\n&lt;class 'numpy.ndarray'&gt;\n(40, 1)\n\n\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import root_mean_squared_error as rmse\n\nprint('mse is: ', mse(yt, yh))\nprint('rmse is: ', rmse(yt, yh))\n\nmse is:  0.001506095\nrmse is:  0.03880844\n\n\n\nplt.scatter(yt,yh)\n\n\n\n\n\n\n\n\n\nresiduals = yt-yh\nplt.hist(residuals)\nplt.show()\n\nimport pandas as pd\npd.DataFrame(residuals).plot(kind='density')",
    "crumbs": [
      "Lab",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>FFNs Regression</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Welcome\nDear Faculty, Students and Learners\nSee the course page for recent information on Lectures, Labs, Resources etc..",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "Deep Learning",
    "section": "Announcements",
    "text": "Announcements\n\n[07-Sep-2024] L01 added.\n[21-Aug-2024] Course website up",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Learning",
    "section": "Overview",
    "text": "Overview\nPrereqs\n\nUndergraduate level exposure to Linear Algebra, Calculus\nAbility to read Python code\nBasic exposure to ML/DL\n\nPart-1: A Mathematical Introduction to Deep Learning Models\n\nTopics\n\nFeed Forward Neural Networks (FFNs)\nConvolution Neural Networks (CNNs)\nRecurrent Neural Networks (RNNs)\nTransformers\n\n\nPart-2: A Mathematical Introduction to Deep Generative Models\n\nTopics\n\nVariational Auto Encoders (VAEs)\nGenerative Adversarial Networks (GANs)\nFlow Networks\nDiffusion Models",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "When I started reading papers about Deep Learning, typically published in conferences, I had hard time following them. The key modeling details are presented in textual form with accompanied by architecture (block) diagrams. If the paper had any Math in them it was for proofs but not for explaining the models or the expressions were restricted to highlighting key contributions (eg. loss function).\nDespite this information, it was hard for me to understand them to be able to reproduce. I have to look at source code to see how they are implemented and go back to the paper and read again, and repeat this process. This was probably due to the my formal training Statistics. I always start with the model (expressed as equations). This unlearning took a long time. I am assuming that folks with training in Maths/ Applied Maths will also have hard time reading papers in the ML space for the same reason - there is no precision.\nIn this course, we will discuss different architectures (models) in Deep Learning using Mathematical language as much as possible (for the sake of precision) and follow-them up with implementation in code.\nThe intended audience is those with math background, that wants to appreciate modern deep learning models. We will not get into “why” deep learning works or their applications. In the resources section, I will provide ample references for those interested that wants to explore further.\n\nDisclaimer\nThis course is by no means a replacement of any other resources available. Hopefully, the content and views presented complement the current practice of MLOps, readers and students benefit from it.\nopenly,\nThe Saddle Point",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Course",
    "section": "",
    "text": "Syllabus & Schedule",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "course.html#references",
    "href": "course.html#references",
    "title": "Course",
    "section": "References",
    "text": "References\n\n[course] CS6910, Prof. Mitesh Khapra’s CS6910 Deep Learning at IIT-M\n[course] CS236 Prof. Stefano Emron’s course on Deep Generative Modeling at Stanford Fall’23\n[Book] Deep Generative Modeling, Jakub Tomxzak\n[Book] Understanding Deep Learning, Simon Prince\n[Book] Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville\n\nSoma S Dhavala\nDiscussion Anchor",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course</span>"
    ]
  },
  {
    "objectID": "lectures/L01.html",
    "href": "lectures/L01.html",
    "title": "FFNs",
    "section": "",
    "text": "Materials:\nDate: Saturday, 07-Sep-2024, 1.30pm",
    "crumbs": [
      "Lectures",
      "FFNs"
    ]
  },
  {
    "objectID": "lectures/L01.html#materials",
    "href": "lectures/L01.html#materials",
    "title": "FFNs",
    "section": "",
    "text": "Pre-work:\n\nRefresh ML foundations.\nRead “The 100 page ML book” by Andiry Burkov. Chapters accessible here\n\n\n\nIn-Class\n\nHistory of Deep Learning\n\nLecture-1 pdf, Interactive slides [R01]\nLecture-1 pdf [R02]\n\nVisualizations\n\nInteractive Figures to visualize NNs [R05]\nRepresentational Power of NNs from [R06]\n\nNeural Networks Motivation\n\nMcCulloch-Pitts Neuron, Perceptron Lecture-2:pdf [R01]\nDigital Logic Modeling by Perceptron Neural Networks:pdf [R03]\n\nFFNs\n\nFFNs for Classification and Language Modeling: pdf\n\n\n\n\nLab\n\nFFN for Classification on Iris data\nFFN for Regression on Friedman2 data\n\n\n\nPost-class:\n\nLecture-3:pdf Sigmoid Neuron, Error Surfaces, Representation Power of FFNs [R02]\nGradient Descent, Word Vectors [R03]\nLecture-4:pdf FFNs and Backprop [R02]\nComputational Graphs, Backprop:pdf [R03]\nLecture - Expressivity and UAT\n\n\n\nPapers\n\nUniversal Approximation Theorem original paper by Cybenko pdf\nMultilayer FFNs are universal approximators Hornik, Stinchcombe, and White\nRepresentation Benefits of Deep FFNs Telagarsky\n\n\n\nReferences\n\nR01: CS6910: Deep Learning Mitesh Khapra @ IIT Madras\nR02: CS7015: Deep Learning Mitesh Khapra @ IIT Madras (earlier version of CS6910)\nR03: LING 574: Deep Learning for NLP Shane Steinhard-Threlkeld @ University of Washington\nR04: Dive into Deep Learning\nR05: Understanding Deep Learning\nR06: Neural Networks and Deep Learning",
    "crumbs": [
      "Lectures",
      "FFNs"
    ]
  }
]